---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Resistant lines

```{r echo=FALSE}
source("libs/Common.R")
```


```{r echo = FALSE}
pkg_ver(c("dplyr", "ggplot2", "MASS"))
```

## Introduction

Ordinary least squares regression lines (those created with the `lm()` function) suffer from sensitivity to outliers. Because `lm`'s best fit line makes use of the mean (which is not a robust measure of location), its breakdown point is $1/n$  meaning that all it takes is for one data point to behave differently from the rest of the points to significantly alter the slope and intercept of the best fit line. For example, let's start with a well behaved dataset where all points are perfectly aligned and fit this batch with a regression line:

```{r fig.height=2.5, fig.width=3, small.mar=TRUE}
x <- seq(1:11)
y <- 5 + 2 * x
plot(y ~ x, pch = 20)
M <- lm(y ~ x)
abline( M , col = "red")
```

As expected, we have a perfect fit. And the regression model's coefficients match those used to create the data.

```{r}
coef(M)
```


Now, what if one of the points is re-positioned in the plot, what happens to the regression line?

```{r fig.height=2.5, fig.width=3, small.mar=TRUE}
y[11] <- 2
plot(y ~ x, pch = 20)
M <- lm(y ~ x)
abline( M , col = "red")
```

Note the significant change in the line's characteristics, its intercept and slope are now:

```{r}
coef(M)
```

The slope dropped from 2 to 0.86 because of a single point!

If our goal is to explore what the bulk of the data has to say about the phenomena being investigated, we certainly don't want a small batch of "maverick" values to hijack the analysis. We therefore need a set of fitting tools that minimize the influence of outliers. There are many options out there; most notable are Tukey's **3-point summary line** and the **bisquare** robust estimation method outlined in Cleveland's text.

&nbsp;&nbsp;

## Robust lines
&nbsp;&nbsp;

### Tukey's 3-point summary

The idea is simple in concept and easy to implement with a pen and paper if the dataset is not too big. It involves dividing the dataset into three approximately equal groups of values and summarizing these groups by computing their respective medians. Two half-slopes are then used to join the three points. Note that points that share the same x value are lumped into the same batch which can lead to unequal group sizes. The motivation behind this plot is to use the three-point summary to provide a robust assessment of the type of relationship between both variables. Such plots are often used to help guide re-expression of the variables x or y or both for the sole purpose of straightening the x-y relationship.

Let's look at an example using the last (modified) dataset. First, we'll divide the plot into three approximately equal batches.


```{r echo=FALSE, fig.show='hide'}
#source("http://www.colby.edu/~mgimond/R/MG_EDA.R")
#r.lm <- sum3pt(x=x, y=y, dir=F)
#source("https://raw.githubusercontent.com/mgimond/tukeyedar/master/R/eda_3pt.R")
library(tukeyedar)
r.lm <- eda_3pt(x = x, y = y, dir = FALSE)
```

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
# Find the three parts
x.u <- unique(x)
n <- length( unique(match(x, x.u[order(x.u)] )) )
span <- floor(n/3)
r    <- n %% 3

# Compute the max index of each third
if( r == 0) d <- c(span, span *2, span *3)
if( r == 1) d <- c(span, span *2 + 1, span *3 + 1)
if( r == 2) d <- c(span + 1, span * 2 + 1, span *3 + 2)
  
plot(x, y, pch = 20)
abline(v = d[1:2], lty = 2, col = "grey80")
```

Next, compute the median x and y values within each section. 

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
plot(x, y, pch = 20)
abline(v = d[1:2], lty = 2, col = "grey80")
points(cbind(r.lm$xmed, r.lm$ymed), pch = 16, col = "red")

```

Note that we do not include the same point in any two median calculations. This implies that for the mid-third of the data, we do not include the point straddling the left boundary line when computing its median value. Likewise with the right-third of the data. 

The x median values are `r r.lm$xmed` and the y median values are `r r.lm$ymed`.

Finally, we fit the tail end medians with a straight line.

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
plot(x, y, pch = 20)
abline(v = d[1:2], lty = 2, col = "grey80")
points(cbind(r.lm$xmed, r.lm$ymed), pch = 16,col = "red")
lines(cbind(r.lm$xmed[-2], r.lm$ymed[-2]), lty = 1, col = "#444444")
```

This gives us the slope of the line (we can, of course, extend the lines to the boundaries of the x range of values). Note that we have not completely eliminated the influence of the outlier. The outlier does take part in helping fit the line, but it doesn't wield the same disproportionate influence on the line as did the regression line.

So what purpose does the middle median serve? It helps us adjust the vertical location of the line. The goal is to nudge the line upward or downward about 1/3 of the way towards the middle median value (parallel to the y-axis). This then defines the line's *intercept* (which is where the line crosses the y-axis when x = 0).

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
plot(x, y, pch = 20)
abline(v = d[1:2], lty = 2, col = "grey80")
points(cbind(r.lm$xmed, r.lm$ymed), pch = 16, col = "red")
lines(cbind(r.lm$xmed[-2], r.lm$ymed[-2] + 0.4), lty = 1, col = "#444444")
```

But you need not stop here. The next logical step is to look at the residuals. The following two plots compare the residuals from the regression model and the 3-point summary plot.

```{r fig.height=2.5, fig.width=6,  small.mar=TRUE,echo=FALSE}
OP <- par(mfrow = c(1, 2))
M.line <- line(x, y)
r1 <- residuals(M)
r2 <- residuals(M.line)
plot(x, r1, ylim = range(r1,r2), pch = 16, main="lm residual", ylab="Residual")
plot(x, r2, ylim = range(r1,r2), pch = 16, main="3pt residual", ylab="Residual")
par(OP)
```

Recall that we are seeking to minimize any non-random pattern in the residual. Even though a monotonic increase in residuals is discernible in both plots, the 3-point summary residual has less of a slope and (ignoring the outlier) has a smaller spread.

The 3-point summary fitting process can be extended by adding the residual slope to the original slope. This process is iterated as many times as needed until the residual slope is close to zero.

### Bisquare

A bisquare fitting strategy makes use of a non-parametric model that assigns *weights* to observations based on their proximity to the center of the distribution. The closer an observation is to that center, the greater its weight. 

The first step is to run a linear regression model on the data then to extract its residuals. Next, a weighting scheme is fit to the residuals such that the points associated with extreme residuals are assigned the smallest weight and the points associated with the smaller residual values are assigned the largest weight. The regression analysis is then re-run using those same weights thus minimizing the influence of the rogue points. This process is repeated several times until the residuals stabilize. The following figure shows three iterations of the bisquare function whereby the weights (shown as grey text next to each point) start off as `1` then are modified following the residuals derived from the most recent regression model.

```{r fig.height=2.5, fig.width=8, small.mar=TRUE, echo = FALSE}
wt.bisquare <- function(u, c = 6) {
  ifelse( abs(u/c) < 1, (1-(u/c)^2)^2, 0)
}

# Initialize weights
wt <- rep(1, length(x))
ramp <- colorRamp(c("gold", "orange", "black"), interpolate = "spline")

OP <- par(mfrow = c(1,3))
 # First plot
 M <- lm(y ~ x, weights = wt)
 plot(y ~ x, pch = 20, xlim = c(0,11), cex = 2)
 abline( M , col = "blue")
 text(x, y, labels = round(wt,2), pos = 2, offset = 0.5 , col = "grey30", font=1)

  # Second plot
 wt <- wt.bisquare( M$residuals/ median(abs(M$residuals)), c=6 )
 M <- lm(y ~ x, weights = wt)
 plot(y ~ x, pch = 20, xlim = c(0,11), cex = 2)
 abline( M, col = "blue")
 text(x, y, labels = round(wt,2), pos = 2, offset = 0.5 , col = "grey30", font=1)

   # Third plot
 wt <- wt.bisquare( M$residuals/ median(abs(M$residuals)), c=6 )
 M <- lm(y ~ x, weights = wt)
 plot(y ~ x, pch = 20, xlim = c(0,11), cex = 2)
 abline(M , col = "blue")
 text(x, y, labels = round(wt,2), pos = 2, offset = 0.5 , col = "grey30", font=1)
 par(OP)
```


There are different weighing strategies that can be implemented. One such implementation is presented next.

```{r fig.height=2.5, fig.width=3, small.mar=TRUE}
# Create the bisquare function
wt.bisquare <- function(u, c = 6) {
   ifelse( abs(u/c) < 1, (1-(u/c)^2)^2, 0)
}

# Assign an equal weight to all points
wt <- rep(1, length(x))

# Compute the regression, then assign weights based on residual values
for(i in 1:10){
  dat.lm <- lm(y ~ x ,weights=wt)
  wt <- wt.bisquare( dat.lm$res/ median(abs(dat.lm$res)), c = 6 )
}

# Plot the data and the resulting line
plot(x, y, pch = 20)
abline(dat.lm, col = rgb(1,0,0,0.3))
```

In the above example, the bisquare method does a great job in eliminating the outlier's influence. 


#### Built-in implementation of the bisquare

The `MASS` package has a robust linear modeling function called `rlm` that will implement a variation of the aforementioned bisquare estimation technique. Its results may differ slightly from those presented here, but the difference will be insignificant for the most part. 

Note that if you make use of `dplyr` in a work flow, loading `MASS` after `dplyr` will mask dplyr's `select` function. This can be problematic. So you either want to load `MASS` before `dplyr`, or you can call the function via `MASS::rlm`. An example of its use follows.

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE, fig.show='hold'}
dat <- data.frame(x,y)
M   <- lm( y ~ x, dat=dat)
Mr  <- MASS::rlm( y ~ x, dat=dat, psi="psi.bisquare")
plot(y ~ x,dat, pch=16, col=rgb(0,0,0,0.2))

# Add the robust line
abline( Mr, col="red")

# Add the default regression line
abline( M , col="grey50", lty=2)
```

The function `rlm` can also be called directly from within `ggplot`.

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE, fig.show='hold'}
library(ggplot2)
ggplot(dat, aes(x=x, y=y)) + geom_point() + 
       stat_smooth(method = "lm", se = FALSE, col = "grey50", lty = 2) +
       stat_smooth(method = MASS::rlm, se = FALSE, col = "red",
                   method.args = list(psi = "psi.bisquare"))
``` 

&nbsp;&nbsp;

## Robust loess

The bisquare estimation method can also be extended to the loess smoothing function by passing the `"symmetric"` option to the `family` parameter.


```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,  fig.show='hold'}
# Fit a regular loess model
lo <- loess(y ~ x, dat)

# Fit a robust loess model
lor <- loess(y ~ x, dat, family = "symmetric")

# Plot the data
plot(y ~ x, dat, pch = 16, col = rgb(0,0,0,0.2))

# Add the default loess
lines(dat$x, predict(lo), col = "grey50", lty = 2)

# Add the robust loess
lines(dat$x, predict(lor), col = "red")
```

The R base `loess` function can be implemented in `ggplot2`'s `stat_smooth` function as follows:

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE, fig.show='hold'}
library(ggplot2)
ggplot(dat, aes(x=x, y=y)) + geom_point() + 
          stat_smooth(method = "loess",                       # Regular loess 
                      se=FALSE, col="grey50", lty=2)  +    
          stat_smooth(method = "loess", 
                      method.args = list(family="symmetric"), # Robust loess
                      se=FALSE,  col="red") 
      
```

The loess parameters are passed to the `loess` function via the call to `method.args`. The red curve is that of the robust loess and the grey dashed curve is that of the standard loess.
