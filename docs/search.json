[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Data Analysis in R",
    "section": "",
    "text": "This book is a compilation of lecture notes used in an Exploratory Data Analysis in R course taught to undergraduates at Colby College. The course assumes little to no background in quantitative analysis nor in computer programming and was first taught in Spring, 2015. The course introduces students to data manipulation in R, data exploration (in the spirit of John Tukey’s EDA) and the R markdown language. Many of the visualization tools are adopted from William Cleveland’s Data Visualization book."
  },
  {
    "objectID": "intro.html#what-is-exploratory-data-analysis-eda",
    "href": "intro.html#what-is-exploratory-data-analysis-eda",
    "title": "1  EDA",
    "section": "1.1 What is Exploratory Data Analysis (EDA)?",
    "text": "1.1 What is Exploratory Data Analysis (EDA)?\nTraditional approaches to data analysis tend to be linear and unidirectional. It often starts with the acquisition or collection of a dataset then ends with the computation of some inferential or confirmatory procedure.\n\nUnfortunately, such practice can lead to faulty conclusions. The following datasets generate identical regression analysis results shown in the previous figure yet, they are all completely different!\n\n\n\n\n\nThe four plots represent Francis Anscombe’s famous quartet which he used to demonstrate the importance of visualizing the data before proceeding with traditional statistical analysis. Of the four plots, only the first is a sensible candidate for the regression analysis; the second dataset highlights a nonlinear relationship between X and Y; the third and fourth plots demonstrate the disproportionate influence of a single outlier on the regression procedure.\nThe aforementioned example demonstrates that a sound data analysis workflow must involve data visualization and exploration techniques. Exploratory data analysis seeks to extract salient features about the data (that may have otherwise gone unnoticed) and to help formulate hypotheses. Only then should appropriate statistical tests be applied to the data to confirm a hypothesis.\nHowever, not all EDA workflows result in a statistical test: We may not be seeking a hypothesis or, if a hypothesis is sought we may not have the statistical tools necessary to test the hypothesis. It’s important to realize that most statistical procedures make restrictive assumptions about the data and the type of hypothesis being tested; data sets seldom meet those stringent requirements.\n\n “Exploratory data analysis is an attitude, a flexibility, and a reliance on display, NOT a bundle of techniques.”\n–John Tukey\n\nJohn Tukey is credited with having coined the term exploratory data analysis and with having written the first comprehensive book (Tukey, 19771) on that subject in 1977. The book is still very much relevant today and several of the techniques highlighted in the book will be covered in this course."
  },
  {
    "objectID": "intro.html#the-role-of-graphics-in-eda",
    "href": "intro.html#the-role-of-graphics-in-eda",
    "title": "1  EDA",
    "section": "1.2 The role of graphics in EDA",
    "text": "1.2 The role of graphics in EDA\nThe preceding example highlights the importance of graphing data. A core component of this course is learning how to construct effective data visualization tools for the purpose of revealing patterns in the data. The graphical tools must allow the data to express themselves without imposing a story.\n\n “Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n–William S. Cleveland\n\nWilliam Cleveland has written extensively about data visualization and has focused on principles founded in the field of cognitive neuroscience to improve data graphic designs. His book, Visualizing Data, is a leading authority on statistical graphics and, despite its age, is as relevant today as it was two decades ago. It focuses on graphical techniques (some newer than others) designed to explore the data. This may differ from graphics generated for public dissemination which benefits from another form of data visualization called information visualization (or infovis for short). Infovis will not be covered in this course (though there is some overlap between the two techniques). For a good discussion on the differences between statistical graphics and infovis see the 2011 Statistical Computing and Graphics Newsletter Statistical Graphics and InfoVis: separated Twins at Birth?2\nCleveland has also contributed a very important tool to EDA: the LOESS curve. The LOESS curve will be used extensively in this course. It is one of many fitting options used in smoothing (or detrending) the data. Others include parametric models such as the family of linear polynomials and Tukey’s suite of smoothers notably the running median and the 3RS3R."
  },
  {
    "objectID": "intro.html#the-workhorse-r",
    "href": "intro.html#the-workhorse-r",
    "title": "1  EDA",
    "section": "2.1 The workhorse: R",
    "text": "2.1 The workhorse: R\nR is an open source data analysis and visualization programming environment whose roots go back to the S programming language developed at Bell Laboratories in the 1970’s by John Chambers. It will be used almost exclusively in this course."
  },
  {
    "objectID": "intro.html#the-friendly-interface-rstudio",
    "href": "intro.html#the-friendly-interface-rstudio",
    "title": "1  EDA",
    "section": "2.2 The friendly interface: RStudio",
    "text": "2.2 The friendly interface: RStudio\nRStudio is an integrated development environment (IDE) to R. An IDE provides a user with an interface to a programming environment (like R) by including features such as a source code editor (with colored syntax). RStudio is not needed to use R (which has its own IDE environment–albeit not as nice as RStudio’s), but makes using R far easier. RStudio is an open source software, but unlike R, it’s maintained by a private entity which also distributes a commercial version of RStudio for businesses or individuals needing customer support."
  },
  {
    "objectID": "intro.html#data-manipulation",
    "href": "intro.html#data-manipulation",
    "title": "1  EDA",
    "section": "2.3 Data manipulation",
    "text": "2.3 Data manipulation\nThe importance of data visualization was already discussed in an earlier section. But before one can begin plotting data, one must have a data table in a form ready to be plotted. In cases where the data table consists of just two variables (columns), little data manipulation may be needed, but in cases where data tables consist of tens or scores of variables, data manipulation, subsetting and/or reshaping may be required. Tackling such a task can be challenging in a point and click spreadsheet environment and can introduce clerical error. R offers an array of data table manipulation tools and packages such as tidyr and dplyr. Furthermore, R’s scripting environment enables one to read through each step of a manipulation procedure in a clear and unambiguous way. Imagine the difficulty in properly documenting all the point-and-click steps followed in a spreadsheet environment.\nFor example, a data table of grain production for North America may consist of six variables and 1501 rows. The following table shows just the first 7 lines of the 1501 rows.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nCrop\nInformation\nYear\nValue\nSource\n\n\n\n\nCanada\nBarley\nArea harvested (Ha)\n2012\n2060000.00\nOfficial data\n\n\nCanada\nBarley\nYield (Hg/Ha)\n2012\n38894.66\nCalculated data\n\n\nCanada\nBuckwheat\nArea harvested (Ha)\n2012\n0.00\nFAO estimate\n\n\nCanada\nCanary seed\nArea harvested (Ha)\n2012\n101900.00\nOfficial data\n\n\nCanada\nCanary seed\nYield (Hg/Ha)\n2012\n12161.92\nCalculated data\n\n\nCanada\nGrain, mixed\nArea harvested (Ha)\n2012\n57900.00\nOfficial data\n\n\n\n\n\nThere are many ways in which we may want to summarize the data table. We could, for example, want to compute the total Barley yield for Canada by year for the years ranging from 2005 and 2007. In R, this would be done in just a few lines of code:\n\nlibrary(dplyr) \ndat2 <- fao %>% \n    filter(Information == \"Yield (Hg/Ha)\",  Crop==\"Barley\", Country==\"Canada\",\n           Year >= 2005, Year <=2010) %>%  \n    group_by(Year) %>% \n    summarise(Barley_yield = round(median(Value))) \n\n\n\n\n\n \n  \n    Year \n    Barley_yield \n  \n \n\n  \n    2005 \n    32134 \n  \n  \n    2006 \n    29703 \n  \n  \n    2007 \n    27476 \n  \n  \n    2008 \n    33646 \n  \n  \n    2009 \n    32620 \n  \n  \n    2010 \n    31859 \n  \n\n\n\n\n\nOn the other hand, creating the same output in a spreadsheet environment would take a bit more effort and its workflow would be less transparent."
  },
  {
    "objectID": "intro.html#reproducible-analysis",
    "href": "intro.html#reproducible-analysis",
    "title": "1  EDA",
    "section": "2.4 Reproducible analysis",
    "text": "2.4 Reproducible analysis\nData table manipulation is inevitable in any data analysis workflow and, as discussed in the last section, can be prone to clerical errors if performed in a point-and-click environment. Furthermore, reproducing a workflow in a spreadsheet environment can be difficult unless each click and each copy-and-paste operations are meticulously documented. And even if the documentation is adequate, there is no way of knowing if the analyst followed those exact procedures (unless his mouse and keyboard moves were recorded). However, with a scripting environment, each step of a workflow is clearly and unambiguously laid out as demonstrated with the FAO grain data above. This leads to another basic tenet of the scientific method: reproducibility of the workflow.\nReproducible research lends credence to scientific work. The need for reproducibility is not limited to data collection or methodology but includes the actual analytical workflow that generated the results including data table output and statistical tests.\nData analysis can be complex. Each data manipulation step that requires human interaction is prone to clerical error. But error can also manifest itself in faulty implementation of an analytical procedure—both technical and theoretical. Unfortunately, workflows are seldom available in technical reports or peer-reviewed publications where the intended audience is only left with the end product of the analysis.\n\n “… a recent survey of 18 quantitative papers published in Nature Genetics in the past two years found reproducibility was not achievable even in principle for 10.”\n–Keith A. Baggerly & Kevin R. Coombes3\n\nUnfortunately, examples of irreproducible research are all too common. An example of such was reported by the New York Times in an article titled How Bright Promise in Cancer Testing Fell Apart. In 2006, researchers at Duke had published a paper in Nature Medicine on a breakthrough approach to fighting cancer. The authors’ research suggested that genomic tests of a cancer cell’s DNA could be used to target the most effective chemotherapy treatment. This was heralded as a major breakthrough in the fight against cancer. Unfortunately, the analysis presented by the authors was flawed. Two statisticians, Dr. Baggerly and Dr. Coombes, sought to replicate the work but discovered instead that the published work was riddled with problems including mis-labeling of genes and confounding experimental designs. The original authors of the research did not make the analytical workflow available to the public thus forcing the statisticians to scavenge for the original data and techniques. It wasn’t until 5 years later, in 2011, that Nature decided to retract the paper because they were “unable to reproduce certain crucial experiments”.\nMany journals now require or strongly encourage authors to “make materials, data and associated protocols promptly available to readers without undue qualifications” (Nature, 2014). Sharing data file is not too difficult, but sharing the analytical workflow used to generate conclusions can prove to be difficult if the data were run though many different pieces of software and point-and-click procedures. An ideal analytical workflow should be scripted in a human readable way from beginning (the moment the data file(s) is/are read) to the generation of the data tables or data figures used in the report of publication. This has two benefits: elimination of clerical errors (associated with poorly implemented point-and-click procedures) and the exposition of the analytical procedures adopted in the workflow."
  },
  {
    "objectID": "The_R_environment.html#r-vs-rstudio",
    "href": "The_R_environment.html#r-vs-rstudio",
    "title": "2  The R and RStudio Environments",
    "section": "2.1 R vs RStudio",
    "text": "2.1 R vs RStudio\n\n\n\nR and RStudio are two distinctly different applications that serve different purposes. R1 is the software that performs the actual instructions. It’s the workhorse. Without R installed on your computer or server, you would not be able to run any commands.\nRStudio2 is a software that provides an nifty interface to R. It’s sometimes referred to as an Integrated Development Environment (IDE). Its purpose is to provide bells and whistles that can improve your experience with the R software.\n\nRStudio comes in two flavors:\n\nA desktop application that installs directly on your computer;\nA server application that is accessible via a web browser.\n\nBoth platforms offer nearly identical experiences. The former runs on top of R installed on your computers, the latter runs off of an instance of R running on a remote server.\n\n2.1.1 Do I need RStudio to interface with R?\nThe answer is No! Many new users to the R environment conflate R with RStudio. R has been around for decades, long before RStudio was developed. In fact, when you install R on your Windows or Mac computer, you are offered a perfectly functional barebones IDE for R.\n\nR can even be run in a shell environment like Linux:\n\nNote that while you do not need RStudio to run R on your computer, the reverse cannot be said. In other words, RStudio is not functional without an installation of R. You therefore need to install R regardless of whether or not you use RStudio.\n\n\n2.1.2 Which software do I cite?\nYou will normally cite R and not RStudio since RStudio does not contribute to the execution of the code (i.e. an R script will run independently of the version of RStudio or of any other IDE used to interface with R).\nYou can access citation information for R via:\n\ncitation()\n\n\nTo cite R in publications use:\n\n  R Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical\n  Computing, Vienna, Austria. URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2022},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it when using it for data analysis.\nSee also 'citation(\"pkgname\")' for citing R packages."
  },
  {
    "objectID": "The_R_environment.html#command-line-vs.-script-file",
    "href": "The_R_environment.html#command-line-vs.-script-file",
    "title": "2  The R and RStudio Environments",
    "section": "2.2 Command line vs. script file",
    "text": "2.2 Command line vs. script file\n\n2.2.1 Command line\nR can be run from a R console or RStudio command line environment. For example, we can assign four numbers to the object x then have R read out the values stored in x:\n\nx <- c(1,2,3,4)\nx\n\n[1] 1 2 3 4\n\n\n\n\n2.2.2 R script files\nIf you intend on typing more than a few lines of code in a command prompt environment, or if you wish to save a series of commands as part of a project’s analysis, it is probably best that you write and store the commands in an R script file. Such a file is usually saved with a .R extension.\nIn RStudio, you can run a line of code of a R script file by placing a cursor anywhere on that line (while being careful not to highlight any subset of that line) and pressing the shortcut keys Ctrl+Enter on a Windows keyboard or Command+Enter on a Mac.\nYou can also run an entire block of code by selecting all lines to be run then pressing the shortcut keys Ctrl+Enter/Command+Enter. Or, you can run the entire R script by pressing Ctrl+Alt+R in Windows or Command+Option+R on a Mac.\nIn the following example, the R script file has three lines of code: two assignment operations and one regression analysis. The lines are run one at a time using the Ctrl+Enter keys and the output is displayed in the console window."
  },
  {
    "objectID": "The_R_environment.html#the-assignment-operator--",
    "href": "The_R_environment.html#the-assignment-operator--",
    "title": "2  The R and RStudio Environments",
    "section": "2.3 The assignment operator <-",
    "text": "2.3 The assignment operator <-\nWhen assigning values or output from operations to a variable, the assignment operator, <-, is placed between the variable name and the value(s) being assigned to that variable. In the preceding example, the values 1,2,3,4 were being assigned to x. The assignment operator is constructed by combining the less then character, <, with the dash character, -. Given that the assignment operator will be used frequently in an R script, it may be worthwhile to learn its shortcut: Alt+- on Windows and Option + - on a Mac.\nNote that, in most cases, you can also use the = to assign values as in:\nx = c(1,2,3,4)\nHowever, this option is not widely adopted in the R community. An advantage in using <- instead of = is in readability. The <- operator makes it easier to spot assignments during a quick visual scan of an R script, more so than the = operator which is also used in functions when assigning variables to function parameters as in:\nM <- lm(y ~ x, data = dat, weights = wt)  \nThe alternative would be:\nM = lm(y ~ x, data = dat, weights = wt)   \nNotice how the assignment of M does not stand out as well in the second example given the recurrence of = on the same line of code (unless, of course, if you benefit from colored syntax)."
  },
  {
    "objectID": "The_R_environment.html#understanding-directory-structures",
    "href": "The_R_environment.html#understanding-directory-structures",
    "title": "2  The R and RStudio Environments",
    "section": "2.4 Understanding directory structures",
    "text": "2.4 Understanding directory structures\nBecause a data file may reside in a different directory than that which houses the R script calling it, you need to explicitly instruct R on how to access that file from the R session’s working directory.\nIn the example that follows, user Jdoe has a project folder called Project1 in which reside a ./Data folder and an ./Analysis folder.\n\nHe opens the R script called Data_manipulation.R from the Analysis folder which contains the following line of code:\n\ndat <- read.csv(\"ACS.csv\")\n\nHe runs that line of code and R returns the following error message:\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nThe error message states that the file ACS.csv cannot be found. This is because the session’s working directory is probably set to a directory other than the D:/Jdoe/Project1/Data directory which houses the data file. An R session’s working directory can be verified by typing the following command:\n\ngetwd()\n\n[1] \"D:/jdoe/Project1/Analysis\"\nThe working directory is used to instruct R where to look for a file (or where to create one) if the directory path is not explicitly defined. So in the above example, user Jdoe is asking R to open the file ACS.csv without explicitly telling R in which directory to look so R is defaulting to the current working directory which is D:/jdoe/Project1/Analysis which does not contain the file ACS.csv.\nThere are two options to resolving this problem. The first is to set the working directory to the folder that contains the ACS.csv file using the setwd() function.\n\nsetwd(\"D:/Jdoe/Project1/Data\")\n\nThe second is to modify the read.csv call by specifying the path to the ACS.csv file.\n\ndat <- read.csv(\"D:/Jdoe/Project1/Data/ACS.csv\")\n\nHowever, this approach makes it difficult to share the project folder with someone else who may choose to place it under a different folder such as C:\\User\\John\\Documents\\Project1\\. In such a scenario, the user would need to modify every R script that references the directory D:\\Jdoe\\Project1\\. A better solution is to specify the location of the data folder relative to the location of the Analysis folder such as,\n\ndat <- read.csv(\"../Data/ACS.csv\")\n\nThe two dots, .., tells R to move up the directory hierarchy relative to the current working directory. So in our working example, ../ tells R to move out of the Analysis/ folder and up into the Project1/ folder. The relative path ../Data/ACS.csv tells R to move out of the Analysis/ directory and over into the Data/ directory before attempting to read the contents of the ACS.csv data file.\nUsing relative paths makes your project folder independent of the full directory structure in which it resides thus facilitating the reproducibility of your work on a different computer or root directory environment. Assume that the user of your code will set the working directory to the project folder."
  },
  {
    "objectID": "The_R_environment.html#packages",
    "href": "The_R_environment.html#packages",
    "title": "2  The R and RStudio Environments",
    "section": "2.5 Packages",
    "text": "2.5 Packages\nOne of R’s attractive features is its rich collection of packages designed for specific applications and techniques. Packages allow researchers and scientists to share R functions and data with other users. Some packages come already installed with R, others must be downloaded separately from a CRAN repository or other locations such as GitHub or personal websites.\n\n2.5.1 Base packages\nR comes installed with a set of default packages. A snapshot of a subset of the installed base packages is shown below:"
  },
  {
    "objectID": "The_R_environment.html#installing-packages-from-cran",
    "href": "The_R_environment.html#installing-packages-from-cran",
    "title": "2  The R and RStudio Environments",
    "section": "2.6 Installing packages from CRAN",
    "text": "2.6 Installing packages from CRAN\nThere are thousands of R packages to choose from. Most can be accessed from the CRAN repository. To install a CRAN package from within RStudio, click on the Packages tab, select Install and choose Repository (CRAN) as the source location. In the following example, the library ggplot2 is installed from CRAN.\n\nPackage installation from CRAN’s repository can also be accomplished using the following line of code:\n\ninstall.packages(\"ggplot2\")\n\nThe installation is usually straightforward and if any other packages need to be installed, RStudio will install those as well as long as the Install dependencies option is checked. In the previous example, ggplot2 requires that a dozen or so packages be present on your computer (such as RColorBrewer and reshape2)–all of which are automatically installed by RStudio.\nNote that R packages are installed in the user’s home directory (C:/Users/…) by default. This is advantageous in that you do not need to have administrative privileges to install any packages. But it can be a disadvantage in that if someone else logs on to the same computer where you installed a package, that person will not have access to it requiring that she install that package in her home directory thereby duplicating an instance of that same package on the same computer.\n\n2.6.1 Installing packages from GitHub\nSome packages may be in development and deemed not mature enough to reside on the CRAN repository. Such packages are often found on GitHub–a website that hosts software projects. Installing a package from GitHub requires the use of another package called devtools available on CRAN.\nFor example, to install the latest version of ggplot2 from GitHub (i.e. the developmental version and not the stable version available on CRAN) type the following:\n\ninstall.packages(\"devtools\")  # Install the devtools package if not already present\nlibrary(devtools)             # Load the devtools package in the current R session\ninstall_github(\"tidyverse/ggplot2\")\n\nThe argument tidyverse points to the name of the repository and ggplot2 to the name of the package.\n\n\n2.6.2 Using a package in a R session\nJust because a package is installed on your computer (in your home directory or in a directory accessible to you) does not mean that you have access to its functions. For example, after installing the ggplot2 library you might want to use one of its functions, qplot, to generate a scatter plot,\n\nggplot(mtcars, aes(mpg, wt)) + geom_point()\n\nonly to see the following error message:\n\n\nError in ggplot(mtcars, aes(mpg, wt)): could not find function \"ggplot\"\n\n\nThis is because the contents of the ggplot2 package have not been loaded into the current R session. To make the functions and/or data of a package available to an existing R session, you must load its content using the library() function:\n\nlibrary(ggplot2)\n\nOnce the package is loaded in the current R session, you should have full access to its functions and datasets.\n\nqplot(mpg, wt, data=mtcars)"
  },
  {
    "objectID": "The_R_environment.html#getting-a-sessions-info",
    "href": "The_R_environment.html#getting-a-sessions-info",
    "title": "2  The R and RStudio Environments",
    "section": "2.7 Getting a session’s info",
    "text": "2.7 Getting a session’s info\nReproducibility is a fundamental idea behind an open source analysis environment such as R. So it’s only fitting that all aspects of your analysis environment be made available (along with your data and analysis results). This is because functions and programming environments may change in their behavior as versions evolve; this may be by design or the result of a bug in the code fixed in later versions. No piece of software, open-source or commercial, is immune to this. It’s therefore important that you publicize the R session used in your analysis. A simple way to do this is to call the sessionInfo() function.\n\nsessionInfo() \n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8  LC_CTYPE=English_United States.utf8    LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                           LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0\n\nloaded via a namespace (and not attached):\n [1] rstudioapi_0.14   knitr_1.42        magrittr_2.0.3    tidyselect_1.2.0  munsell_0.5.0     colorspace_2.1-0 \n [7] R6_2.5.1          rlang_1.0.6       fastmap_1.1.0     fansi_1.0.4       dplyr_1.0.10      tools_4.2.2      \n[13] grid_4.2.2        gtable_0.3.1      xfun_0.36         utf8_1.2.2        cli_3.6.0         withr_2.5.0      \n[19] htmltools_0.5.4   digest_0.6.31     tibble_3.1.8      lifecycle_1.0.3   farver_2.1.1      htmlwidgets_1.6.1\n[25] vctrs_0.5.2       codetools_0.2-18  glue_1.6.2        evaluate_0.20     rmarkdown_2.20    labeling_0.4.2   \n[31] compiler_4.2.2    pillar_1.8.1      generics_0.1.3    scales_1.2.1      jsonlite_1.8.4    pkgconfig_2.0.3  \n\n\nOutput includes all loaded base packages and external packages (e.g. ggplot2 in this working example) as well as their version."
  },
  {
    "objectID": "data_objects.html#core-data-types",
    "href": "data_objects.html#core-data-types",
    "title": "3  Data Object Type and Structure",
    "section": "3.1 Core data types",
    "text": "3.1 Core data types\nThese data types, or modes, define how the values are stored in the computer. You can get an object’s mode using the typeof() function. Note that R also has a built-in mode() function that will serve the same purpose with the one exception in that it will not distinguish integers from doubles.\n\n3.1.1 Numeric\nThe numeric data type is probably the simplest. It consists of numbers such as integers (e.g. 1 ,-3 ,33 ,0) or doubles (e.g. 0.3, 12.4, -0.04, 1.0). For example, to create a numeric (double) vector we can type:\n\nx <- c(1.0, -3.4, 2, 140.1)\nmode(x)\n\n[1] \"numeric\"\n\n\nTo assess if the number is stored as an integer or a double use the typeof() function.\n\ntypeof(x)\n\n[1] \"double\"\n\n\nNote that removing the fraction part of a number when creating a numeric object does not necessarily create an integer. For example, creating what seems to be an integer object returns double when queried by typeof():\n\nx <- 4\ntypeof(x)\n\n[1] \"double\"\n\n\nTo force R to recognize a value as an integer add an upper case L to the number.\n\nx <- 4L\ntypeof(x)\n\n[1] \"integer\"\n\n\n\n\n3.1.2 Character\nThe character data type consists of letters or words such as \"a\", \"f\", \"project\", \"house value\".\n\nx <- c(\"a\", \"f\", \"project\", \"house value\")\ntypeof(x)\n\n[1] \"character\"\n\n\nCharacters can also consist of numbers represented as characters. The distinction between a character representation of a number and a numeric one is important. For example, if we have two numeric vectors x and y such as\n\nx <- 3\ny <- 5.3\n\nand we choose to sum the two variables, we get:\n\nx + y\n\n[1] 8.3\n\n\nIf we repeat these steps but instead choose to represent the numbers 3 and 5.3 as characters we get the following error message:\n\nx <- \"3\"\ny <- \"5.3\"\nx + y\n\nError in x + y: non-numeric argument to binary operator\n\n\nNote the use of quotes to force numbers to character mode.\n\n\n3.1.3 Logical\nLogical values can take on one of two values: TRUE or FALSE. These can also be represented as 1 or 0. For example, to create a logical vector of 4 elements, you can type\n\nx <- c(TRUE, FALSE, FALSE, TRUE)\n\nor\n\nx <- as.logical(c(1,0,0,1))\n\nNote that in both cases, typeof(x) returns logical. Also note that the 1’s and 0’s in the last example are converted to TRUE’s and FALSE’s internally."
  },
  {
    "objectID": "data_objects.html#naming-r-objects",
    "href": "data_objects.html#naming-r-objects",
    "title": "3  Data Object Type and Structure",
    "section": "3.2 Naming R objects",
    "text": "3.2 Naming R objects\nYou can use any combination of alphanumeric characters, along with dots and underscores, to name an R object. But there are a few exceptions:\n\nNames cannot start with a number;\nNames cannot have spaces;\nNames cannot be a standalone number such as 12 or 0.34;\nNames cannot be a reserved word such as if, else, function, TRUE, FALSE and NULL just to name a few (to see the full list of reserved words, type ?Reserved).\n\nExamples of valid names include a, dat2, cpi_index, .tmp, and even a standalone dot . (though a dot can make reading code difficult under certain circumstances).\nExamples of invalid names include 1dat, dat 2 (note the space between dat and 2), df-ver2 (the dash is treated as a mathematical operator), and Inf (the latter is a reserved word listed in the ?Reserved help document).\nYou can mix cases, but use upper cases with caution since some letters look very much the same in both lower and upper cases (e.g. s and S)."
  },
  {
    "objectID": "data_objects.html#derived-data-types",
    "href": "data_objects.html#derived-data-types",
    "title": "3  Data Object Type and Structure",
    "section": "3.3 Derived data types",
    "text": "3.3 Derived data types\nWe’ve learned that data are stored as either numeric, character or logical, but they can carry additional attribute information that allow these objects to be treated in special ways by certain functions in R. These attributes define an object’s class and can be extracted from that object via the class() function.\n\n3.3.1 Factor\nFactors are normally used to group variables into a fixed number of unique categories or levels. For example, a dataset may be grouped by gender or month of the year. Such data are usually loaded into R as a numeric or character data type requiring that they be converted to a factor using the as.factor() function.\nIn the following chunk of code, we create a factor from a character object.\n\na      <- c(\"M\", \"F\", \"F\", \"U\", \"F\", \"M\", \"M\", \"M\", \"F\", \"U\")\na.fact <- as.factor(a)\n\nNote that a is of character data type.\n\ntypeof(a)\n\n[1] \"character\"\n\n\nHowever, the derived object a.fact is now stored as an integer!\n\ntypeof(a.fact)\n\n[1] \"integer\"\n\n\nYet, when displaying the contents of a.fact we see character values.\n\na.fact\n\n [1] M F F U F M M M F U\nLevels: F M U\n\n\nHow can this be? Well, a.fact is a more complicated object than the simple objects created thus far in that the factor is storing additional information not seen in its output. This hidden information is stored in attributes. To view these hidden attributes, use the attributes function.\n\nattributes(a.fact)\n\n$levels\n[1] \"F\" \"M\" \"U\"\n\n$class\n[1] \"factor\"\n\n\nThere are two attributes: levels and class. The levels attribute lists the three unique values in a.fact. The order in which these levels are listed reflect their numeric representation. So in essence, a.fact is storing each value as an integer that points to one of the three unique levels.\n\nSo why doesn’t R output the integer values when we output a.fact? To understand why, we first need to know that when we call the object name, R is wrapping that object name with the print command, so the following lines of code are identical.\n\na.fact\nprint(a.fact)\n\nThe print function then looks for a class attribute in the object. The class type instructs the print function on how to generate the output. Since a.fact has a factor class attribute, the print function is instructed to replace the integer values with the level “tags”.\nNaturally, this all happens behind the scenes without user intervention.\nAnother way to determine a.fact’s class type is to call the class function.\n\nclass(a.fact)\n\n[1] \"factor\"\n\n\nThe unique levels of a factor, and the order in which they are stored can be extracted using the levels function.\n\nlevels(a.fact)\n\n[1] \"F\" \"M\" \"U\"\n\n\nRemember, the order in which the levels are displayed match their integer representation.\nNote that if a class attribute is not present, the class function will return the object’s data type (though it will not distinguish between integer and double).\n\nclass(a)\n\n[1] \"character\"\n\n\nIn such a case, the object is treated as a generic element.\nTo appreciate the benefits of a factor we’ll first create a dataframe (dataframes are data tables whose structure will be covered later in this tutorial). One column will be assigned the a.fact factor and another will be assigned some random numeric values.\n\nx      <- c(166, 47, 61, 148, 62, 123, 232, 98, 93, 110)\ndat    <- data.frame(x = x, gender = a.fact)\ndat\n\n     x gender\n1  166      M\n2   47      F\n3   61      F\n4  148      U\n5   62      F\n6  123      M\n7  232      M\n8   98      M\n9   93      F\n10 110      U\n\n\nThe gender column is now a factor with three levels: F, M and U. We can use the str() function to view the dataframe’s structure as well as its columns classes.\n\nstr(dat)\n\n'data.frame':   10 obs. of  2 variables:\n $ x     : num  166 47 61 148 62 123 232 98 93 110\n $ gender: Factor w/ 3 levels \"F\",\"M\",\"U\": 2 1 1 3 1 2 2 2 1 3\n\n\nMany functions other than print will recognize factor data types and will allow you to split the output into groups defined by the factor’s unique levels. For example, to create three boxplots of the value x, one for each gender group F, M and U, type the following:\n\nboxplot(x ~ gender, dat, horizontal = TRUE)\n\n\n\n\nThe tilde ~ operator is used in the plot function to split (or condition) the data into separate plots based on the factor gender.\nFactors will prove to be quite useful in many analytical and graphical procedures as we’ll see in subsequent sections.\n\n3.3.1.1 Rearranging level order\nA factor will define a hierarchy for its levels. When we invoked the levels function in the last example, you may have noted that the levels output were ordered F, M andU–this is the level hierarchy defined for gender (i.e. F>M>U ). This means that regardless of the order in which the factors appear in a table, anytime a plot or operation is conditioned by the factor, the grouped elements will appear in the order defined by the levels’ hierarchy. When we created the boxplot from our dat object, the plotting function ordered the boxplot (bottom to top) following gender’s level hierarchy (i.e. F first, then M, then U).\nIf we wanted the boxplots to be plotted in a different order (i.e. U first followed by F then M) we would need to modify the gender column by recreating the factor object as follows:\n\ndat$gender <- factor(dat$gender, levels=c(\"U\",\"F\",\"M\"))\nstr(dat)\n\n'data.frame':   10 obs. of  2 variables:\n $ x     : num  166 47 61 148 62 123 232 98 93 110\n $ gender: Factor w/ 3 levels \"U\",\"F\",\"M\": 3 2 2 1 2 3 3 3 2 1\n\n\nThe factor function is given the original factor values (dat$gender) but is also given the levels in the new order in which they are to appear(levels=c(\"U\",\"F\",\"M\")). Now, if we recreate the boxplot, the plot order (plotted from bottom to top) will reflect the new level hierarchy.\n\nboxplot(x ~ gender, dat, horizontal = TRUE)\n\n\n\n\n\n\n3.3.1.2 Subsetting table by level and dropping levels\nIn this example, we can subset the table by level using the subset function. For example, to subset the values associated with F and M, type:\n\ndat.f <- subset(dat, gender == \"F\" | gender == \"M\")\ndat.f\n\n    x gender\n1 166      M\n2  47      F\n3  61      F\n5  62      F\n6 123      M\n7 232      M\n8  98      M\n9  93      F\n\n\nThe double equality sign == differs from the single equality sign = in that the former asses a condition: it checks if the variable to the left of == equals the variable to the right.\nHowever, if you display the levels associated with this new dataframe, you’ll still see the level U even though it no longer exists in the gender column.\n\nlevels(dat.f$gender)\n\n[1] \"U\" \"F\" \"M\"\n\n\nThis can be a nuisance when plotting the data subset.\n\nboxplot(x ~ gender, dat.f, horizontal = TRUE)\n\n\n\n\nEven though no records are available for U, the plot function allocates a slot for that level. To resolve this, we can use the droplevels function to remove all unused levels.\n\ndat.f$gender <- droplevels(dat.f$gender)\nlevels(dat.f$gender)\n\n[1] \"F\" \"M\"\n\n\n\nboxplot(x ~ gender, dat.f, horizontal = TRUE)\n\n\n\n\n\n\n\n3.3.2 Date\nDate values are stored as numbers. But to be properly interpreted as a date object in R, their attribute must be explicitly defined as a date. R provides many facilities to convert and manipulate dates and times, but a package called lubridate makes working with dates/times much easier. A separate chapter is dedicated to the creation and manipulation of date objects.\n\n\n3.3.3 NA and NULL\nYou will find that many data files contain missing or unknown values. It may be tempting to assign these missing or unknown values a 0 but doing so can lead to many undesirable results when analyzing the data. R has two placeholders for such elements: NA and NULL.\nFor example, let’s say that we made four measurements where the second measurement was not available but we wanted that missing value to be recorded in our table, we would encode that missing value as follows:\n\nx <- c(23, NA, 1.2, 5)\n\nNA (Not Available) is a missing value indicator. It suggests that a value should be present but is unknown.\nThe NULL object also represents missing values but its interpretation is slightly different in that it suggests that the value does not exist or that it’s not measurable.\n\ny <- c(23, NULL, 1.2, 5)\n\nThe difference between NA and NULL may seem subtle, but their interpretation in some functions can lead to different outcomes. For example, when computing the mean of x, R returns an NA value:\n\nmean(x)\n\n[1] NA\n\n\nThis serves as a check to remind the user that one of the elements is missing. This can be overcome by setting the na.rm parameter to TRUE as in mean(x, na.rm=T) in which case R ignores the missing value.\nA NULL object, on the other hand, is treated differently. Since NULL implies that a value should not be present, R no longer feels the need to treat such element as questionable and allows the mean value to be computed:\n\nmean(y)\n\n[1] 9.733333\n\n\nIt’s more common to find data tables with missing elements populated with NA’s than NULL’s so unless you have a specific reason to use NULL as a missing value placeholder, use NA instead.\n\n3.3.3.1 NA data types\nNA has different data types. By default, it’s a logical variable.\n\ntypeof(NA)\n\n[1] \"logical\"\n\n\nBut it can be coerced to other types/classes such as character.\n\ntypeof( as.character(NA))\n\n[1] \"character\"\n\n\nIt can also be coerced to a derived data type such as a date.\n\nclass( as.Date(NA))\n\n[1] \"Date\"\n\n\nNote the use of class instead of typeof (recall that a date object is stored as a number but has a Date class attribute).\nAlternatively, you can make use of built-in reserved words such as NA_character_ and NA_integer_. Note that there is no reserved word for an NA date type.\n\ntypeof(NA_character_)\n\n[1] \"character\"\n\ntypeof(NA_integer_)\n\n[1] \"integer\"\n\n\nThe distinction between NA types can be important in certain settings. Examples of these will be highlighted in section 9.3."
  },
  {
    "objectID": "data_objects.html#data-structures",
    "href": "data_objects.html#data-structures",
    "title": "3  Data Object Type and Structure",
    "section": "3.4 Data structures",
    "text": "3.4 Data structures\nMost datasets we work with consist of batches of values such as a table of temperature values or a list of survey results. These batches are stored in R in one of several data structures. These include (atomic) vectors, matrices, data frames and lists.\n\n\n3.4.1 (Atomic) Vectors\nThe atomic vector (or vector for short) is the simplest data structure in R which consists of an ordered set of values of the same type and or class (e.g. numeric, character, date, etc…). A vector can be created using the combine function c() as in\n\nx <- c(674 , 4186 , 5308 , 5083 , 6140 , 6381)\nx\n\n[1]  674 4186 5308 5083 6140 6381\n\n\nA vector object is an indexable collection of values which allows one to access a specific index number. For example, to access the third element of x, type:\n\nx[3]\n\n[1] 5308\n\n\nYou can also select a subset of elements by index values using the combine function c().\n\nx[c(1,3,4)]\n\n[1]  674 5308 5083\n\n\nOr, if you are interested in a range of indexed values such as index 2 through 4, use the : operator.\n\nx[2:4]\n\n[1] 4186 5308 5083\n\n\nYou can also assign new values to a specific index. For example, we can replace the second value in vector x with 0.\n\nx[2] <- 0\nx\n\n[1]  674    0 5308 5083 6140 6381\n\n\nNote that a vector can store any data type such as characters.\n\nx <- c(\"all\", \"b\", \"olive\")\nx\n\n[1] \"all\"   \"b\"     \"olive\"\n\n\nHowever, a vector can only be of one type. For example, you cannot mix numeric and character types as follows:\n\nx <- c( 1.2, 5, \"Rt\", \"2000\")\n\nIn such a situation, R will convert the element types to the highest common mode following the order NULL < logical < integer < double < character. In our working example, the elements are coerced to character:\n\ntypeof(x)\n\n[1] \"character\"\n\n\n\n\n3.4.2 Matrices and arrays\nMatrices in R can be thought of as vectors indexed using two indices instead of one. For example, the following line of code creates a 3 by 3 matrix of randomly generated values. The parameters nrow and ncol define the matrix dimension and the function runif() generates the nine random numbers that populate this matrix.\n\nm <- matrix(runif(9,0,10), nrow = 3, ncol = 3)\nm\n\n         [,1]       [,2]      [,3]\n[1,] 6.432146 3.95568744 0.4775159\n[2,] 5.129415 7.86956812 6.8602402\n[3,] 4.428387 0.02690613 5.7419993\n\n\nIf a higher dimension vector is desired, then use the array() function to generate the n-dimensional object. A 3x3x3 array can be created as follows:\n\nm <- array(runif(27,0,10), c(3,3,3))\nm\n\n, , 1\n\n         [,1]     [,2]     [,3]\n[1,] 5.980109 5.879841 1.040604\n[2,] 8.287531 1.535440 7.907427\n[3,] 6.112140 5.325980 9.314898\n\n, , 2\n\n         [,1]      [,2]     [,3]\n[1,] 8.092936 0.6341535 3.262427\n[2,] 8.306673 9.3903959 5.535109\n[3,] 1.199712 8.9266883 5.653008\n\n, , 3\n\n         [,1]      [,2]     [,3]\n[1,] 1.453354 6.7173773 4.869986\n[2,] 2.193595 3.5314945 9.318592\n[3,] 7.511607 0.1554047 4.359003\n\n\nMatrices and arrays can store numeric or character data types, but they cannot store both. This is not to say that you can’t have a matrix of the kind\n\n\n     [,1] [,2]\n[1,] \"a\"  \"2\" \n[2,] \"b\"  \"4\" \n\n\nbut the value 2 and 4 are no longer treated as numeric values but as character values instead.\n\n\n3.4.3 Data frames\nA data frame is what comes closest to our perception of a traditional data table. Unlike a matrix, a data frame can mix data types across columns (e.g. both numeric and character columns can coexist in a data frame) but data type remains the same across rows.\n\nname   <- c(\"a1\", \"a2\", \"b3\")\nvalue1 <- c(23, 4, 12)\nvalue2 <- c(1, 45, 5)\ndat    <- data.frame(name, value1, value2)\ndat\n\n  name value1 value2\n1   a1     23      1\n2   a2      4     45\n3   b3     12      5\n\n\nTo view each column’s data type use the structure str function.\n\nstr(dat)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"a1\" \"a2\" \"b3\"\n $ value1: num  23 4 12\n $ value2: num  1 45 5\n\n\nYou’ll notice that the value1 and value2 columns are stored as numeric (i.e. as doubles) and not as integer. There is some inconsistency in R’s characterization of data type. Here, numeric represents double whereas an integer datatype would display integer. For example:\n\nvalue2 <- c(1L, 45L, 5L)\ndat    <- data.frame(name, value1, value2)\nstr(dat)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"a1\" \"a2\" \"b3\"\n $ value1: num  23 4 12\n $ value2: int  1 45 5\n\n\nLike a vector, elements of a data frame can be accessed by their index (aka subscripts). The first index represents the row number and the second index represents the column number. For example, to list the second row of the third column, type:\n\ndat[2, 3]\n\n[1] 45\n\n\nIf you wish to list all rows for columns one through two leave the first index blank:\n\ndat[ , 1:2 ]\n\n  name value1\n1   a1     23\n2   a2      4\n3   b3     12\n\n\nor if you wish to list the third row for all columns, leave the second index blank:\n\ndat[ 3 , ]\n\n  name value1 value2\n3   b3     12      5\n\n\nYou can also reference columns by their names if you append the $ character to the dataframe object name. For example, to list the values in the column named value2, type:\n\ndat$value2\n\n[1]  1 45  5\n\n\nDataframes also have attributes:\n\nattributes(dat)\n\n$names\n[1] \"name\"   \"value1\" \"value2\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n\nIt’s class is data.frame. The names attribute lists the column names and can be extracted using the names function.\n\nnames(dat)\n\n[1] \"name\"   \"value1\" \"value2\"\n\n\nIt also has a row.names attribute. Since we did not explicitly define row names, R simply assigned the row number as row names. You can extract the row names using the rownames function.\n\nrownames(dat)\n\n[1] \"1\" \"2\" \"3\"\n\n\nFinally, to get the dimensions of a dataframe (or a matrix), use the dim() function.\n\ndim(dat)\n\n[1] 3 3\n\n\nThe first value returned by the function represents the number of rows (3 rows), the second value returned by the function represents the number of columns (3 columns).\n\n\n3.4.4 Lists\nA list is an ordered set of components stored in a 1D vector. In fact, it’s another kind of vector called a recursive vector where each vector element can be of different data type and structure. This implies that each element of a list can hold complex objects such as matrices, data frames and other list objects too! Think of a list as a single column spreadsheet where each cell stores anything from a number, to a three paragraph sentence, to a five column table.\nA list is constructed using the list() function. For example, the following list consists of 3 components: a two-column data frame (tagged as component A), a two element logical vector (tagged as component B) and a three element character vector (tagged as component D).\n\nA <- data.frame(\n     x = c(7.3, 29.4, 29.4, 2.9, 12.3, 7.5, 36.0, 4.8, 18.8, 4.2),\n     y = c(5.2, 26.6, 31.2, 2.2, 13.8, 7.8, 35.2, 8.6, 20.3, 1.1) )\nB <- c(TRUE, FALSE)\nD <- c(\"apples\", \"oranges\", \"round\")\n\nlst <- list(A = A, B = B, D = D)\n\nYou can view each component’s structure using the str() function.\n\nstr(lst)\n\nList of 3\n $ A:'data.frame':  10 obs. of  2 variables:\n  ..$ x: num [1:10] 7.3 29.4 29.4 2.9 12.3 7.5 36 4.8 18.8 4.2\n  ..$ y: num [1:10] 5.2 26.6 31.2 2.2 13.8 7.8 35.2 8.6 20.3 1.1\n $ B: logi [1:2] TRUE FALSE\n $ D: chr [1:3] \"apples\" \"oranges\" \"round\"\n\n\nEach component of a list can be extracted using the $ symbol followed by that component’s name. For example, to access component A from list lst, type:\n\nlst$A\n\n      x    y\n1   7.3  5.2\n2  29.4 26.6\n3  29.4 31.2\n4   2.9  2.2\n5  12.3 13.8\n6   7.5  7.8\n7  36.0 35.2\n8   4.8  8.6\n9  18.8 20.3\n10  4.2  1.1\n\n\nYou can also access that same component using its numerical index. Since A is the first component in lst, its numerical index is 1.\n\nlst[[1]]\n\n      x    y\n1   7.3  5.2\n2  29.4 26.6\n3  29.4 31.2\n4   2.9  2.2\n5  12.3 13.8\n6   7.5  7.8\n7  36.0 35.2\n8   4.8  8.6\n9  18.8 20.3\n10  4.2  1.1\n\n\nNote that we are using double brackets to extract A. In doing so, we are extracting A in its native data format (a data frame in this example). Had we used single brackets, A would have been extracted as a single component list regardless of its native format. The following compares the different data structure outputs between single and double bracketed indices:\n\nclass(lst[[1]])\n\n[1] \"data.frame\"\n\nclass(lst[1])\n\n[1] \"list\"\n\n\nTo list the names for each component in a list use the names() function:\n\nnames(lst)\n\n[1] \"A\" \"B\" \"D\"\n\n\nNote that components do not require names. For example, we could have created a list as follows (note the omission of A=, B=, etc…):\n\nlst.notags <- list(A, B, D)\n\nListing its contents displays bracketed indices instead of component names.\n\nlst.notags\n\n[[1]]\n      x    y\n1   7.3  5.2\n2  29.4 26.6\n3  29.4 31.2\n4   2.9  2.2\n5  12.3 13.8\n6   7.5  7.8\n7  36.0 35.2\n8   4.8  8.6\n9  18.8 20.3\n10  4.2  1.1\n\n[[2]]\n[1]  TRUE FALSE\n\n[[3]]\n[1] \"apples\"  \"oranges\" \"round\"  \n\n\nWhen lists do not have component names, the names() function will return NULL.\n\nnames(lst.notags)\n\nNULL\n\n\nIt’s usually good practice to assign names to components as these can provide meaningful descriptions of each component.\nYou’ll find that many functions in R return list objects such as the linear regression model function lm. For example, run a regression analysis for vector elements x and y (in data frame A) and save the output of the regression analysis to an object called M:\n\nM <- lm( y ~ x, A)\n\nNow let’s verify M’s data structure. The following shows just the first few lines of the output.\n\nstr(M)\n\n\n\nList of 12\n $ coefficients : Named num [1:2] 0.0779 0.991\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"x\"\n  [list output truncated]\n - attr(*, \"class\")= chr \"lm\"\n...\n\n\nThe contents of the regression model object M consists of 12 components which include various diagnostic statistics, regression coefficients, and residuals. Let’s extract each component’s name:\n\nnames(M)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"          \"fitted.values\" \"assign\"        \"qr\"           \n [8] \"df.residual\"   \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nFortunately, the regression function assigns descriptive names to each of its components making it easier to figure out what most of these components represent. For example, it’s clear that the residuals component stores the residual values from the regression model.\n\nM$residuals\n\n         1          2          3          4          5          6          7          8          9         10 \n-2.1119291 -2.6122265  1.9877735 -0.7516888  1.5332525  0.2898782 -0.5525869  3.7654802  1.5919885 -3.1399416 \n\n\nThe M list is more complex than the simple list lst we created earlier. In addition to having more components, it stores a wider range of data types and structures. For example, element qr is itself a list of five elements!\n\nstr(M$qr)\n\nList of 5\n $ qr   : num [1:10, 1:2] -3.162 0.316 0.316 0.316 0.316 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:10] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:2] \"(Intercept)\" \"x\"\n  ..- attr(*, \"assign\")= int [1:2] 0 1\n $ qraux: num [1:2] 1.32 1.44\n $ pivot: int [1:2] 1 2\n $ tol  : num 0.0000001\n $ rank : int 2\n - attr(*, \"class\")= chr \"qr\"\n\n\nSo if we want to access the element rank in the component qr of list M, we can type:\n\nM$qr$rank\n\n[1] 2\n\n\nIf we want to access rank using indices instead, and noting that qr is the 7th component in list M and that rank is the 5th element in list qr we type:\n\nM[[7]][[5]]\n\n[1] 2\n\n\nThis should illustrate the value in assigning names to list components; not only do the double brackets clutter the expression, but finding the element numbers can be daunting in a complex list structure."
  },
  {
    "objectID": "data_objects.html#coercing-data-from-one-type-to-another",
    "href": "data_objects.html#coercing-data-from-one-type-to-another",
    "title": "3  Data Object Type and Structure",
    "section": "3.5 Coercing data from one type to another",
    "text": "3.5 Coercing data from one type to another\nData can be coerced from one type to another. For example, to coerce the following vector object from character to numeric, use the as.numeric() function.\n\ny   <- c(\"23.8\", \"6\", \"100.01\",\"6\")\ny.c <- as.numeric(y)\ny.c\n\n[1]  23.80   6.00 100.01   6.00\n\n\nThe as.numeric function forces the vector to a double (you could have also used the as.double function). If you convert y to an integer, R will remove all fractional parts of the number.\n\nas.integer(y)\n\n[1]  23   6 100   6\n\n\nTo convert a number to a character use as.character().\n\nnumchar <- as.character(y.c)\nnumchar\n\n[1] \"23.8\"   \"6\"      \"100.01\" \"6\"     \n\n\nYou can also coerce a number or character to a factor.\n\nnumfac <- as.factor(y)\nnumfac\n\n[1] 23.8   6      100.01 6     \nLevels: 100.01 23.8 6\n\ncharfac <- as.factor(y.c)\ncharfac\n\n[1] 23.8   6      100.01 6     \nLevels: 6 23.8 100.01\n\n\nThere are many other coercion functions in R, a summary of some the most common ones we’ll be using in this course follows:\n\n\n\n\n\n\n\nas.character()\nConvert to character\n\n\nas.numeric() or as.double()\nConvert to double\n\n\nas.integer()\nConvert to integer\n\n\nas.factor()\nConvert to factor\n\n\nas.logical()\nConvert to a Boolean\n\n\n\n\n3.5.1 A word of caution when converting from factors\nIf you need to coerce a factor whose levels represent characters to a character data type, use the as.character() function.\n\nchar <- as.character(charfac)\nclass(char)\n\n[1] \"character\"\n\n\nNumbers stored as factors can also be coerced back to numbers, but be careful, the following will not produce the expected output:\n\nnum <- as.numeric(numfac)\nnum\n\n[1] 2 3 1 3\n\n\nThe output does not look like a numeric representation of the original elements in y. Instead it lists the integers that point to the set of unique levels (see the earlier section on factors). To see the unique levels in numfac, type:\n\nlevels(numfac)\n\n[1] \"100.01\" \"23.8\"   \"6\"     \n\n\nThere are three unique values in our vector. Recall that the order in which the levels appear in the above output represents the ordered pointer values. So 1 points to level 100.01, 2 points to level 23.8, and 3 points to level 6.\nSo, to extract the actual values (and not the pointers), you must first convert the factor to character before converting to a numeric vector.\n\nnum <- as.numeric(as.character(numfac))\nnum\n\n[1]  23.80   6.00 100.01   6.00"
  },
  {
    "objectID": "data_objects.html#adding-metadata-to-objects",
    "href": "data_objects.html#adding-metadata-to-objects",
    "title": "3  Data Object Type and Structure",
    "section": "3.6 Adding metadata to objects",
    "text": "3.6 Adding metadata to objects\nEarlier, you were introduced to object attributes. Attributes can be thought of as ancillary information attached to a data object’s header. In other words, it is not part of the set of values stored in the object. If one or more attributes is/are present in the data object, they can be revealed using the attributes command. We already explored the attributes associated with a factor object. For example. a.fact has two attributes: levels and class.\n\nattributes(a.fact)\n\n$levels\n[1] \"F\" \"M\" \"U\"\n\n$class\n[1] \"factor\"\n\n\nIf an attribute is not explicitly defined for a data object, then a NULL is returned.\n\nattributes(a)\n\nNULL\n\n\nYou can create your own attribute name, but it’s best to avoid attribute names commonly used in R such as class, dim, dimnames, names, row.names.\nAn object’s attribute is usually a part of a data object that a user does not need to explictly deal with, but it can be leveraged to store metadata (data documentation). R has a reserved attribute name, comment, that can be used for this purpose. You can add this attribute using the attr function.\n\nattr(a, \"comment\")  <- \"a batch of character values\"\n\nNote that R has a special function for the comment attribute which can be used to extract the comment attribute value.\n\ncomment(a) \n\n[1] \"a batch of character values\"\n\n\nYou can also use the comment function to set the comment value.\n\ncomment(a) <- \"comment added using the comment function\"\ncomment(a) \n\n[1] \"comment added using the comment function\"\n\n\nYou can, of course, create your own attribute. For example, to create an attribute named dim.units and assigning it the value \"meters\" to the object y.c created earlier in this tutorial, type:\n\nattr(y.c, \"dim.units\") <-  \"meters\"\n\nNote that many R packages will also implement their own set of attribute names. So you should familiarize yourself with the packages used in a worklfow before defining your own attribute name."
  },
  {
    "objectID": "read_write_files.html#reading-data-files-into-r",
    "href": "read_write_files.html#reading-data-files-into-r",
    "title": "4  Reading and Writing Data Files",
    "section": "4.1 Reading data files into R",
    "text": "4.1 Reading data files into R\nData files can be loaded from the R session’s working directory, from a directory structure relative to the working directory using the single dot . or double dot .. syntax, or (for some file types) directly from a website. The following sections will expose you to a mixture of data file environments. For a refresher on directory structures, review Understanding directory structures.\n\n4.1.1 Reading from a comma delimitted (CSV) file\nA popular data file format (and one that has withstood the test of time) is the text file format where columns are separated by a tab, space or comma. In the following example, R reads a comma delimited file called ACS.csv into a data object called dat.\n\ndat <- read.csv(\"ACS.csv\", header=TRUE)\n\nIf the CSV file resides on a website, you can load the file directly from that site as follows:\n\ndat <- read.csv(\"http://mgimond.github.io/ES218/Data/ACS.csv\", header=TRUE)\n\nNote that not all data file formats can be readily loaded directly from a website in a “read” function without additional lines of code. Examples are given in the next two sub-sections.\nTo read other text formats that use different delimiters invoke the command read.table() and define the type of delimiter using the sep= parameter. For example, to read a tab delimited data file called ACS.txt, run the command read.table(\"ACS.txt\", sep=\"\\t\").\nNote that if a number or a string is identified as being a placeholder for missing values in the data file, you can use the na.strings = parameter in the read.csv function. For example, assume that the word \"missing\" was used in the csv file to denote a missing value, the function would be modified as follows:\n\ndat <- read.csv(\"http://mgimond.github.io/ES218/Data/ACS.csv\", \n                na.strings = \"missing\")\n\nIf more than one value is used as a placeholder for a missing value, you will need to combine the values using the c() operator. For example, if in addition to the word \"missing\" the value of -9999 was used to designate missing values, you would modify the above chunk of code as follows:\n\ndat <- read.csv(\"http://mgimond.github.io/ES218/Data/ACS.csv\", \n                na.strings = c(\"missing\", \"-9999\") )\n\nNote how the number is wrapped in double quotes. Also, note that the na.strings parameter is applied to all columns in the dataframe. So if the word \"missing\" or the number -9999 are valid values for some of the columns, you should not use this option. Instead, you would need to selectively replace the missing values after the dataset is loaded. This option will be explored in subsequent tutorials.\n\n\n4.1.2 Reading from a R data file\nR has its own data file format–it’s usually saved using the .rds extension. To read a R data file, invoke the readRDS() function.\n\ndat <- readRDS(\"ACS.rds\")\n\nAs with a CSV file, you can load a RDS file straight from a website, however, you must first run the file through a decompressor before attempting to load it via readRDS. A built-in decompressor function called gzcon can be used for this purpose.\n\ndat <- readRDS(gzcon(url(\"http://mgimond.github.io/ES218/Data/ACS.rds\")))\n\nThe .rds file format is usually smaller than its text file counterpart and will therefore take up less storage space. The .rds file will also preserve data types and classes such as factors and dates eliminating the need to redefine data types after loading the file.\n\n\n4.1.3 Reading from an Excel file\nA package that does a good job in importing Excel files is readxl. It recognizes most column formats defined by Excel including date formats. However, only one sheet can be loaded at a time. So if multiple Excel sheets are to be worked on, each sheet will need to be loaded into separate dataframe objects.\nIf you don’t have the readxl package installed, install the package as you would any other package via RStudio’s interface or in R using the following command:\n\ninstall.packages(\"readxl\")\n\nIn this example, we will load an Excel data sheet called Discharge which tabulates daily river water discharge. The sample file, Discharge_2004_2014.xlsx, can be downloaded here. The following chunk of code assumes that the Excel file is saved in a folder called Data/ inside the R session’s working directory.\n\nlibrary(readxl)\nxl <- read_excel(\"./Data/Discharge_2004_2014.xlsx\", sheet = \"Discharge\")\n\nNote that the single dot . that precedes the Data/ folder name instructs R to look for the Data/ folder in the current working directory. A later example will show you how to instruct R to look for files and folders outside of the current working directory using two dots .. instead of one.\nAn advantage to using this package for loading Excel files is its ability to preserve data types–including date formatted columns! In the above example, the Excel file has a column called Date which stores the month/day/year data as a date object. We can check that the loaded xl object recognizes the Date column as a date data type:\n\nstr(xl)\n\ntibble [3,866 × 3] (S3: tbl_df/tbl/data.frame)\n $ Date     : POSIXct[1:3866], format: \"2004-06-01\" \"2004-06-02\" \"2004-06-03\" \"2004-06-04\" ...\n $ Discharge: num [1:3866] 6170 6590 6210 7120 6990 6160 5570 4500 4940 4550 ...\n $ Code     : chr [1:3866] \"A\" \"A\" \"A\" \"A\" ...\n\n\nThe Date column is defined as a POSIXct data type; this is the computer’s way of storing dates as the number of seconds since some internal reference date. We would therefore not need to convert the date column as would be the case if the date column was loaded from a CSV file. If such was the case, then the date column would most likely be loaded as a character or factor data type. A more in-depth discussion on date objects and their manipulation in R is covered in the next chapter.\nExcel files can be loaded directly from the web using the following chunk of code:\n\nweb.file <- \"http://mgimond.github.io/ES218/Data/Discharge_2004_2014.xlsx\"\ntmp      <- tempfile(fileext=\".xlsx\")\ndownload.file(web.file,destfile=tmp, mode=\"wb\")\nxl       <-  read_excel(tmp, sheet = \"Discharge\")\n\nInstead of downloading the file into virtual memory, R needs to download the file into a temporary folder before it can open it. However, that temporary file my not be available in a later session, so you will probably need to reload the data if you launch a new R session.\n\n\n4.1.4 Importing data from proprietary data file formats\nIt’s usually recommended that a data file be stored as a CSV or tab delimited file format if compatibility across software platforms is desired. However, you might find yourself in a situation where you have no option but to import data stored in a proprietary format. This requires the use (and installation) of a package called Hmisc. The package will convert the following file formats: SAS (XPT format), SPSS (SAV format) and Stata (dta format). You can install the package on your computer as follows:\n\ninstall.packages(\"Hmisc\")\n\nIn this example, a SAS file of blood pressure from the CDC will be loaded into an object called dat (file documentation can be found here).\n\nlibrary(Hmisc)\ndat <- sasxport.get(\"http://personal.colby.edu/personal/m/mgimond/R/Data/BPX_G.xpt\")\n\nLikewise, to import an SPSS file, use the spss.get() function; and to import a STATA file, use the stata.get() function."
  },
  {
    "objectID": "read_write_files.html#how-to-save-r-objects-to-data-files",
    "href": "read_write_files.html#how-to-save-r-objects-to-data-files",
    "title": "4  Reading and Writing Data Files",
    "section": "4.2 How to save R objects to data files",
    "text": "4.2 How to save R objects to data files\n  \n\n4.2.1 Export to a CSV file\nTo export a data object called dat.sub as a comma delimited file in a folder called Data/ residing at a level above the R session’s working directory, run the following:\n\nwrite.csv(dat.sub, \"../Data/ACS_sub.csv\")\n\nNote that the two dots .. tells the function to get out of the current folder (i.e. moving up a directory) before proceeding down into the Data/ folder.\n\n\n4.2.2 Export to a Rds file\nTo export a data object called dat.sub to a .Rds (R) file format in a folder called Data/ residing at a level above the R session’s working directory, run the following:\n\nsaveRDS(dat.sub, \"../Data/ACS_sub.rds\")"
  },
  {
    "objectID": "read_write_files.html#saving-an-r-session",
    "href": "read_write_files.html#saving-an-r-session",
    "title": "4  Reading and Writing Data Files",
    "section": "4.3 Saving an R session",
    "text": "4.3 Saving an R session\nYou can save an entire R session (which includes all data objects) using the save function.\nTo save all objects, set the list= parameter to ls():\n\nsave(list = ls(), file = \"../Data/ACS_all.Rdata\")\n\nTo save only two R session objects–dat and dat.sub–to a file, pass the list of objects to the list= parameter:\n\nsave(list = c(dat, dat.sub), file = \"../Data/ACS_subset.Rdata\")"
  },
  {
    "objectID": "read_write_files.html#loading-an-r-session",
    "href": "read_write_files.html#loading-an-r-session",
    "title": "4  Reading and Writing Data Files",
    "section": "4.4 Loading an R session",
    "text": "4.4 Loading an R session\nTo load a previously saved R session type:\n\nload(\"../Data/ACS_all.Rdata\")"
  },
  {
    "objectID": "date_objects.html",
    "href": "date_objects.html",
    "title": "5  Working with Dates",
    "section": "",
    "text": "Date values can be represented in tables as numbers or characters. But to be properly interpreted by R as dates, date values should be converted to an R date object class or a POSIXct/POSIXt object class. R provides many facilities to convert and manipulate dates and times, but a package called lubridate makes working with dates/times much easier."
  },
  {
    "objectID": "date_objects.html#creating-datetime-objects",
    "href": "date_objects.html#creating-datetime-objects",
    "title": "5  Working with Dates",
    "section": "5.1 Creating date/time objects",
    "text": "5.1 Creating date/time objects\n  \n\n5.1.1 From complete date strings\nYou can convert many representations of date and time to date objects. For example, let’s create a vector of dates represented as month/day/year character strings,\n\nx <- c(\"06/23/2013\", \"06/30/2013\", \"07/12/2014\")\nclass(x)\n\n[1] \"character\"\n\n\nAt this point, R treats the vector x as characters. To force R to interpret these as dates, use lubridate’s mdy function. mdy will convert date strings where the date elements are ordered as month, day and year.\n\nlibrary(lubridate)\nx.date <- mdy(x)\nclass(x.date)\n\n[1] \"Date\"\n\n\nIf you need to specify the time zone, add the parameter tz=. For example, to specify Eastern Standard Time, type:\n\nx.date <- mdy(x, tz=\"EST\")\nx.date\n\n[1] \"2013-06-23 EST\" \"2013-06-30 EST\" \"2014-07-12 EST\"\n\n\nNote that using the mode or typeof functions will not help us determine if the object is an R date object. This is because a date is stored as a numeric (double) internally. Use the class function instead as shown in the above code chunk.\nThe mdy function can read in date formats that use different delimiters so that mdy(\"06/23/2013\"), mdy(\"06-23-2013\") and mdy(\"06.23.2013\") are parsed exactly the same so long as the order remains month/day/year.\nFor different month/day/year arrangements, other lubridate functions need to be used:\n\n\n\nFunctions\nDate Format\n\n\n\n\ndmy()\nday/month/year\n\n\nymd()\nyear/month/day\n\n\nydm()\nyear/day/month\n\n\n\nIf your data contains both date and time in a “month/day/year hour:minutes:seconds” format use the mdy_hms function.\n\nx <- c(\"06/23/2013 03:45:23\", \"07/30/2013 14:23:00\", \"08/12/2014 18:01:59\")\nx.date <- mdy_hms(x, tz=\"EST\")\nx.date\n\n[1] \"2013-06-23 03:45:23 EST\" \"2013-07-30 14:23:00 EST\" \"2014-08-12 18:01:59 EST\"\n\n\nThe characters _h, _hm or _hms can be appended to any of the four date function names described earlier to accommodate time formats. A few examples follow:\n\nmdy_h(\"6/23/2013 3\", tz=\"EST\") \n\n[1] \"2013-06-23 03:00:00 EST\"\n\ndmy_hm(\"23/6/2013 3:15\", tz=\"EST\")\n\n[1] \"2013-06-23 03:15:00 EST\"\n\nymd_hms(\"2013/06/23 3:15:7\", tz=\"EST\")\n\n[1] \"2013-06-23 03:15:07 EST\"\n\n\nNote that adding a time element to the date object will create POSIXct and POSIXt object classes instead of Date object classes.\n\nclass(x.date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nAlso, if a timezone is not explicitly defined for a time based date, the function assigns UTC ( Universal Coordinated Time).\n\ndmy_hm(\"23/6/2013 3:15\")\n\n[1] \"2013-06-23 03:15:00 UTC\"\n\n\n\n\n5.1.2 Setting and modifying timezones\nR does not maintain its own list of timezone names, instead, it relies on the operating system’s naming convention. To list the supported timezone names for your particular R environment, type:\n\nOlsonNames()\n\nFor example, to select Daylight Savings Time type tz = \"EST5EDT\".\n\nx.date <- mdy_hms(x, tz=\"EST5EDT\")\nx.date\n\n[1] \"2013-06-23 03:45:23 EDT\" \"2013-07-30 14:23:00 EDT\" \"2014-08-12 18:01:59 EDT\"\n\n\n\nclass(x.date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nIf you need to convert the day/time to another timezone, use lubridate’s with_tz() function. For example, to convert x.date from it’s current EST5DST timezone to the US/Alaska time zone, type:\n\nwith_tz(x.date, tzone = \"US/Alaska\") \n\n[1] \"2013-06-22 23:45:23 AKDT\" \"2013-07-30 10:23:00 AKDT\" \"2014-08-12 14:01:59 AKDT\"\n\n\nNote that the with_tz function will change the timestamp to reflect the new time zone. If you simply want to change the time zone definition and not the timestamp, use the tz() function.\n\ntz(x.date) <- \"US/Alaska\"\nx.date\n\n[1] \"2013-06-23 03:45:23 AKDT\" \"2013-07-30 14:23:00 AKDT\" \"2014-08-12 18:01:59 AKDT\"\n\n\n\n\n5.1.3 From separate date elements\nIf your data table splits the date elements into separate vector objects or columns, use the paste function to combine the elements into a single date string before passing it to one of the lubridate functions. Let’s look at an example:\n\ndat1 <- read.csv(\"http://mgimond.github.io/ES218/Data/CO2.csv\")\nhead(dat1)\n\n  Year Month Average Interpolated  Trend Daily_mean\n1 1959     1  315.62       315.62 315.70         -1\n2 1959     2  316.38       316.38 315.88         -1\n3 1959     3  316.71       316.71 315.62         -1\n4 1959     4  317.72       317.72 315.56         -1\n5 1959     5  318.29       318.29 315.50         -1\n6 1959     6  318.15       318.15 315.92         -1\n\n\nThe CO2 dataset has the date split across two columns: Year and Month (both stored as integers). You can combine the columns into a character string using the paste function. For example, if we want to create a “Year-Month” string as in 1959-10, we could type:\n\npaste(dat1$Year,dat1$Month, sep=\"-\")\n\nThe above example uses three arguments: the two objects that are pasted together (i.e. Year and Month) and the sep=\"-\" parameter which fills the gap between both objects with a dash - (by default, paste would have added spaces thus creating strings in the form of 1959 10).\nlubridate does not have a function along the lines of ym to convert just the year-month strings, this requires that we add an artificial day of the month to the string. We’ll choose to add the 15th day of the month as in\n\npaste(dat1$Year, dat1$Month, \"15\", sep=\"-\")\n\nAnd finally, we’ll add a new column called Date to the dat object, and fill that column with the newly created date string wrapped with the ymd function:\n\ndat1$Date <- ymd( paste(dat1$Year, dat1$Month, \"15\", sep=\"-\") )\nhead(dat1)\n\n  Year Month Average Interpolated  Trend Daily_mean       Date\n1 1959     1  315.62       315.62 315.70         -1 1959-01-15\n2 1959     2  316.38       316.38 315.88         -1 1959-02-15\n3 1959     3  316.71       316.71 315.62         -1 1959-03-15\n4 1959     4  317.72       317.72 315.56         -1 1959-04-15\n5 1959     5  318.29       318.29 315.50         -1 1959-05-15\n6 1959     6  318.15       318.15 315.92         -1 1959-06-15\n\n\nThe sep=\"-\" option is not needed with the lubridate function so the last piece of code could have been written as:\n\ndat1$Date <- ymd( paste(dat1$Year, dat1$Month, \"15\") )\n\nTo confirm that the Date column is indeed formatted as a date object type:\n\nstr(dat1)\n\n'data.frame':   721 obs. of  7 variables:\n $ Year        : int  1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 ...\n $ Month       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Average     : num  316 316 317 318 318 ...\n $ Interpolated: num  316 316 317 318 318 ...\n $ Trend       : num  316 316 316 316 316 ...\n $ Daily_mean  : int  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ Date        : Date, format: \"1959-01-15\" \"1959-02-15\" \"1959-03-15\" \"1959-04-15\" ...\n\n\nor you could type,\n\nclass(dat1$Date)\n\n[1] \"Date\"\n\n\nSince we did not add a timezone or a time component to the date object the Date column was assigned a Date class as opposed to the POSIX... class.\n\n\n5.1.4 Padding time values\nThe lubridate functions may expect the time values to consist of a specific number of characters if a delimiter such as : is not present to split the time elements. For example, the following will not generate a valide date/time object:\n\nhrmin <- 712           # Time 7:12\ndate  <- \"2018/03/17\"  # Date \nymd_hm(paste(date, hrmin))\n\n[1] NA\n\n\nOne solution is to pad the time element with 0’s to complete a four character vector (or a six character vector if seconds are part of the time element). We can use the str_pad function from the stringr package to pad the time object (the stringr package is covered in chapter 6).\n\nlibrary(stringr)\nymd_hm(paste(date, str_pad(hrmin, width=4, pad=\"0\")))\n\n[1] \"2018-03-17 07:12:00 UTC\""
  },
  {
    "objectID": "date_objects.html#extracting-date-information",
    "href": "date_objects.html#extracting-date-information",
    "title": "5  Working with Dates",
    "section": "5.2 Extracting date information",
    "text": "5.2 Extracting date information\nIf you want to extract the day of the week from a date vector, use the wday function.\n\nwday(x.date) \n\n[1] 1 3 3\n\n\nIf you want the day of the week displayed as its three letter designation, add the label=TRUE parameter.\n\nwday(x.date, label=TRUE) \n\n[1] Sun Tue Tue\nLevels: Sun < Mon < Tue < Wed < Thu < Fri < Sat\n\n\nYou’ll note that the function returns a factor object with seven levels–one for each day of the week (Sun, Mon, Tue, Wed, Thu, Fri, Sat)–as well as the level hierarchy which will dictate the order in which values will be displayed if grouped by this factor. The levels are not necessarily reflected in the vector elements (only Sun, Tue are present), but the levels are there if we were ever to add addition day elements to this vector.\nThe following table lists functions that extract different elements of a date object.\n\n\n\nFunctions\nExtracted element\n\n\n\n\nhour()\nHour of the day\n\n\nminute()\nMinute of the hour\n\n\nday()\nDay of the month\n\n\nyday()\nDay of the year\n\n\ndecimal_date()\nDecimal year\n\n\nmonth()\nMonth of the year\n\n\nyear()\nYear\n\n\ntz()\nTime zone"
  },
  {
    "objectID": "date_objects.html#operating-on-dates",
    "href": "date_objects.html#operating-on-dates",
    "title": "5  Working with Dates",
    "section": "5.3 Operating on dates",
    "text": "5.3 Operating on dates\nYou can apply certain operations to dates as you would to numbers. For example, to list the number of days between the first and third elements of the vector x.date type the following:\n\n(x.date[3] - x.date[1]) / ddays()\n\n[1] 415.5949\n\n\nTo get the number of weeks between both dates:\n\n(x.date[3] - x.date[1]) / dweeks()\n\n[1] 59.37069\n\n\nLikewise, you can get the number of minutes between dates by dividing by dminutes() and the number of years by dividing by dyears().\nYou can also apply Boolean operations on dates. For example, to find which date element in x.date falls between the 11th and 24th day of any month, type:\n\n(mday(x.date) > 11) & (mday(x.date) < 24)\n\n[1]  TRUE FALSE  TRUE\n\n\nIf you want the command to return just the dates that satisfy this query, pass the Boolean operation as an index to the x.date vector:\n\nx.date[ (mday(x.date) > 11) & (mday(x.date) < 24) ]\n\n[1] \"2013-06-23 03:45:23 AKDT\" \"2014-08-12 18:01:59 AKDT\""
  },
  {
    "objectID": "date_objects.html#formatting-date-objects",
    "href": "date_objects.html#formatting-date-objects",
    "title": "5  Working with Dates",
    "section": "5.4 Formatting date objects",
    "text": "5.4 Formatting date objects\nYou can create a character vector from a date object. This is useful if you want to annotate plots with dates or include date values in reports. For example, to convert the date object x.date to a “Month_name Year” character format, type the following:\n\nas.character(x.date, format=\"%B %Y\")\n\n[1] \"June 2013\"   \"July 2013\"   \"August 2014\"\n\n\nThe format= parameter accepts many different date/time format codes listed in the following table (note the case!).\n\n\n\n\n\n\n\n\nFormat codes\nDescription\nExample\n\n\n\n\n%a\nAbbreviated weekday name\nSun, Tue, Tue\n\n\n%A\nFull weekday name\nSunday, Tuesday, Tuesday\n\n\n%m\nMonth as decimal number\n06, 07, 08\n\n\n%b\nAbbreviated month name\nJun, Jul, Aug\n\n\n%B\nFull month name\nJune, July, August\n\n\n%c\nDate and time, locale-specific\nSun Jun 23 03:45:23 2013, Tue Jul 30 14:23:00 2013, Tue Aug 12 18:01:59 2014\n\n\n%d\nDay of the month as decimal number\n23, 30, 12\n\n\n%H\nHours as decimal number (00 to 23)\n03, 14, 18\n\n\n%I\nHours as decimal number (01 to 12)\n03, 02, 06\n\n\n%p\nAM/PM indicator in the locale\nAM, PM, PM\n\n\n%j\nDay of year as decimal number\n174, 211, 224\n\n\n%M\nMinute as decimal number (00 to 59)\n45, 23, 01\n\n\n%S\nSecond as decimal number\n23, 00, 59\n\n\n%U\nWeek of the year starting on the first Sunday\n25, 30, 32\n\n\n%W\nWeek of the year starting on the first Monday\n24, 30, 32\n\n\n%w\nWeekday as decimal number (Sunday = 0)\n0, 2, 2\n\n\n%x\nDate (locale-specific)\n6/23/2013, 7/30/2013, 8/12/2014\n\n\n%X\nTime (locale-specific)\n3:45:23 AM, 2:23:00 PM, 6:01:59 PM\n\n\n%Y\n4-digit year\n2013, 2013, 2014\n\n\n%y\n2-digit year\n13, 13, 14\n\n\n%Z\nAbbreviated time zone\nAKDT, AKDT, AKDT\n\n\n%z\nTime zone\n-0800, -0800, -0800"
  },
  {
    "objectID": "strings.html",
    "href": "strings.html",
    "title": "6  Working with string objects",
    "section": "",
    "text": "Numbers and dates are not the only variable types we might be interested in exploring. We often find ourselves having to manipulate character (text) objects as well. In the programming environment, such queries are often referred to as string searches. String queries may involve assessing if a variable matches or contains an exact set of characters; it can also involve extracting a certain set of characters given some pattern. R has a very capable set of string operations built into its environment however, many find it difficult to master. A package that will be used in this tutorial that simplifies this task is called stringr. A write-up of its capabilities can be found here."
  },
  {
    "objectID": "strings.html#finding-patterns-in-a-string",
    "href": "strings.html#finding-patterns-in-a-string",
    "title": "6  Working with string objects",
    "section": "6.1 Finding patterns in a string",
    "text": "6.1 Finding patterns in a string\n  \n\n6.1.1 Checking for an exact string match\nThis is the simplest string operation one can perform. It involves assessing if a variable is equal (or not) to a complete text string.\nWe’ve already seen how the conditional statements can be used to check whether a variable is equal to, less than or greater than a number. We can use conditional statements to evaluate if a variable matches an exact string. For example, the following chunk of code returns TRUE since the strings match exactly.\n\na <- \"Abc def\"\na == \"Abc def\"\n\n[1] TRUE\n\n\nHowever, note that R differentiates cases so that the following query, returns FALSE since the first character does not match in case (i.e. upper case A vs. lower case a).\n\na == \"abc def\"\n\n[1] FALSE\n\n\n\n6.1.1.1 How to ignore case sensitivity?\nIf you want R to ignore cases in any string operations, simply force all variables to a lower case and define the pattern being compared against in lower case. For example:\n\ntolower(a) == \"abc def\"\n\n[1] TRUE\n\n\n\n\n\n6.1.2 Checking for a partial match\n\n6.1.2.1 Matching anywhere in the string\nTo check if object a has the pattern \"c d\" (note the space in between the letters) anywhere in its string, use stringr’s str_detect function as follows:\n\nlibrary(stringr)\nstr_detect(a, \"c d\")\n\n[1] TRUE\n\n\nThe following example compares the string to \"cd\" (note the omission of the space):\n\nstr_detect(a, \"cd\")\n\n[1] FALSE\n\n\n\n\n6.1.2.2 Matching the begining of the string\nTo check if object a starts with the pattern \"c d\" add the carat character ^ in front of the pattern as in:\n\nstr_detect(a, \"^c d\")\n\n[1] FALSE\n\n\n\n\n6.1.2.3 Matching the end of the sring\nTo check if object a ends with the pattern \"Abc\" add the dollar character $ to the end of the pattern as in:\n\nstr_detect(a, \"Abc$\")\n\n[1] FALSE\n\n\n\n\n\n6.1.3 Locating the position of a pattern in a string\nIf you want to find where a particular pattern lies in a string, use the str_locate function. For example, to find where the pattern \"c d\" occurs in object a type:\n\nstr_locate(a, \"c d\")\n\n     start end\n[1,]     3   5\n\n\nThe function returns two values: the position in the string where the pattern starts (e.g. position 3) and the position where the pattern ends (e.g. position 5 )\nNote that if the pattern is not found, str_locate returns NA’s:\n\nstr_locate(a, \"cd\")\n\n     start end\n[1,]    NA  NA\n\n\nNote too that the str_locate function only returns the position of the first occurrence. For example, the following chunk will only return the start/end positions of the first occurrence of Ab.\n\nb <- \"Abc def Abg\"\nstr_locate(b, \"Ab\")\n\n     start end\n[1,]     1   2\n\n\nTo find all occurrences, use the str_locate_all() function as in:\n\nstr_locate_all(b,\"Ab\")\n\n[[1]]\n     start end\n[1,]     1   2\n[2,]     9  10\n\n\nThe function returns a list object. To extract the position values into a dateframe, simply wrap the function in a call to as.data.frame, for example:\n\nstr.pos <- as.data.frame(str_locate_all(b,\"Ab\"))\nstr.pos\n\n  start end\n1     1   2\n2     9  10\n\n\nThe reason str_locate_all returns a list and not a matrix or a data frame can be understood in the following example:\n\n# Create a 5 element string vector\nd <- c(\"Abc\", \"Def \", \"Abc Def Ab\", \" bc \", \"ef \")\n\n# Search for all instances of \"Ab\"\nstr_locate_all(d,\"Ab\")\n\n[[1]]\n     start end\n[1,]     1   2\n\n[[2]]\n     start end\n\n[[3]]\n     start end\n[1,]     1   2\n[2,]     9  10\n\n[[4]]\n     start end\n\n[[5]]\n     start end\n\n\nHere, d is a five element string vector (so far we’ve worked with single element vectors). The str_locate_all function returns a result for each element of that vector, and since patterns can be found multiple times in a same vector element, the output can only be conveniently stored in a list.\n\n\n6.1.4 Finding the length of a string\nA natural extension to finding the positions of patterns in a text is to find the string’s total length. This can be accomplished with the str_length() function:\n\nstr_length(b)\n\n[1] 11\n\n\nFor a multi-element vector, the output looks like this:\n\nstr_length(d)\n\n[1]  3  4 10  4  3\n\n\n\n\n6.1.5 Finding a pattern’s frequency\nTo find out how often the pattern Ab occurs in each element of object d, use the str_count() function.\n\nstr_count(d, \"Ab\")\n\n[1] 1 0 2 0 0"
  },
  {
    "objectID": "strings.html#modifying-strings",
    "href": "strings.html#modifying-strings",
    "title": "6  Working with string objects",
    "section": "6.3 Modifying strings",
    "text": "6.3 Modifying strings\n  \n\n6.3.1 Padding numbers with leading zeros\nThe str_pad() function can be used to pad numbers with leading zeros. Note that in doing so, you are creating a character object from a numeric object.\n\ne <- c(12, 2, 503, 20, 0)\nstr_pad(e, width=3, side=\"left\", pad = \"0\" )\n\n[1] \"012\" \"002\" \"503\" \"020\" \"000\"\n\n\n\n\n6.3.2 Appending text to strings\nYou can append strings with custom text using the str_c() functions. For example, to add the string length at the end of each vector element in b type,\n\nstr_c(d, \" has \", str_length(d), \" characters\" )\n\n[1] \"Abc has 3 characters\"         \"Def  has 4 characters\"        \"Abc Def Ab has 10 characters\"\n[4] \" bc  has 4 characters\"        \"ef  has 3 characters\"        \n\n\n\n\n6.3.3 Removing white spaces\nYou can remove leading or ending (or both) white spaces from a string. For example, to remove leading white spaces from object d type,\n\nd.left.trim <- str_trim(d, side=\"left\")\n\nNow let’s compare the original to the left-trimmed version:\n\nstr_length(d)\n\n[1]  3  4 10  4  3\n\nstr_length(d.left.trim)\n\n[1]  3  4 10  3  3\n\n\nTo remove trailing spaces set side = \"right\" and to remove both leading and trailing spaces set side = \"both\".\n\n\n6.3.4 Replacing elements of a string\nTo replace all instances of a specified set of characters in a string with another set of characters, use the str_replace_all() function. For example, to replace all spaces in object b with dashes, type:\n\nstr_replace_all(b, \" \", \"-\")\n\n[1] \"Abc-def-Abg\""
  },
  {
    "objectID": "strings.html#extracting-parts-of-a-string",
    "href": "strings.html#extracting-parts-of-a-string",
    "title": "6  Working with string objects",
    "section": "6.4 Extracting parts of a string",
    "text": "6.4 Extracting parts of a string\n \n\n6.4.1 Extracting elements of a string given start and end positions\nTo find the character elements of a vector at a given position of a given string, use the str_sub() function. For example, to find the characters between positions two and five (inclusive) type:\n\nstr_sub(b, start=2, end=5)\n\n[1] \"bc d\"\n\n\nIf you don’t specify a start position, then all characters up to and including the end position will be returned. Likewise, if the end position is not specified then all characters from the start position to the end of the string will be returned.\n\n\n6.4.2 Splitting a string by a character\nIf you want to break a string up into individual components based on a character delimiter, use the str_split() function. For example, to split the following string into separate elements by comma, type the following:\n\ng <- \"Year:2000, Month:Jan, Day:23\"\nstr_split(g, \",\")\n\n[[1]]\n[1] \"Year:2000\"  \" Month:Jan\" \" Day:23\"   \n\n\nThe output is a one element list. If object g consists of more than one element, the output will be a list of as many elements as there are g elements.\nDepending on your workflow, you may need to convert the str_split output to an atomic vector. For example, if you want to find an element in the above str_split output that matches the string Year:2000, the following will return FALSE and not TRUE as expected:\n\n\"Year:2000\" %in% str_split(g, \",\")\n\n[1] FALSE\n\n\nThe workaround is to convert the right-hand output to a single vector using the unlist function:\n\n\"Year:2000\" %in% unlist(str_split(g, \",\"))\n\n[1] TRUE\n\n\nIf you are applying the split function to a column of data from a dataframe, you will want to use the function str_split_fixed instead. This function assumes that the number of components to be extracted via the split will be the same for each vector element. For example, the following vector, T1, has two time components that need to be extracted. The separator is a dash, -.\n\nT1 <- c(\"9:30am-10:45am\", \"9:00am- 9:50am\", \"1:00pm- 2:15pm\")\nT1\n\n[1] \"9:30am-10:45am\" \"9:00am- 9:50am\" \"1:00pm- 2:15pm\"\n\nstr_split_fixed(T1, \"-\", 2)\n\n     [,1]     [,2]     \n[1,] \"9:30am\" \"10:45am\"\n[2,] \"9:00am\" \" 9:50am\"\n[3,] \"1:00pm\" \" 2:15pm\"\n\n\nThe third parameter in the str_split_fixed function is the number of elements to return which also defines the output dimension (here, a three row and two column table). If you want to extract both times to separate vectors, reference the columns by index number:\n\nT1.start <- str_split_fixed(T1, \"-\", 2)[ ,1]\nT1.start\n\n[1] \"9:30am\" \"9:00am\" \"1:00pm\"\n\nT1.end   <- str_split_fixed(T1, \"-\", 2)[ ,2]\nT1.end\n\n[1] \"10:45am\" \" 9:50am\" \" 2:15pm\"\n\n\nYou will want to use the indexes if you are extracting strings in a data frame. For example:\n\ndat <- data.frame( Time = c(\"9:30am-10:45am\", \"9:00am-9:50am\", \"1:00pm-2:15pm\"))\ndat$Start_time <- str_split_fixed(dat$Time, \"-\", 2)[ , 1]\ndat$End_time   <- str_split_fixed(dat$Time, \"-\", 2)[ , 2]\ndat\n\n            Time Start_time End_time\n1 9:30am-10:45am     9:30am  10:45am\n2  9:00am-9:50am     9:00am   9:50am\n3  1:00pm-2:15pm     1:00pm   2:15pm\n\n\n\n\n6.4.3 Extracting parts of a string that follow a pattern\nTo extract the three letter months from object g (defined in the last example), you can use a combination of stringr functions as in:\n\nloc <- str_locate(g, \"Month:\")\nstr_sub(g, start = loc[,\"end\"] + 1, end = loc[,\"end\"]+3)\n\n[1] \"Jan\"\n\n\nThe above chunk of code first identifies the position of the Month: string and passes its output to the object loc (a matrix). It then uses the loc’s end position in the call to str_sub to extract the three characters making up the month abbreviation. The value 1 is added to the start parameter in str_sub to omit the last character of Month: (recall that the str_locate positions are inclusive).\nThis can be extend to multi-element vectors as follows:\n\n# Note the differences in spaces and string lenghts between the vector\n# elements.\ngs <- c(\"Year:2000, Month:Jan, Day:23\",\n        \"Year:345, Month:Mar, Day:30\",\n        \"Year:1867 , Month:Nov, Day:5\")\n\nloc <- str_locate(gs, \"Month:\")\nstr_sub(gs, start = loc[,\"end\"] + 1, end = loc[,\"end\"]+3)\n\n[1] \"Jan\" \"Mar\" \"Nov\"\n\n\nNote the non-uniformity in each element’s length and Month: position which requires that we explicitly search for the Month: string position in each element. Had all elements been of equal length and format, we could have simply assigned the position numbers in the call to str_sub function."
  },
  {
    "objectID": "boolean.html",
    "href": "boolean.html",
    "title": "7  Relational and boolean operations",
    "section": "",
    "text": "You’ve already been exposed to a few examples of relational and boolean operations in earlier tutorials. A formal exploration of these techniques follow."
  },
  {
    "objectID": "boolean.html#relational-operations",
    "href": "boolean.html#relational-operations",
    "title": "7  Relational and boolean operations",
    "section": "7.1 Relational operations",
    "text": "7.1 Relational operations\nRelational operations play an important role in data manipulation. Anytime you subset a dataset based on one or more criterion, you are making use of a relational operation. The relational operators (also known as logical binary operators) include ==, !=, <, <=, > and >=. The output of a condition is a logical vector TRUE or FALSE.\n\n\n\n\nRelational operator\nSyntax\nExample\n\n\n\n\nExact equality\n==\n3 == 4 -> FALSE\n\n\nExact inequality\n!=\n3 != 4 -> TRUE\n\n\nLess than\n<\n3 < 4 -> TRUE\n\n\nLess than or equal\n<=\n4 <= 4 -> TRUE\n\n\nGreater than\n>\n3 > 4 -> FALSE\n\n\nGreater than or equal\n>=\n4 >= 4 -> TRUE"
  },
  {
    "objectID": "boolean.html#boolean-operations",
    "href": "boolean.html#boolean-operations",
    "title": "7  Relational and boolean operations",
    "section": "7.2 Boolean operations",
    "text": "7.2 Boolean operations\nBoolean operations can be used to piece together multiple evaluations.\nR has three boolean operators: The AND operator, &; The NOT operator, !; And the OR operator, |.\nThe & operator requires that the conditions on both sides of the boolean operator be satisfied. You would normally use this operator when addressing a question along the lines of “x must be satisfied AND y must be satisfied”.\nThe | operator requires that at least one condition be met on either side of the boolean operator. You would normally use this operator when addressing a question along the lines of “x must be satisfied OR y must be satisfied”. Note that the output will also be TRUE if both conditions are met.\nThe ! operator is a negation operator. It will reverse the outcome of a condition. It can be interpreted as “I do NOT want x to be true”. So if the outcome of an expression is TRUE, preceding that expression with ! will reverse the outcome to FALSE and vice-versa.\n\n\n\n\n\n\n\n\n\n\nBoolean operator\nSyntax\nExample\nOutcome\n\n\n\n\nAND\n&\n4 == 3 & 1 == 1  4 == 4 & 1 == 1\nFALSE  TRUE\n\n\nOR\n|\n4 == 4 | 1 == 1  4 == 3 | 1 == 1  4 == 3 | 1 == 2\nTRUE  TRUE  FALSE\n\n\nNOT\n!\n!(4 == 3)  !(4 == 4)\nTRUE  FALSE\n\n\n\nThe following table breaks down all possible Boolean outcomes where T = TRUE and F = FALSE:\n\n\n\nBoolean operation\nOutcome\n\n\n\n\nT & T\nTRUE\n\n\nT & F\nFALSE\n\n\nF & F\nFALSE\n\n\nT | T\nTRUE\n\n\nT | F\nTRUE\n\n\nF | F\nFALSE\n\n\n!T\nFALSE\n\n\n!F\nTRUE\n\n\n\nIf the input values to a boolean operation are numeric vectors and not logical vectors, the numeric values will be interpreted as FALSE if zero and TRUE otherwise. For example:\n\n1 & 2\n\n[1] TRUE\n\n1 & 0\n\n[1] FALSE\n\n\n\n7.2.1 A word of caution\nNote that the operation a == (3 | 4) is not the same as (a == 3) | (a == 4). The former will return FALSE whereas the latter will return TRUE if a = 3. This is because the Boolean operator evaluates both sides of its expression as separate logical outcomes (i.e. T and F values). In the latter case, the Boolean expression is asking “is a equal to 3 OR is a equal to 4”. Since one of the conditions is true, the expression ends up evaluating TRUE | FALSE which returns TRUE (see above table).\n\na <- 3\nb <- 4\n(a == 3) | (a == 4)\n\n[1] TRUE\n\n\nIn the former expression, the boolean operator | is evaluating 3 OR 4 on its right-hand side. As mentioned in the previous section, logical values take on a value of 0 for FALSE and any non-zero value for TRUE, so when evaluating 3 | 4, it’s really seeing TRUE | TRUE which, according to the aforementioned table will output TRUE.\n\n3 | 4\n\n[1] TRUE\n\n\nSo in the end, the expression a == (3 | 4) is really evaluating the condition a == TRUE which returns false (since 3 is not equal to the logical value TRUE).\n\na == (3 | 4)\n\n[1] FALSE\n\n\nYou may wonder how a relational operator such as == can compare two different data types? (Recall that a is numeric and TRUE is logical). R is not strict about mixing data types in many of its operations. It circumvents differences in data types by coercing all values to the highest common mode (see the chapter on data types). Here, numeric overrides logical type thus coercing the TRUE variable to a numeric data type.\n\nas.numeric(TRUE)\n\n[1] 1\n\n\nSo a == (3 | 4) reduces to a == TRUE which is reduced to a == 1.\n\n\n7.2.2 Pecking order in operations\nThe previous example made use of R’s built-in operation precedence rules. For example, comparison operations such as <= and > are performed before boolean operations such that a == 3 | 4 will first evaluate a == 3 before evaluating ... | 4.\nEven boolean operations follow a pecking order such that ! precedes & which precedes |. For example:\n! TRUE & FALSE | TRUE\nwill first evaluate ! TRUE, then ... & FALSE, then ... | TRUE.\nTo overrride R’s built-in precedence, use paretheses. For example:\n! TRUE & (FALSE | TRUE)\nwill first evaluate (FALSE | TRUE) and ! TRUE, then pass their output to ... | ..., then ... | TRUE.\nFor a full list of operation precedence, access the help page for Syntax.\n?Syntax\nThe following lists the pecking order from high to low precedence.\n\n\n\n:: :::\naccess variables in a namespace\n\n\n$ @\ncomponent / slot extraction\n\n\n[ [[\nindexing\n\n\n^\nexponentiation (right to left)\n\n\n- +\nunary minus and plus\n\n\n:\nsequence operator\n\n\n%any% |>\nspecialoperators (including %% and %/%)\n\n\n* /\nmultiply, divide\n\n\n+ -\n(binary) add, subtract\n\n\n< > <= >= == !=\nordering and comparison\n\n\n!\nnegation\n\n\n& &&\nand\n\n\n| ||\nor\n\n\n~\nas in formulae\n\n\n-> ->>\nrightwards assignment\n\n\n<- <<-\nassignment (right to left)\n\n\n=\nassignment (right to left)\n\n\n?\nhelp"
  },
  {
    "objectID": "boolean.html#comparing-multidimensional-objects",
    "href": "boolean.html#comparing-multidimensional-objects",
    "title": "7  Relational and boolean operations",
    "section": "7.3 Comparing multidimensional objects",
    "text": "7.3 Comparing multidimensional objects\nThe relational operators are used to compare single elements (i.e. one element at a time). If you want to compare two objects as a whole (e.g. multi-element vectors or data frames), use the identical() function. For example:\n\na <- c(1, 5, 6, 10)\nb <- c(1, 5, 6)\nidentical(a, a)\n\n[1] TRUE\n\nidentical(a, b)\n\n[1] FALSE\n\nidentical(mtcars, mtcars)\n\n[1] TRUE\n\n\nNotice that identical returns a single logical vector, regardless the input object’s dimensions.\nNote that the data structure must match as well as its element values. For example, if d is a list and a is an atomic vector, the output of identical will be false even if the internal values match.\n\nd <- list( c(1, 5, 6, 10) )\nidentical(a, d)\n\n[1] FALSE\n\n\nIf we convert d from a list to an atomic vector using the unlist function (thus matching data structures), we get:\n\nidentical(a, unlist(d))\n\n[1] TRUE"
  },
  {
    "objectID": "boolean.html#the-match-operator-in",
    "href": "boolean.html#the-match-operator-in",
    "title": "7  Relational and boolean operations",
    "section": "7.4 The match operator %in%",
    "text": "7.4 The match operator %in%\nThe match operator %in% compares two sets of vectors and assesses if an element on the left-hand side of %in% matches any of the elements on the right-hand side of the operator. For each element in the left-hand vector, R returns TRUE if the value is present in any of the right-hand side elements or FALSE if not.\nFor example, given the following vectors:\n\nv1 <- c( \"a\", \"b\", \"cd\", \"fe\")\nv2 <- c( \"b\", \"e\")\n\nfind the elements in v1 that match any of the values in v2.\n\nv1 %in% v2\n\n[1] FALSE  TRUE FALSE FALSE\n\n\nThe function checks whether each element in v1 has a matching value in v2. For example, element \"a\" in v1 is compared to elements \"b\" and \"e\" in v2. No matches are found and a FALSE is returned. The next element in v1, \"b\", is compared to both elements in v2. This time, there is a match (v2 has an element \"b\") and TRUE is returned. This process is repeated for all elements in v1.\nThe logical vector output has the same length as the input vector v1 (four in this example).\nIf we swap the vector objects, we get a two element logical vector since we are now comparing each element in v2 to any matching elements in v1.\n\nv2 %in% v1\n\n[1]  TRUE FALSE"
  },
  {
    "objectID": "boolean.html#checking-if-a-value-is-na",
    "href": "boolean.html#checking-if-a-value-is-na",
    "title": "7  Relational and boolean operations",
    "section": "7.5 Checking if a value is NA",
    "text": "7.5 Checking if a value is NA\nWhen assessing if a value is equal to NA the following evaluation may behave unexpectedly.\n\na <- c (3, 67, 4, NA, 10)\na == NA\n\n[1] NA NA NA NA NA\n\n\nThe output is not a logical data type we would expect from an evaluation. Instead, you must make use of the is.na() function:\n\nis.na(a)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nAs another example, if we want to keep all rows in dataframe d where z = NA, we would type:\n\nd <- data.frame(x = c(1,4,2,5,2,3,NA), \n                y = c(3,2,5,3,8,1,1), \n                z = c(NA,NA,4,9,7,8,3))\n\nd[ is.na(d$z), ]\n\n  x y  z\n1 1 3 NA\n2 4 2 NA\n\n\nYou can, of course, use the ! operator to reverse the evaluation and omit all rows where z = NA,\n\nd[ !is.na(d$z), ]\n\n   x y z\n3  2 5 4\n4  5 3 9\n5  2 8 7\n6  3 1 8\n7 NA 1 3"
  },
  {
    "objectID": "subset_base.html#the-subset-function",
    "href": "subset_base.html#the-subset-function",
    "title": "8  Subsetting and replacing values using base functions",
    "section": "8.1 The subset function",
    "text": "8.1 The subset function\nYou can subset a dataframe object by criteria using the subset function.\n\nsubset(mtcars, mpg > 25)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nYou can combine individual criterion using boolean operators when selecting by row.\n\nsubset(mtcars, (mpg > 25) & (hp > 65)  )\n\n               mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128      32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nFiat X1-9     27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nYou can also subset by column. To select columns, add the select = parameter.\n\nsubset(mtcars, (mpg > 25) & (hp > 65) , select = c(mpg, cyl, hp))\n\n               mpg cyl  hp\nFiat 128      32.4   4  66\nFiat X1-9     27.3   4  66\nPorsche 914-2 26.0   4  91\nLotus Europa  30.4   4 113\n\n\nNote that the subset function can behave in unexpected ways. This is why its authors recommend against using subset in a script (see ?subset). Instead, they advocate using indices."
  },
  {
    "objectID": "subset_base.html#subset-using-indices",
    "href": "subset_base.html#subset-using-indices",
    "title": "8  Subsetting and replacing values using base functions",
    "section": "8.2 Subset using indices",
    "text": "8.2 Subset using indices\nYou’ve already learned how to identify elements in an atomic vector or a data frame in Chapter 3. For example, to extract the second and fourth element of the following vector,\n\nx <- c(\"a\", \"f\", \"a\", \"d\", \"a\")\n\ntype,\n\nx[c(2,4)]\n\n[1] \"f\" \"d\"\n\n\nTo subset a dataframe by index, you need to define both dimension’s index (separated by a comma). For example, to extract all rows where mpg > 25 as well as the first and fourth columns from the built-in mtcars dataset type,\n\nmtcars[ mtcars$mpg > 25, c(1,4)]\n\n                mpg  hp\nFiat 128       32.4  66\nHonda Civic    30.4  52\nToyota Corolla 33.9  65\nFiat X1-9      27.3  66\nPorsche 914-2  26.0  91\nLotus Europa   30.4 113\n\n\nNote that we have to add mtcars$ to the expression since the variable mpg does not exist as a standalone object.\nYou can also reference columns by names as in,\n\nmtcars[ mtcars$mpg > 25, c(\"mpg\", \"hp\")]\n\n                mpg  hp\nFiat 128       32.4  66\nHonda Civic    30.4  52\nToyota Corolla 33.9  65\nFiat X1-9      27.3  66\nPorsche 914-2  26.0  91\nLotus Europa   30.4 113"
  },
  {
    "objectID": "subset_base.html#extracting-using-logical-expression-and-indices",
    "href": "subset_base.html#extracting-using-logical-expression-and-indices",
    "title": "8  Subsetting and replacing values using base functions",
    "section": "8.3 Extracting using logical expression and indices",
    "text": "8.3 Extracting using logical expression and indices\nWe apply conditions to indices to reveal elements of a vector or a table that satisfy one or more criterion. For example:\n\nx[ x == \"a\" ]\n\n[1] \"a\" \"a\" \"a\"\n\n\nLet’s breakdown the above expression. The output of the expression x == \"a\" is TRUE FALSE TRUE FALSE TRUE, a logical vector with the same number of elements as x. The logical elements are then passed to the indexing brackets where they act as a “mask” as shown in the following graphic.\n\nThe elements that make it through the extraction mask are then combined into a new vector element.\nThe same idea applies to dataframes. For example, to extract all rows where mpg > 30 type:\n\nmtcars[ mtcars$mpg > 30, ]\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nFiat 128       32.4   4 78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4 71.1  65 4.22 1.835 19.90  1  1    4    1\nLotus Europa   30.4   4 95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nHere, we are “masking” the rows that do not satisfy the criterion using the TRUE/FALSE logical outcomes from the conditional operation."
  },
  {
    "objectID": "subset_base.html#replacing-values-using-logical-expressions",
    "href": "subset_base.html#replacing-values-using-logical-expressions",
    "title": "8  Subsetting and replacing values using base functions",
    "section": "8.4 Replacing values using logical expressions",
    "text": "8.4 Replacing values using logical expressions\nWe can adopt the same masking properties of logical variables to replace values in a vector. For example, to replace all instances of \"a\" in vector x with the character \"z\", we first expose the elements equal to \"a\", then assign the new value to the exposed elements.\n\nx[ x == \"a\" ] <- \"z\"\nx\n\n[1] \"z\" \"f\" \"z\" \"d\" \"z\"\n\n\nYou can think of the logical mask as a template applied to a road surface before spraying that template with a can of \"z\" spray. Only the exposed portion of the street surface will be sprayed with the \"z\" values.\n\nYou can apply this technique to dataframes as well. For example, to replace all elements in mpg with -1 if mpg < 25, type:\n\nmtcars2 <- mtcars \nmtcars2[ mtcars2$mpg < 25, \"mpg\"]  <-  -1\nmtcars2\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           -1.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       -1.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          -1.0   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      -1.0   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   -1.0   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             -1.0   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          -1.0   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           -1.0   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            -1.0   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            -1.0   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           -1.0   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          -1.0   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          -1.0   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         -1.0   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  -1.0   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental -1.0   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   -1.0   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       -1.0   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    -1.0   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         -1.0   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          -1.0   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    -1.0   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      -1.0   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        -1.0   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       -1.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          -1.0   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nNote that we had to specify the column, mpg, into which we are replacing the values. Had we left the second index empty, we would have replaced values across all columns for records having an mpg value less than 30."
  },
  {
    "objectID": "dplyr.html",
    "href": "dplyr.html",
    "title": "9  Manipulating data tables with dplyr",
    "section": "",
    "text": "The data file FAO_grains_NA.csv will be used in this exercise. This dataset consists of grain yield and harvest year by North American country. The dataset was downloaded from http://faostat3.fao.org/ in June of 2014.\nRun the following line to load the FAO data file into your current R session.\nBefore tackling the examples that follow, make sure to load the dplyr package."
  },
  {
    "objectID": "dplyr.html#the-dplyr-basics",
    "href": "dplyr.html#the-dplyr-basics",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.1 The dplyr basics",
    "text": "9.1 The dplyr basics\nThe basic set of R tools can accomplish many data table queries, but the syntax can be overwhelming and verbose. The package dplyr offers some nifty and simple querying functions as shown in the next subsections. Some of dplyr’s key data manipulation functions are summarized in the following table:\n\n\n\ndplyr function\nDescription\n\n\n\n\nfilter()\nSubset by row values\n\n\narrange()\nSort rows by column values\n\n\nselect()\nSubset columns\n\n\nmutate()\nAdd columns\n\n\nsummarise()\nSummarize columns\n\n\n\nNote that all of these functions take as first argument the data table name except when used in a piping operation (pipes are discussed later in this chapter). For example:\nWhen used alone, dataframe dat is the first argument inside the dplyr function.\n\ndat2 <- select(dat, crop)\n\nWhen used in a pipe, dataframe dat is outside of the dplyr function.\n\ndat2 <- dat %>% select(crop)\n\n\n9.1.1 filter: Subset by rows\nTables can be subsetted by rows based on column values. For example, we may wish to grab all rows associated with the string Oats in the Crop column:\n\ndat.query1 <- filter(dat, Crop == \"Oats\")\nsummary(dat.query1)\n\n   Country              Crop           Information             Year     \n Length:208         Length:208         Length:208         Min.   :1961  \n Class :character   Class :character   Class :character   1st Qu.:1974  \n Mode  :character   Mode  :character   Mode  :character   Median :1986  \n                                                          Mean   :1986  \n                                                          3rd Qu.:1999  \n                                                          Max.   :2012  \n     Value            Source         \n Min.   :  12667   Length:208        \n 1st Qu.:  20392   Class :character  \n Median : 204821   Mode  :character  \n Mean   :1350024                     \n 3rd Qu.:1638250                     \n Max.   :9666550                     \n\n\nNote that R is case sensitive, so make sure that you respect each letter’s case (i.e. upper or lower).\nWe can expand our query by including both Oats, Buckwheat and limiting the Country column to Canada.\n\ndat.query2 <- filter(dat, Crop == \"Oats\" | Crop == \"Buckwheat\", \n                          Country == \"Canada\")\nsummary(dat.query2)\n\n   Country              Crop           Information             Year     \n Length:200         Length:200         Length:200         Min.   :1961  \n Class :character   Class :character   Class :character   1st Qu.:1973  \n Mode  :character   Mode  :character   Mode  :character   Median :1986  \n                                                          Mean   :1986  \n                                                          3rd Qu.:1998  \n                                                          Max.   :2012  \n     Value            Source         \n Min.   :      0   Length:200        \n 1st Qu.:  11822   Class :character  \n Median :  22197   Mode  :character  \n Mean   : 504098                     \n 3rd Qu.: 924675                     \n Max.   :4277000                     \n\n\nThe character | is the Boolean operator OR. So in our example, the query can be read as “… crop equals oats OR crop equals buckwheat”. Had we used the AND operator, &, instead as in Crop == \"Oats\" & Crop == \"Buckwheat\" the output would have returned zero rows since a Crop value cannot be both Oats AND Buckwheat.\nWe can expand this query by limiting our output to the years 2005 to 2010\n\nlibrary(dplyr)\ndat.query3 <- filter(dat, Crop == \"Oats\" | Crop == \"Buckwheat\", \n                          Country == \"Canada\", \n                          Year >= 2005 & Year <= 2010)\nsummary(dat.query3)\n\n   Country              Crop           Information             Year     \n Length:18          Length:18          Length:18          Min.   :2005  \n Class :character   Class :character   Class :character   1st Qu.:2006  \n Mode  :character   Mode  :character   Mode  :character   Median :2007  \n                                                          Mean   :2007  \n                                                          3rd Qu.:2008  \n                                                          Max.   :2010  \n     Value            Source         \n Min.   :   2000   Length:18         \n 1st Qu.:  11500   Class :character  \n Median :  26615   Mode  :character  \n Mean   : 453810                     \n 3rd Qu.: 961825                     \n Max.   :1815700                     \n\n\nNote the use of the AND Boolean operator (&) instead of the OR operator (|) for the Year query. We want the Year value to satisfy two criteria simultaneously: greater than or equal to 2005 AND less than or equal to 2010. Had we used the | operator, R would have returned all years since all year values satisfy at least one of the two criterion.\n\n\n9.1.2 arrange: Sort rows by column value\nYou can sort a table based on a column’s values. For example, to sort dat by crop name type:\n\ndat.sort1 <- arrange(dat, Crop)\nhead(dat.sort1)\n\n                   Country   Crop         Information Year      Value\n1                   Canada Barley Area harvested (Ha) 2012 2060000.00\n2                   Canada Barley       Yield (Hg/Ha) 2012   38894.66\n3 United States of America Barley Area harvested (Ha) 2012 1312810.00\n4 United States of America Barley       Yield (Hg/Ha) 2012   36533.24\n5                   Canada Barley Area harvested (Ha) 2011 2364800.00\n6                   Canada Barley       Yield (Hg/Ha) 2011   32796.43\n           Source\n1   Official data\n2 Calculated data\n3   Official data\n4 Calculated data\n5   Official data\n6 Calculated data\n\ntail(dat.sort1)\n\n     Country      Crop         Information Year    Value\n1496  Canada Triticale Area harvested (Ha) 1991  1093.00\n1497  Canada Triticale       Yield (Hg/Ha) 1991 21957.91\n1498  Canada Triticale Area harvested (Ha) 1990  1074.00\n1499  Canada Triticale       Yield (Hg/Ha) 1990 26396.65\n1500  Canada Triticale Area harvested (Ha) 1989  1093.00\n1501  Canada Triticale       Yield (Hg/Ha) 1989 21957.91\n                                       Source\n1496                            Official data\n1497                          Calculated data\n1498 FAO data based on imputation methodology\n1499                          Calculated data\n1500                             FAO estimate\n1501                          Calculated data\n\n\nBy default, arrange sorts by ascending order. To sort by descending order, wrap the column name with the function desc(). For example, to sort the table by Crop in ascending order then by Year in descending order, type:\n\ndat.sort2 <- arrange(dat, Crop, desc(Year))\nhead(dat.sort2)\n\n                   Country   Crop         Information Year      Value\n1                   Canada Barley Area harvested (Ha) 2012 2060000.00\n2                   Canada Barley       Yield (Hg/Ha) 2012   38894.66\n3 United States of America Barley Area harvested (Ha) 2012 1312810.00\n4 United States of America Barley       Yield (Hg/Ha) 2012   36533.24\n5                   Canada Barley Area harvested (Ha) 2011 2364800.00\n6                   Canada Barley       Yield (Hg/Ha) 2011   32796.43\n           Source\n1   Official data\n2 Calculated data\n3   Official data\n4 Calculated data\n5   Official data\n6 Calculated data\n\ntail(dat.sort2)\n\n     Country      Crop         Information Year    Value\n1496  Canada Triticale Area harvested (Ha) 1991  1093.00\n1497  Canada Triticale       Yield (Hg/Ha) 1991 21957.91\n1498  Canada Triticale Area harvested (Ha) 1990  1074.00\n1499  Canada Triticale       Yield (Hg/Ha) 1990 26396.65\n1500  Canada Triticale Area harvested (Ha) 1989  1093.00\n1501  Canada Triticale       Yield (Hg/Ha) 1989 21957.91\n                                       Source\n1496                            Official data\n1497                          Calculated data\n1498 FAO data based on imputation methodology\n1499                          Calculated data\n1500                             FAO estimate\n1501                          Calculated data\n\n\n\n\n9.1.3 select: Subset by column\nYou can subset a table by column(s). To extract the columns Crop, Year and Value, type:\n\ndat.subcol <- select(dat, Crop, Year, Value)\nhead(dat.subcol, 2)\n\n    Crop Year      Value\n1 Barley 2012 2060000.00\n2 Barley 2012   38894.66\n\n\nIf you want all columns other than Crop, Year and Value, add the negative - symbol before the column name:\n\ndat.subcol <- select(dat, -Crop, -Year, -Value)\nhead(dat.subcol, 2)\n\n  Country         Information          Source\n1  Canada Area harvested (Ha)   Official data\n2  Canada       Yield (Hg/Ha) Calculated data\n\n\n\n\n9.1.4 mutate: Creating and/or calculating column values\nYou can add columns (and compute their values) using the mutate function. For example, to add a column Ctr_abbr and assign it the abbreviated values CAN for Canada and USA for the United States of America based on the values in column Country type:\n\ndat.extended <- mutate(dat, Ctr_abbr = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))\nhead(dat.extended,3)\n\n  Country      Crop         Information Year      Value          Source\n1  Canada    Barley Area harvested (Ha) 2012 2060000.00   Official data\n2  Canada    Barley       Yield (Hg/Ha) 2012   38894.66 Calculated data\n3  Canada Buckwheat Area harvested (Ha) 2012       0.00    FAO estimate\n  Ctr_abbr\n1      CAN\n2      CAN\n3      CAN\n\ntail(dat.extended,3)\n\n                      Country    Crop         Information Year      Value\n1499 United States of America     Rye       Yield (Hg/Ha) 1961   11121.79\n1500 United States of America Sorghum Area harvested (Ha) 1961 4445000.00\n1501 United States of America Sorghum       Yield (Hg/Ha) 1961   27442.07\n              Source Ctr_abbr\n1499 Calculated data      USA\n1500   Official data      USA\n1501 Calculated data      USA\n\n\nHere, we make use of an embedded function, ifelse, which performs a conditional operation: if the Country value is Canada return CAN if not, return USA.\nNote that if you wish to rename a column, you can use the rename() function instead of mutate.\nYou can also use mutate to recompute column values. For example, to replace the Country column values with CAN or USA type:\n\ndat.overwrite <- mutate(dat, Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))\nhead(dat.overwrite, 3)\n\n  Country      Crop         Information Year      Value          Source\n1     CAN    Barley Area harvested (Ha) 2012 2060000.00   Official data\n2     CAN    Barley       Yield (Hg/Ha) 2012   38894.66 Calculated data\n3     CAN Buckwheat Area harvested (Ha) 2012       0.00    FAO estimate\n\ntail(dat.overwrite, 3)\n\n     Country    Crop         Information Year      Value          Source\n1499     USA     Rye       Yield (Hg/Ha) 1961   11121.79 Calculated data\n1500     USA Sorghum Area harvested (Ha) 1961 4445000.00   Official data\n1501     USA Sorghum       Yield (Hg/Ha) 1961   27442.07 Calculated data\n\n\n\n\n9.1.5 mutate_at: Creating and/or calculating values across muliple columns\nYou might find yourself wanting to apply a same set of mutate operations across multiple variables. For example, given the following sample dataset,\n\nmet <- data.frame(Wind = c(3.4, 5.0, 99, 4.1, 1.5),\n                  Dir  = c(181, 220, 15,  15,  99 ),\n                  Prec = c(99 , 0.5,  0,  99,  99))\nmet\n\n  Wind Dir Prec\n1  3.4 181 99.0\n2  5.0 220  0.5\n3 99.0  15  0.0\n4  4.1  15 99.0\n5  1.5  99 99.0\n\n\nwhere the value 99 is a placeholder for a missing value for the variables Wind and Prec but a valid value for Dir, we want to replace all missing values with NA. We could either create two mutate operations as in,\n\nmet2 <- mutate( met, Wind = ifelse(Wind == 99, NA, Wind),\n                     Prec = ifelse(Prec == 99, NA, Prec))\nmet2\n\n  Wind Dir Prec\n1  3.4 181   NA\n2  5.0 220  0.5\n3   NA  15  0.0\n4  4.1  15   NA\n5  1.5  99   NA\n\n\nor, we could reduce the separate mutate operations into a single mutate_at operation,\n\nmet2 <- mutate_at( met, vars(Wind, Prec),\n                        list(~ ifelse( . == 99, NA, .)))\nmet2\n\n  Wind Dir Prec\n1  3.4 181   NA\n2  5.0 220  0.5\n3   NA  15  0.0\n4  4.1  15   NA\n5  1.5  99   NA\n\n\nThe dot . is a placeholder for each column listed in the vars() function. Also, note the ~ operator in lieu of . =.\n\n\n9.1.6 summarise: Summarize columns\nYou can summarize (or “collapse”) one or more columns using the summarise function. For instance, to get the minimum and maximum years from the Year column, type:\n\nsummarise(dat, yr_min = min(Year), yr_max=max(Year))\n\n  yr_min yr_max\n1   1961   2012"
  },
  {
    "objectID": "dplyr.html#combining-data-manipulation-functions-using-the-pipe",
    "href": "dplyr.html#combining-data-manipulation-functions-using-the-pipe",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.2 Combining data manipulation functions using the pipe %>%",
    "text": "9.2 Combining data manipulation functions using the pipe %>%\nIn most cases, you will find yourself wanting to combine several of dplyr’s data manipulation functions. For example,\n\ndat.yield  <- filter(dat, Information == \"Yield (Hg/Ha)\", \n                          Crop == \"Oats\",\n                          Year == 2012)\ndat.rename <- mutate(dat.yield, Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))\ndat.final  <- select(dat.rename, Country, Value)\n\nhead(dat.final, 3)\n\n  Country    Value\n1     CAN 24954.79\n2     USA 21974.70\n\n\nThe downside to this approach is the creation of several intermediate objects (e.g. dat.yield and dat.rename). This can make the workflow difficult to follow and clutter your R session with needless intermediate objects.\nAnother approach to combining dplyr operations is to use the piping operator ,%>%, which daisy chains dplyr operations. So our previous workflow could look like:\n\ndat.final <- dat %>%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Crop == \"Oats\",\n         Year == 2012)  %>% \n  mutate(Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\")) %>%\n  select(Country, Value)\n\nhead(dat.final, 3)\n\n  Country    Value\n1     CAN 24954.79\n2     USA 21974.70\n\n\nThe chunk of code can be read as “… with the dat table, filter by …, then mutate …., then select …” with the result from one operation being passed on to the next using the %>% operator. Note that the filter, mutate and select functions do not include the data table name making the chunk of code less cluttered and easier to read. The input data table dat appears just once at the beginning of the pipe.\n\n9.2.1 R has a native pipe too\nR has recently (as of version 4.1) added its own native pipe to its base function. Its infix operator is written as |>. In most code chunks covered in these tutorials, you can substitute %>% with |>. For example, you can write the previous code chunk as:\n\ndat.final <- dat  |> \n  filter(Information == \"Yield (Hg/Ha)\", \n         Crop == \"Oats\",\n         Year == 2012)   |>  \n  mutate(Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))  |> \n  select(Country, Value)\n\nThere are, however, a few subtle differences between the two. A deeper dive in how they differ can be found here.\nIn this course, we’ll stick with the %>% operator given that |> is new and is not yet as widely adopted as %>%."
  },
  {
    "objectID": "dplyr.html#conditional-statements",
    "href": "dplyr.html#conditional-statements",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.3 Conditional statements",
    "text": "9.3 Conditional statements\n  \n\n9.3.1 The base ifelse\nConditional statements are used when you want to create an output value that is conditioned on an evaluation. For example, if you want to output a value of 1 if an input value is less than 23 and a value of 0 otherwise, you can make use of the ifelse function as follows:\n\nx <- c(12,102, 43, 20, 90, 0, 12, 6)\nifelse(x < 23, 1, 0)\n\n[1] 1 0 0 1 0 1 1 1\n\n\nThe base ifelse function works as expected when the input/output values are numeric or character, but does not work as expected when applied to factors or dates. For example, if you wish to replace one factor level with another, the following example will not return the expected output.\n\nx <- as.factor( c(\"apple\", \"banana\", \"banana\", \"pear\", \"apple\"))\nifelse(x == \"pear\", \"apple\", x)\n\n[1] \"1\"     \"2\"     \"2\"     \"apple\" \"1\"    \n\n\nThe output is a character representation of the level number (recall that factors encode level values as numbers behind the scenes, i.e. apple =1, banana=2, etc…). Likewise, if you wish to replace an erroneous date with a missing value you will get:\n\nlibrary(lubridate)\ny <- mdy(\"1/23/2016\", \"3/2/2016\", \"12/1/1901\", \"11/23/2016\")\nifelse( year(y) != 2016, NA, y)\n\n[1] 16823 16862    NA 17128\n\n\nHere, ifelse converts the date object to its internal numeric representation as number of days since 1970.\nif_else does not preserve the attributes that might be present in a vector. In other words, it will strip away the vector’s class. If you want to ensure that the vector’s class is preserved, a safer alternative is to use dplyr’s if_else function.\n\n\n9.3.2 dplyr’s if_else\nThe if_else function (note the underscore _), will preserve data type but does so strictly. For example, the following code will return an error:\n\nif_else( year(y) != 2016, NA, y)\n\nError in `if_else()`:\n! `false` must be a logical vector, not a `Date` object.\n\n\nThe output data types (NA and y) are different. NA is a logical data type (see typeof(NA)) and y is not. The solution is to force NA as a date object by wrapping it with a date function like mdy(NA) or the base as.Date(NA) function, or by replacing the logical NA with NA_Date_ which is found in the lubridate package.\n\nif_else( year(y) != 2016, mdy(NA), y)\n\n[1] \"2016-01-23\" \"2016-03-02\" NA           \"2016-11-23\"\n\n\nLikewise, if the input vector is of type character, you need to ensure that all output values are characters too.\n\ny <- c(\"apple\", \"banana\", \"banana\", \"pear\", \"apple\")\nif_else( y == \"banana\", as.character(NA), y)\n\n[1] \"apple\" NA      NA      \"pear\"  \"apple\"\n\n\nRecall from the Week 02 lecture notes that R has several NA reserved words for different data types (e.g. NA_character_ and NA_integer_ to name a few). So the last chunk of code could have been written as,\n\nif_else( y == \"banana\", NA_character_, y)\n\n[1] \"apple\" NA      NA      \"pear\"  \"apple\"\n\n\nNote that there is no reserved word for an NA date or Posix type in the base R environment. But they are available in the lubridate package (NA_Date_ and NA_POSIXct_).\n\n\n9.3.3 Recoding factors using recode\nWhen working with factors, however, if_else (as of dplyr version 0.7) will produce the following error:\n\nif_else(x == \"pear\", \"apple\", x)\n\nError in `if_else()`:\n! `false` must be a character vector, not a `factor` object.\n\n\nThe first outcome listed in the function defines the data type or class. Here, \"apple\" is a character, so it expects the FALSE outcome to be a character as well (here, the FALSE outcome is a factor). A better option for recoding factors is to use dplyr’s recode function:\n\nrecode(x , \"pear\" = \"apple\")\n\n[1] apple  banana banana apple  apple \nLevels: apple banana\n\n\nYou can recode more than one factor level. In fact, you can even introduce new levels in the recoding scheme:\n\nrecode(x , \"pear\" = \"apple\", \"banana\" = \"pineapple\" )\n\n[1] apple     pineapple pineapple apple     apple    \nLevels: apple pineapple\n\n\nAs with if_else and case_when, recode is strict about preserving data types and classes. So if you want to recode a factor level to NA make sure to use NA_character_ or as.character(NA). recode will automatcially convert the character value to a factor if the input vector (x in our example) is a factor.\nNote that recode can also be used with numeric and character data types (i.e. vectors devoid of class attributes). For example,\n\n# Example of recoding a character vector\nrecode(y , \"pear\" = \"apple\", \"banana\" = \"pineapple\" )\n\n[1] \"apple\"     \"pineapple\" \"pineapple\" \"apple\"     \"apple\"    \n\n\n\n9.3.3.1 Note on replacing NA factor levels\nOne operation you cannot perform with recode is converting an NA level to another factor level. For example, the following will generate an error message:\n\nx[2] <- NA\nx\n\n[1] apple  <NA>   banana pear   apple \nLevels: apple banana pear\n\n\n\nrecode(x, NA = \"other\")\n\nError: unexpected '=' in \"recode(x, NA =\"\nThe simplest solution is to make use of a specialized factor package called forcats and its function, fct_explicit_na.\n\nlibrary(forcats)\nx <- fct_explicit_na(x, na_level = \"Other\")\nx\n\n[1] apple  Other  banana pear   apple \nLevels: apple banana pear Other\n\n\n\n\n\n9.3.4 Changing values based on multiple conditions: case_when\nifelse and if_else work great when a single set of conditions is to be satisfied. But if multiple sets of conditions are to be tested, nested if/else statements become cumbersome and are prone to clerical error. The following code highlights an example of nested if/else statements.\n\nunit <- c(\"F\",\"F\",\"C\", \"K\")\nif_else( unit == \"C\", \"Celsius\", if_else(unit == \"F\", \"Fahrenheit\", \"Kelvin\"))\n\n[1] \"Fahrenheit\" \"Fahrenheit\" \"Celsius\"    \"Kelvin\"    \n\n\nA simpler solution is to use the recode function discussed in the previous section.\n\nrecode(unit, \"C\" = \"Celsius\",\n             \"F\" = \"Fahrenheit\",\n             \"K\" = \"Kelvin\")\n\n[1] \"Fahrenheit\" \"Fahrenheit\" \"Celsius\"    \"Kelvin\"    \n\n\nrecode is well suited for replacing values but it will not allow for more complex operations. For example, given two vectors, unit and temp, we would like to convert all temp values to Fahrenheit by applying a temperature conversion dependent on the unit value.\n\ntemp <- c(45.2, 56.3, 11.0, 285)\n\nThis operation is best performed using the case_when function.\n\ncase_when(unit == \"F\" ~ temp,\n          unit == \"C\" ~ (temp * 9/5) + 32,\n          TRUE ~ (temp - 273.15) * 9/5 + 32)\n\n[1] 45.20 56.30 51.80 53.33\n\n\nThe last parameter, TRUE ~, applies to all conditions not satisfied by the previous two conditions (otherwise, not doing so would return NA values).\nNote that the order in which these conditions are listed matters since evaluation stops at the first TRUE outcome encountered. So, had the last condition been moved to the top of the stack, all temp values would be assigned the first conversion option.\n\n# What not to do ...\ncase_when(TRUE ~ (temp - 273.15) * 9/5 + 32,\n          unit == \"F\" ~ temp,\n          unit == \"C\" ~ (temp * 9/5) + 32)\n\n[1] -378.31 -358.33 -439.87   53.33\n\n\nAs with the if_else function, case_when is strict about data type in that all output must be of the same data type.\nNote that ifelse, if_else, recode and case_when can all be used inside of a mutate function. For example, to replace Canada and United States of America in variable Country with CAN and USA respectively and to create a new variable called Type which will take on the values of 1, 2 or 3 depending on the values in variable Source, type the following:\n\ndat1 <- dat %>% \n  mutate(Country = recode(Country, \"Canada\" = \"CAN\",\n                                   \"United States of America\" = \"USA\"),\n         Type = case_when(Source == \"Calculated data\" ~ 1,\n                          Source == \"Official data\" ~ 2,\n                          TRUE ~ 3)) \nhead(dat1)   \n\n  Country         Crop         Information Year      Value          Source Type\n1     CAN       Barley Area harvested (Ha) 2012 2060000.00   Official data    2\n2     CAN       Barley       Yield (Hg/Ha) 2012   38894.66 Calculated data    1\n3     CAN    Buckwheat Area harvested (Ha) 2012       0.00    FAO estimate    3\n4     CAN  Canary seed Area harvested (Ha) 2012  101900.00   Official data    2\n5     CAN  Canary seed       Yield (Hg/Ha) 2012   12161.92 Calculated data    1\n6     CAN Grain, mixed Area harvested (Ha) 2012   57900.00   Official data    2"
  },
  {
    "objectID": "dplyr.html#miscellaneous",
    "href": "dplyr.html#miscellaneous",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.4 Miscellaneous",
    "text": "9.4 Miscellaneous\n\n9.4.1 Replacing values with NA\nYou can replace a specific set of values (numeric or character) using the na_if() function. For example, to replace -999 with NA:\n\nval <- c(-999, 6, -1, -999)\nna_if( val , -999 )\n\n[1] NA  6 -1 NA\n\n\nLikewise, to replace all empty character values:\n\nval <- c(\"ab\", \"\", \"A b\", \"  \")\nna_if( val , \"\" )\n\n[1] \"ab\"  NA    \"A b\" \"  \" \n\n\nNote that na_if() will return the datatype passed to its first argument and not necessarily that of the parent vector. For example, if you use stringr’s str_length() function to identify zero length vectors in a character vector, you get a numeric output.\n\nlibrary(stringr)\nval <- c(\"ab\", \"\", \"A b\", \"  \")\nna_if( str_length(val) , 0 )\n\n[1]  2 NA  3  2\n\n\nTo use na_if() in a piping operation, it needs to be embedded in a mutate() function.\n\n\n9.4.2 Outputting a vector instead of a table using pull\nPiping operations will output a table, even if a single value is returned. For example, the following summarization operation returns the total oats yield as a data table:\n\noats <- dat %>% \n  filter(Crop == \"Oats\",\n         Information == \"Yield (Hg/Ha)\") %>% \n  summarise(Oats_sum = sum(Value))\noats\n\n  Oats_sum\n1  2169334\n\nclass(oats)\n\n[1] \"data.frame\"\n\n\nThere may be times when you want to output as a vector element and not a data table. To output a vector, use the pull() function.\n\noats <- dat %>% \n  filter(Crop == \"Oats\",\n         Information == \"Yield (Hg/Ha)\") %>% \n  summarise(Oats_sum = sum(Value)) %>% \n  pull()\noats\n\n[1] 2169334\n\nclass(oats)\n\n[1] \"numeric\"\n\n\nThe pull function can also be used to convert a data table column to a multi-element vector, e.g.:\n\n# This outputs a one column table\nyield <- dat %>% \n  filter(Crop == \"Oats\",\n         Information == \"Yield (Hg/Ha)\") %>% \n  select(Value)\n\nhead(yield)\n\n     Value\n1 24954.79\n2 21974.70\n3 29109.36\n4 20492.37\n5 27364.53\n6 23056.62\n\nclass(yield)\n\n[1] \"data.frame\"\n\n\n\n# This outputs a multi-element vector\nyield <- dat %>% \n  filter(Crop == \"Oats\",\n         Information == \"Yield (Hg/Ha)\") %>% \n  pull(Value)\n\nhead(yield)\n\n[1] 24954.79 21974.70 29109.36 20492.37 27364.53 23056.62\n\nclass(yield)\n\n[1] \"numeric\""
  },
  {
    "objectID": "group_by.html#summarizing-data-by-group",
    "href": "group_by.html#summarizing-data-by-group",
    "title": "10  Grouping and summarizing",
    "section": "10.1 Summarizing data by group",
    "text": "10.1 Summarizing data by group\nLet’s first create a dataframe listing the average delay time in minutes, by day of the week and by quarter, for Logan airport’s 2014 outbound flights.\n\ndf <- data.frame(\n  Weekday = factor(rep(c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\"), each = 4), \n                   levels = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\")),\n  Quarter = paste0(\"Q\", rep(1:4, each = 5)), \n  Delay = c(9.9, 5.4, 8.8, 6.9, 4.9, 9.7, 7.9, 5, 8.8, 11.1, 10.2, 9.3, 12.2,\n            10.2, 9.2, 9.7, 12.2, 8.1, 7.9, 5.6))\n\nThe goal will be to summarize the table by Weekday as shown in the following graphic.\n\nThe data table has three variables: Weekday, Quarter and Delay. Delay is the value we will summarize which leaves us with one variable to collapse: Quarter. In doing so, we will compute the Delay statistics for all quarters associated with a unique Weekday value.\nThis workflow requires two operations: a grouping operation using the group_by function and a summary operation using the summarise function. Here, we’ll compute two summary statistics: minimum delay time and maximum delay time.\n\nlibrary(dplyr)\n\ndf %>% \n  group_by(Weekday) %>% \n  summarize(min_delay = min(Delay), max_delay = max(Delay))\n\n# A tibble: 5 × 3\n  Weekday min_delay max_delay\n  <fct>       <dbl>     <dbl>\n1 Mon           5.4       9.9\n2 Tues          4.9       9.7\n3 Wed           8.8      11.1\n4 Thurs         9.2      12.2\n5 Fri           5.6      12.2\n\n\nNote that the weekday follows the chronological order as defined in the Weekday factor.\nYou’ll also note that the output is a tibble. This data class is discussed at the end of this page.\n\n10.1.1 Grouping by multiple variables\nYou can group by more than one variable. For example, let’s build another dataframe listing the average delay time in minutes, by quarter, by weekend/weekday and by inbound/outbound status for Logan airport’s 2014 outbound flights.\n\ndf2 <- data.frame(\n  Quarter = paste0(\"Q\", rep(1:4, each = 4)), \n  Week = rep(c(\"Weekday\", \"Weekend\"), each=2, times=4),\n  Direction = rep(c(\"Inbound\", \"Outbound\"), times=8),\n  Delay = c(10.8, 9.7, 15.5, 10.4, 11.8, 8.9, 5.5, \n            3.3, 10.6, 8.8, 6.6, 5.2, 9.1, 7.3, 5.3, 4.4))\n\nThe goal will be to summarize the delay time by Quarter and by Week type as shown in the following graphic.\n\nThis time, the data table has four variables. We are wanting to summarize by Quater and Week which leaves one variable, Direction, that needs to be collapsed.\n\ndf2 %>% \n  group_by(Quarter, Week) %>% \n  summarize(min_delay = min(Delay), max_delay = max(Delay))\n\n# A tibble: 8 × 4\n# Groups:   Quarter [4]\n  Quarter Week    min_delay max_delay\n  <chr>   <chr>       <dbl>     <dbl>\n1 Q1      Weekday       9.7      10.8\n2 Q1      Weekend      10.4      15.5\n3 Q2      Weekday       8.9      11.8\n4 Q2      Weekend       3.3       5.5\n5 Q3      Weekday       8.8      10.6\n6 Q3      Weekend       5.2       6.6\n7 Q4      Weekday       7.3       9.1\n8 Q4      Weekend       4.4       5.3\n\n\nThe following section demonstrates other grouping/summarizing operations on a larger dataset."
  },
  {
    "objectID": "group_by.html#a-working-example",
    "href": "group_by.html#a-working-example",
    "title": "10  Grouping and summarizing",
    "section": "10.2 A working example",
    "text": "10.2 A working example\nThe data file FAO_grains_NA.csv will be used in this exercise. This dataset consists of grain yield and harvest year by North American country. The dataset was downloaded from http://faostat3.fao.org/ in June of 2014.\nRun the following line to load the FAO data file into your current R session.\n\ndat <- read.csv(\"http://mgimond.github.io/ES218/Data/FAO_grains_NA.csv\", header=TRUE)\n\nMake sure to load the dplyr package before proceeding with the following examples.\n\nlibrary(dplyr)\n\n\n10.2.1 Summarizing by crop type\nThe group_by function will split any operations applied to the dataframe into groups defined by one or more columns. For example, if we wanted to get the minimum and maximum years from the Year column for which crop data are available by crop type, we would type the following:\n\ndat %>% \n  group_by(Crop) %>% \n  summarise(yr_min = min(Year), yr_max=max(Year))\n\n# A tibble: 11 × 3\n   Crop         yr_min yr_max\n   <chr>         <int>  <int>\n 1 Barley         1961   2012\n 2 Buckwheat      1961   2012\n 3 Canary seed    1980   2012\n 4 Grain, mixed   1961   2012\n 5 Maize          1961   2012\n 6 Millet         1961   2012\n 7 Oats           1961   2012\n 8 Popcorn        1961   1982\n 9 Rye            1961   2012\n10 Sorghum        1961   2012\n11 Triticale      1989   2012\n\n\n\n\n10.2.2 Count the number of records in each group\nIn this example, we are identifying the number of records by Crop type. There are two ways this can be accomplished:\n\ndat %>%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Year >= 2005 & Year <=2010, \n         Country==\"United States of America\") %>%\n  group_by(Crop) %>%\n  count()\n\nOr,\n\ndat %>%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Year >= 2005 & Year <=2010, \n         Country==\"United States of America\") %>%\n  group_by(Crop) %>%\n  summarise(Count = n())\n\n# A tibble: 7 × 2\n  Crop      Count\n  <chr>     <int>\n1 Barley        6\n2 Buckwheat     6\n3 Maize         6\n4 Millet        6\n5 Oats          6\n6 Rye           6\n7 Sorghum       6\n\n\nThe former uses the count() function and the latter uses the summarise() and n() functions.\n\n\n10.2.3 Summarize by mean yield and year range\nHere’s another example where two variables are summarized in a single pipe.\n\ndat.grp <- dat %>%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Year >= 2005 & Year <=2010, \n         Country==\"United States of America\") %>%\n  group_by(Crop) %>%\n  summarise( Yield = mean(Value), `Number of Years` = max(Year) - min(Year)) \n\ndat.grp\n\n# A tibble: 7 × 3\n  Crop       Yield `Number of Years`\n  <chr>      <dbl>             <int>\n1 Barley    35471.                 5\n2 Buckwheat 10418.                 5\n3 Maize     96151.                 5\n4 Millet    16548.                 5\n5 Oats      22619.                 5\n6 Rye       17132.                 5\n7 Sorghum   42258.                 5\n\n\n\n\n10.2.4 Normalizing each value in a group by the group median\nIn this example, we are subtracting each value in a group by that group’s median. This can be useful in identifying which year yields are higher than or lower than the median yield value within each crop group. We will concern ourselves with US yields only and sort the output by crop type. We’ll save the output dataframe as dat2.\n\ndat2 <- dat %>% \n  filter(Information == \"Yield (Hg/Ha)\",\n         Country == \"United States of America\") %>%\n  select(Crop, Year, Value)                     %>%\n  group_by(Crop)                                %>%\n  mutate(NormYield = Value - median(Value))     %>%\n  arrange(Crop)\n\nLet’s plot the normalized yields by year for Barley and add a 0 line representing the (normalized) central value.\n\nplot( NormYield ~ Year, dat2[dat2$Crop == \"Barley\",] )\nabline(h = 0, col=\"red\")\n\n\n\n\nThe relative distribution of points does not change, but the values do (they are re-scaled) allowing us to compare values based on some localized (group) context. This technique will prove very useful later on in the course when EDA topics are explored.\n\n\n10.2.5 dplyr’s output data structure\nSome of dplyr’s functions such as group_by/summarise generate a tibble data table. For example, the dat.grp object created in the last chunk of code is associated with a tb_df (a tibble).\n\nclass(dat.grp)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nA tibble table will behave a little differently than a data frame table when printing to a screen or subsetting its elements. In most cases, a tibble rendering of the table will not pose a problem in a workflow, however, this format may prove problematic with some older functions. To remedy this, you can force the dat.grp object to a standalone dataframe as follows:\n\ndat.df <- as.data.frame(dat.grp)\nclass(dat.df)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "tidyr.html#introduction",
    "href": "tidyr.html#introduction",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nData tables come in different sizes and shape; they can be a very simple two column dataset or they can consist of many columns and “sub-columns”. Understanding its structure, and learning how to reshape it into a workable form is critical to an effective and error free analysis.\nFor example, a median earnings data table downloaded from the U.S. census bureau’s website might look something like this:\n\nWe are conditioned to think of a table as consisting of three components: rows, columns and data values. Implicit in this paradigm is that each column represents a unique attribute. However, this may not always be the case. For example, in the above table, each column represents two distinct variables: gender and educational attainment (two distinct sets of attributes).\n\nAnother way of describing a dataset is by defining its variable(s), values and observations. In the above example, we have four variables: gender, education, region and income. Each variable consists of either categorical values (e.g. region, gender and education) or numerical values (income).\nAn observation consists of a unique set of attribute values. For example the values West Region, Female, Graduate and $57,914 make up one observation: there is just one instance of these combined values in the data. This perspective affords us another option in presenting the dataset: we can assign each column its own variable, and each row its own observation.\n\nNote that each row of the table is part of a unique set of variable attributes. A dataset in this format may not be human “readable” (unlike its wide counterpart), but is the format of choice for many data analysis and visualization operations.\nThe next sections will demonstrate how one can convert a wide format to a long format and vice versa."
  },
  {
    "objectID": "tidyr.html#wide-and-long-table-formats",
    "href": "tidyr.html#wide-and-long-table-formats",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.2 Wide and long table formats",
    "text": "11.2 Wide and long table formats\nA 2014 Boston (Logan airport) flight data summary table will be used in this exercise. The summary displays average mean delay time (in minutes) by day of the work week and quarter.\n\ndf <- data.frame( Weekday = c( \"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\" ),\n                  Q1      = c(  9.9 ,  4.9  ,  8.8 ,   12.2 ,  12.2 ),\n                  Q2      = c(  5.4 ,  9.7  , 11.1 ,   10.2 ,   8.1 ),\n                  Q3      = c(  8.8 ,  7.9  , 10.2 ,   9.2  ,   7.9 ),\n                  Q4      = c(  6.9 ,    5  ,  9.3 ,   9.7  ,   5.6 ) )\n\nReshaping a table involves modifying its layout (or “shape”). In our example, df is in a “wide” format.\n\ndf\n\n  Weekday   Q1   Q2   Q3  Q4\n1     Mon  9.9  5.4  8.8 6.9\n2    Tues  4.9  9.7  7.9 5.0\n3     Wed  8.8 11.1 10.2 9.3\n4   Thurs 12.2 10.2  9.2 9.7\n5     Fri 12.2  8.1  7.9 5.6\n\n\nThere are three unique variables: day of week, quarter of year, and mean departure delay.\n\n11.2.1 Creating a long table from a wide table\nA package that facilitates converting from wide to long (and vice versa) is tidyr. To go from wide to long we use the pivot_longer function. Note that if you are using a version of tidyr older than 1.0 you will want to use the gather() function..\nThe pivot_longer function takes three arguments:\n\ncols: list of columns that are to be collapsed. The columns can be referenced by column number or column name.\nnames_to: This is the name of the new column which will combine all column names (e.g. Q1, Q2, Q3 and Q4).\nvalues_to: This is the name of the new column which will combine all column values (e.g. average delay times) associated with each variable combination.\n\nIn our example, the line of code needed to re-express the table into a long form can be written in at least one of four ways:\n\nlibrary(tidyr)\ndf.long <- pivot_longer(df, cols=2:5, names_to = \"Quarter\", values_to = \"Delay\")\n# or\ndf.long <- pivot_longer(df, cols=-1, names_to = \"Quarter\", values_to = \"Delay\")\n# or\ndf.long <- pivot_longer(df, cols=Q1:Q4, names_to = \"Quarter\", values_to = \"Delay\")\n# or\ndf.long <- pivot_longer(df, cols=c(Q1,Q2,Q3,Q4), names_to = \"Quarter\", values_to = \"Delay\")\n\nAll four lines produce the same output, they differ only by how we are referencing the columns that are to be collapsed. Note that we assigned the names Quarter and Delay to the two new columns.\nThe first 10 lines of the output table are shown here. Note how each Delay value has its own row.\n\n\n   Weekday Quarter Delay\n1      Mon      Q1   9.9\n2     Tues      Q1   4.9\n3      Wed      Q1   8.8\n4    Thurs      Q1  12.2\n5      Fri      Q1  12.2\n6      Mon      Q2   5.4\n7     Tues      Q2   9.7\n8      Wed      Q2  11.1\n9    Thurs      Q2  10.2\n10     Fri      Q2   8.1\n\n\nThe following figure summarizes the wide to long conversion.\n\n\n\n11.2.2 Creating a wide table from a long table\nIf a table is to be used for a visual assessment of the values, a long format may be difficult to work with. A long table can be re-expressed into a wide form by picking the two variables that will define the new column names and values.\nContinuing with our example, we will convert df.long back to a wide format using the pivot_wider() function. This replaces the spread() function from earlier versions of tidyr (<1.0). The pivot_wider() function takes at least two arguments:\n\nnames_from: Variable whose values will be converted to column names.\nvalues_from: Variable whose values will populate the table’s block of cell values.\n\n\ndf.wide <- pivot_wider(df.long, names_from = Quarter, values_from = Delay) \n\nWe’ve now recreated the wide version of our table.\n\n\n# A tibble: 5 × 5\n  Weekday    Q1    Q2    Q3    Q4\n  <chr>   <dbl> <dbl> <dbl> <dbl>\n1 Mon       9.9   5.4   8.8   6.9\n2 Tues      4.9   9.7   7.9   5  \n3 Wed       8.8  11.1  10.2   9.3\n4 Thurs    12.2  10.2   9.2   9.7\n5 Fri      12.2   8.1   7.9   5.6\n\n\nThe following figure summarizes the long to wide conversion."
  },
  {
    "objectID": "tidyr.html#advanced-pivot_longer-options",
    "href": "tidyr.html#advanced-pivot_longer-options",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.3 Advanced pivot_longer options",
    "text": "11.3 Advanced pivot_longer options\nHere’s a subset of median income by sex and by work experience for 2017.\n\ndf2 <- data.frame(state = c(\"Maine\", \"Massachusetts\", \n                             \"New Hampshire\", \"Vermont\"),\n                   male_fulltime = c(50329,66066, 59962, 50530), \n                   male_other = c(18099, 18574, 20274, 17709), \n                   female_fulltime = c(40054, 53841, 46178, 42198),\n                   female_other = c(13781, 14981, 15121, 14422))\ndf2\n\n          state male_fulltime male_other female_fulltime female_other\n1         Maine         50329      18099           40054        13781\n2 Massachusetts         66066      18574           53841        14981\n3 New Hampshire         59962      20274           46178        15121\n4       Vermont         50530      17709           42198        14422\n\n\nAt first glance, it might seem that we have three variables as in the earlier example, but upon closer examination, we see that we can tease out two variables from the column names: sex (male and female) and work experience (fulltime and other).\n\npivot_longer has an argument, names_sep, that is passed the character that is used to delimit the two variable values. In our example, this character is _. Since the column values will be split across two variables we will also need to pass two column names to the names_to argument.\n\n\ndf2.long <- pivot_longer(df2, cols = -state, \n                         names_to = c(\"sex\",\"work\"), \n                         names_sep = \"_\", \n                         values_to = \"income\")\ndf2.long\n\n# A tibble: 16 × 4\n   state         sex    work     income\n   <chr>         <chr>  <chr>     <dbl>\n 1 Maine         male   fulltime  50329\n 2 Maine         male   other     18099\n 3 Maine         female fulltime  40054\n 4 Maine         female other     13781\n 5 Massachusetts male   fulltime  66066\n 6 Massachusetts male   other     18574\n 7 Massachusetts female fulltime  53841\n 8 Massachusetts female other     14981\n 9 New Hampshire male   fulltime  59962\n10 New Hampshire male   other     20274\n11 New Hampshire female fulltime  46178\n12 New Hampshire female other     15121\n13 Vermont       male   fulltime  50530\n14 Vermont       male   other     17709\n15 Vermont       female fulltime  42198\n16 Vermont       female other     14422"
  },
  {
    "objectID": "tidyr.html#advanced-pivot_wider-options",
    "href": "tidyr.html#advanced-pivot_wider-options",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.4 Advanced pivot_wider options",
    "text": "11.4 Advanced pivot_wider options\n \n\n11.4.1 Combining variable names when spreading\nContinuing with the df2.long dataframe, we can spread the long table back to a wide table while combining the sex and work variables. We’ll add the names_sep argument which defines the character to use to separate the two variable names. We’ll use a dot . separator in this example.\n\npivot_wider(df2.long, \n            names_from = c(sex,work), \n            values_from = income,\n            names_sep = \".\")\n\n# A tibble: 4 × 5\n  state         male.fulltime male.other female.fulltime female.other\n  <chr>                 <dbl>      <dbl>           <dbl>        <dbl>\n1 Maine                 50329      18099           40054        13781\n2 Massachusetts         66066      18574           53841        14981\n3 New Hampshire         59962      20274           46178        15121\n4 Vermont               50530      17709           42198        14422\n\n\n\n\n11.4.2 Spreading duplicate variable combinations\nIf your long table has more than one unique combination of variables, pivot_wider() will return a list. Note that if you have used the older version of tidyr (version <1.0) its spread() function would have returned an error.\n\ndf3 <- data.frame(var1 = c(\"a\", \"a\", \"b\", \"b\"),\n                  var2 = c(\"x\", \"x\", \"y\", \"y\"),\n                  val  = c(5,3,1,4))\ndf3\n\n  var1 var2 val\n1    a    x   5\n2    a    x   3\n3    b    y   1\n4    b    y   4\n\n\n\npivot_wider(df3, names_from = var2, values_from = val)\n\n# A tibble: 2 × 3\n  var1  x         y        \n  <chr> <list>    <list>   \n1 a     <dbl [2]> <NULL>   \n2 b     <NULL>    <dbl [2]>\n\n\nSince the intersections of a:x and b:y each have two possible values, the function returns a list of values. Assuming that the duplicate records are not an erroneous entry, you will need to instruct the function on how to summarize the multiple values using the values_fn argument. For example, to return the maximum value, type:\n\npivot_wider(df3, \n            names_from = var2, \n            values_from = val, \n            values_fn = list(val = min))\n\n# A tibble: 2 × 3\n  var1      x     y\n  <chr> <dbl> <dbl>\n1 a         3    NA\n2 b        NA     1\n\n\nYou’ll note the empty cells resulting from there not being a valid combination for a:y and b:x. You can specify the missing values using the values_fill argument. For example, to replace NA with 0 type:\n\npivot_wider(df3, names_from = var2, values_from = val, \n            values_fn   = list(val = min),\n            values_fill = list(val = 0))\n\n# A tibble: 2 × 3\n  var1      x     y\n  <chr> <dbl> <dbl>\n1 a         3     0\n2 b         0     1"
  },
  {
    "objectID": "tidyr.html#additional-manipulation-options",
    "href": "tidyr.html#additional-manipulation-options",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.5 Additional manipulation options",
    "text": "11.5 Additional manipulation options\nThe tidyr package offers other functions not directly tied to pivoting. Some of these functions are highlihghted next.\n\n11.5.1 Separating elements in one column into separate columns\nTo split a column into two or more columns based on a existing column’s delimited value, use the separate() function.\n\n# Let's first create a delimited table\ndf2.long <- pivot_longer(df2, cols = -1, \n                         names_to = \"var1\", \n                         values_to = \"income\")\ndf2.long\n\n# A tibble: 16 × 3\n   state         var1            income\n   <chr>         <chr>            <dbl>\n 1 Maine         male_fulltime    50329\n 2 Maine         male_other       18099\n 3 Maine         female_fulltime  40054\n 4 Maine         female_other     13781\n 5 Massachusetts male_fulltime    66066\n 6 Massachusetts male_other       18574\n 7 Massachusetts female_fulltime  53841\n 8 Massachusetts female_other     14981\n 9 New Hampshire male_fulltime    59962\n10 New Hampshire male_other       20274\n11 New Hampshire female_fulltime  46178\n12 New Hampshire female_other     15121\n13 Vermont       male_fulltime    50530\n14 Vermont       male_other       17709\n15 Vermont       female_fulltime  42198\n16 Vermont       female_other     14422\n\n# Split var1 column into two columns\ndf2.sep <- separate(df2.long, \n                    col = var1, \n                    sep = \"_\", \n                    into = c(\"sex\", \"work\"))\ndf2.sep\n\n# A tibble: 16 × 4\n   state         sex    work     income\n   <chr>         <chr>  <chr>     <dbl>\n 1 Maine         male   fulltime  50329\n 2 Maine         male   other     18099\n 3 Maine         female fulltime  40054\n 4 Maine         female other     13781\n 5 Massachusetts male   fulltime  66066\n 6 Massachusetts male   other     18574\n 7 Massachusetts female fulltime  53841\n 8 Massachusetts female other     14981\n 9 New Hampshire male   fulltime  59962\n10 New Hampshire male   other     20274\n11 New Hampshire female fulltime  46178\n12 New Hampshire female other     15121\n13 Vermont       male   fulltime  50530\n14 Vermont       male   other     17709\n15 Vermont       female fulltime  42198\n16 Vermont       female other     14422\n\n\n\n\n11.5.2 Splitting rows into multiple rows based on delimited values\nTo split delimited values across rows, use the separate_rows function.\n\nseparate_rows(df2.long, var1, sep = \"_\")\n\n# A tibble: 32 × 3\n   state         var1     income\n   <chr>         <chr>     <dbl>\n 1 Maine         male      50329\n 2 Maine         fulltime  50329\n 3 Maine         male      18099\n 4 Maine         other     18099\n 5 Maine         female    40054\n 6 Maine         fulltime  40054\n 7 Maine         female    13781\n 8 Maine         other     13781\n 9 Massachusetts male      66066\n10 Massachusetts fulltime  66066\n# … with 22 more rows\n\n\nNote that the output is a tibble even if the input is a dataframe.\n\n\n11.5.3 Replicate rows by count\nYou can expand rows based on a count column using the uncount() function. This is the opposite of a group_by(...) %>% count() operation that tallies up the observations based on a grouping variable. Here, we’ll replicate rows based on the column count value.\n\ndf2b <- data.frame(var1 = c(\"a\", \"b\"), \n                   var2 = c(\"x\", \"y\"), \n                   count = c(1, 3))\n\ndf2b\n\n  var1 var2 count\n1    a    x     1\n2    b    y     3\n\nuncount(df2b, count)\n\n  var1 var2\n1    a    x\n2    b    y\n3    b    y\n4    b    y\n\n\nIf you want to add an index column that identifies the replicated rows, add an .id argument.\n\nuncount(df2b, count, .id = \"id\")\n\n  var1 var2 id\n1    a    x  1\n2    b    y  1\n3    b    y  2\n4    b    y  3\n\n\n\n\n11.5.4 Combining elements from many columns into a single column\nAnother practical function in the tidyr package is unite(). It combines columns into a single column by chaining the contents of the combined columns. For example, the following table has hours, minutes and seconds in separate columns.\n\ndf <- data.frame(\n      Index = c(1,   2,  3),\n      Hour  = c(2,  14, 20),\n      Min   = c(34,  2, 55),\n      Sec   = c(55, 17, 23))\n\ndf\n\n  Index Hour Min Sec\n1     1    2  34  55\n2     2   14   2  17\n3     3   20  55  23\n\n\nTo combine the three time elements into a single column, type:\n\ndf2 <- unite(df,  col = Time, c(Hour, Min, Sec) , sep=\":\", remove =TRUE)\ndf2\n\n  Index     Time\n1     1  2:34:55\n2     2  14:2:17\n3     3 20:55:23\n\n\nThe col parameter defines the new column name; the parameter c(Hour, Min, Sec) defines the columns to be combined into column Time; sep=\":\" tells the function what characters are to be used to separate the elements (here, we are separating the time elements using :); remove=TRUE tells the function to remove columns two through four.\n\n\n11.5.5 Creating unique combinations of variable values\nYou can use expand_grid to automatically generate a table with unique combinations of a set of variable values. For example, to fill a table with a combination of student names and homework assignments, type:\n\ndf3.long <- expand_grid(\n  student    = c(\"Joe\", \"Jane\", \"Kim\"),  # Define all unique student names\n  assignment = c(paste0(\"HW\", 1:4)),     # Define all unique HW assignments\n  value      = NA)\ndf3.long\n\n# A tibble: 12 × 3\n   student assignment value\n   <chr>   <chr>      <lgl>\n 1 Joe     HW1        NA   \n 2 Joe     HW2        NA   \n 3 Joe     HW3        NA   \n 4 Joe     HW4        NA   \n 5 Jane    HW1        NA   \n 6 Jane    HW2        NA   \n 7 Jane    HW3        NA   \n 8 Jane    HW4        NA   \n 9 Kim     HW1        NA   \n10 Kim     HW2        NA   \n11 Kim     HW3        NA   \n12 Kim     HW4        NA   \n\n\nWe can then create a wide version of the table using pivot_wider.\n\npivot_wider(df3.long, names_from = assignment, values_from = value)\n\n# A tibble: 3 × 5\n  student HW1   HW2   HW3   HW4  \n  <chr>   <lgl> <lgl> <lgl> <lgl>\n1 Joe     NA    NA    NA    NA   \n2 Jane    NA    NA    NA    NA   \n3 Kim     NA    NA    NA    NA   \n\n\n\n\n11.5.6 Expanding table with missing sets of values\nIt’s not uncommon to be handed a table with incomplete combinations of observations. For example, the following table gives us yield and data source values for each combination of year and grain type. However, several combinations of year/grain are missing.\n\ndf4.long <- data.frame( Year  = c(1999,1999,2000,2000,2001,2003,2003,2005),\n                        Grain = c(\"Oats\", \"Corn\",\"Oats\", \"Corn\",\"Oats\", \"Oats\", \"Corn\",\"Oats\"),\n                        Yield = c(23,45,24,40,20,19,41,22),\n                        Src   = c(\"a\",\"a\",\"b\",\"c\",\"a\",\"a\",\"c\",\"a\"),\n                        stringsAsFactors = FALSE)\ndf4.long\n\n  Year Grain Yield Src\n1 1999  Oats    23   a\n2 1999  Corn    45   a\n3 2000  Oats    24   b\n4 2000  Corn    40   c\n5 2001  Oats    20   a\n6 2003  Oats    19   a\n7 2003  Corn    41   c\n8 2005  Oats    22   a\n\n\nWe are missing records for 2001 and Corn, 2003 and Corn, and data for both grains are missing for 2002 and 2004. To add rows for all missing pairs of year/grain values, use the complete function. Here, we’ll assign 0 to missing Yield values and NA to the Src values.\n\ndf.all <- complete(df4.long, Year=1999:2005, Grain=  c(\"Oats\", \"Corn\"),\n                   fill = list(Yield = 0, Src = NA))\ndf.all\n\n# A tibble: 14 × 4\n    Year Grain Yield Src  \n   <dbl> <chr> <dbl> <chr>\n 1  1999 Corn     45 a    \n 2  1999 Oats     23 a    \n 3  2000 Corn     40 c    \n 4  2000 Oats     24 b    \n 5  2001 Corn      0 <NA> \n 6  2001 Oats     20 a    \n 7  2002 Corn      0 <NA> \n 8  2002 Oats      0 <NA> \n 9  2003 Corn     41 c    \n10  2003 Oats     19 a    \n11  2004 Corn      0 <NA> \n12  2004 Oats      0 <NA> \n13  2005 Corn      0 <NA> \n14  2005 Oats     22 a    \n\n\nNOTE 1: The dataframe should be ungrouped using ungroup() before calling complete() if a group_by() operation was previously applied to the table.\nNOTE 2: If one of the columns used to complete the set of unique values is a factor, then passing that column name as an argument will automatically create unique sets of values from that factor’s levels.\nIf you want to show just the missing rows, use dplyr::anti_join().\n\ndplyr::anti_join(df.all, df4.long)\n\n# A tibble: 6 × 4\n   Year Grain Yield Src  \n  <dbl> <chr> <dbl> <chr>\n1  2001 Corn      0 <NA> \n2  2002 Corn      0 <NA> \n3  2002 Oats      0 <NA> \n4  2004 Corn      0 <NA> \n5  2004 Oats      0 <NA> \n6  2005 Corn      0 <NA> \n\n\n\n\n11.5.7 Filling date/time gaps in a table\n\n11.5.7.1 Example using a Date object class\nThis is another example that makes use of the complete function. Here, we seek to add missing date/time values in a time series table.\n\ndf.dt <- data.frame( Date = as.Date(c(\"2000-01-01\", \"2000-01-03\", \"2000-01-05\")),\n                     Value = c(1,3,5))\n\nTo fill the missing dates between the minimum and maximum dates in df.dt, we first need to create the sequence of dates between these minimum and maximum values. We could do this in the complete function, but it will be clearer to the reader if these operations are performed separately.\n\ndates.all <- seq(min(df.dt$Date), max(df.dt$Date), by = \"1 day\")\n\nNext, we pass the complete set of dates, dates.all to the complete function. We’ll assign NA to the missing Value values.\n\ndf.dt.all <- complete(df.dt, Date = dates.all, fill = list(Value = NA))\ndf.dt.all\n\n# A tibble: 5 × 2\n  Date       Value\n  <date>     <dbl>\n1 2000-01-01     1\n2 2000-01-02    NA\n3 2000-01-03     3\n4 2000-01-04    NA\n5 2000-01-05     5\n\n\nThe seq function used in this example has a special method for Date objects. As such, it accepts a unique set of incremental parameters (via the by = parameter). In the above example, we are asking the function to increment the date object by 1 day. Other increments can be applied to a sequenced Date object as shown in the following table.\n\n\n\n\n\n\n\nIncrement units\nDescription\n\n\n\n\nday\nX number of days between dates\n\n\nweek\nX number of weeks between dates\n\n\nmonths\nX number of months between dates\n\n\nquarter\nX number of quarters between dates\n\n\nyear\nX number of years between dates\n\n\n\n\n\n11.5.7.2 Example using a Posix object class\nRecall that if a time element is to be stored in a date, the date object becomes a Posix object class. This adds additional increment units to the seq function. In the following example, we create a new data frame (with a time value), then fill the table with date/times at 6 hour increments.\n\ndf.tm <- data.frame( Date = as.POSIXct(c(\"2000-01-01 18:00 EST\", \n                                         \"2000-01-02 6:00 EST\", \n                                         \"2000-01-03 12:00 EST\")),\n                     Value = c(1,3,5))\n\n\npos.all <- seq(min(df.tm$Date), max(df.tm$Date), by = \"6 hour\")\n\n\ndf.tm.all <- complete(df.tm, Date = pos.all, fill = list(Value = NA))\ndf.tm.all\n\n# A tibble: 8 × 2\n  Date                Value\n  <dttm>              <dbl>\n1 2000-01-01 18:00:00     1\n2 2000-01-02 00:00:00    NA\n3 2000-01-02 06:00:00     3\n4 2000-01-02 12:00:00    NA\n5 2000-01-02 18:00:00    NA\n6 2000-01-03 00:00:00    NA\n7 2000-01-03 06:00:00    NA\n8 2000-01-03 12:00:00     5\n\n\nIn this example the seq method for the Posix object adds a few more increment units to those available with the seq.Date method shown in the previous table. These include:\n\n\n\n\n\n\n\nIncrement units\nDescription\n\n\n\n\nsec\nX number of seconds between timestamps.\n\n\nmin\nX number of minutes between timestamps.\n\n\nhour\nX number of hours between timestamps.\n\n\nDSTday\nX number of days between timestamps while taking into account changes to/from daylight savings time.\n\n\n\n\n\n\n11.5.8 Identifying missing combination in dataframes\nIn an earlier example, we had the function automatically add the missing combinations using explicitly defined ranges of values. If you just want to output the missing combinations from the existing set of values in both columns, use the expand() function.\n\np.all <- expand(df4.long, Year, Grain) # List all possible combinations\np.all\n\n# A tibble: 10 × 2\n    Year Grain\n   <dbl> <chr>\n 1  1999 Corn \n 2  1999 Oats \n 3  2000 Corn \n 4  2000 Oats \n 5  2001 Corn \n 6  2001 Oats \n 7  2003 Corn \n 8  2003 Oats \n 9  2005 Corn \n10  2005 Oats \n\n\nNote that this only outputs the columns of interest. If you need to see the other columns in the output, perform a join.\n\ndplyr::left_join(p.all, df4.long, by=c(\"Year\", \"Grain\"))\n\n# A tibble: 10 × 4\n    Year Grain Yield Src  \n   <dbl> <chr> <dbl> <chr>\n 1  1999 Corn     45 a    \n 2  1999 Oats     23 a    \n 3  2000 Corn     40 c    \n 4  2000 Oats     24 b    \n 5  2001 Corn     NA <NA> \n 6  2001 Oats     20 a    \n 7  2003 Corn     41 c    \n 8  2003 Oats     19 a    \n 9  2005 Corn     NA <NA> \n10  2005 Oats     22 a    \n\n\n\n\n11.5.9 Auto-fill down or up\nThe fill() function is used to replace NA values with the closest non-NA value in a column. For example, to fill down, set the .direction argument to \"down\".\n\ndf5 <- data.frame(Month = 1:12, \n                 Year = c(2000, rep(NA, 4),2001, rep(NA,6)))\ndf5\n\n   Month Year\n1      1 2000\n2      2   NA\n3      3   NA\n4      4   NA\n5      5   NA\n6      6 2001\n7      7   NA\n8      8   NA\n9      9   NA\n10    10   NA\n11    11   NA\n12    12   NA\n\nfill(df5, Year, .direction=\"down\")\n\n   Month Year\n1      1 2000\n2      2 2000\n3      3 2000\n4      4 2000\n5      5 2000\n6      6 2001\n7      7 2001\n8      8 2001\n9      9 2001\n10    10 2001\n11    11 2001\n12    12 2001"
  },
  {
    "objectID": "joins.html",
    "href": "joins.html",
    "title": "12  Joining Data Tables",
    "section": "",
    "text": "We can use dplyr’s join operations to join elements from one table to another table. Four such functions (with differing behaviors) are left_join, right_join, inner_join, and full join.\nTo demonstrate these functions, we’ll be joining two dataframes: df and dj.\nIn the examples that follow, we will join both tables by the common column y."
  },
  {
    "objectID": "joins.html#left-join",
    "href": "joins.html#left-join",
    "title": "12  Joining Data Tables",
    "section": "12.1 Left join",
    "text": "12.1 Left join\nIn this example, if a join element in df does not exist in dj, NA will be assigned to column z. In other words, all elements in df will exist in the output regardless if a matching element is found in dj. Note that the output is sorted in the same order as df (the left table).\n\nleft_join(df, dj, by=\"y\")\n\n   x y     z\n1  1 a apple\n2 23 b  pear\n3  4 b  pear\n4 43 b  pear\n5  2 a apple\n6 17 d  <NA>"
  },
  {
    "objectID": "joins.html#right-join",
    "href": "joins.html#right-join",
    "title": "12  Joining Data Tables",
    "section": "12.2 Right join",
    "text": "12.2 Right join\nIf a join element in df does not exist in dj, that element is removed from the output. A few additional important notes follow:\n\nAll elements in dj appear at least once in the output (even if they don’t have a match in df in which case an NA value is added),\nThe output table is sorted in the order in which the y elements appear in dj.\nElement y will appear as many times as there matching ys in `df.\n\n\nright_join(df, dj, by=\"y\")\n\n   x y      z\n1  1 a  apple\n2 23 b   pear\n3  4 b   pear\n4 43 b   pear\n5  2 a  apple\n6 NA c orange"
  },
  {
    "objectID": "joins.html#inner-join",
    "href": "joins.html#inner-join",
    "title": "12  Joining Data Tables",
    "section": "12.3 Inner join",
    "text": "12.3 Inner join\nIn this example, only matching elements in both df and dj are saved in the output.\n\ninner_join(df, dj, by=\"y\")\n\n   x y     z\n1  1 a apple\n2 23 b  pear\n3  4 b  pear\n4 43 b  pear\n5  2 a apple"
  },
  {
    "objectID": "joins.html#full-join",
    "href": "joins.html#full-join",
    "title": "12  Joining Data Tables",
    "section": "12.4 Full join",
    "text": "12.4 Full join\nIn this example, all elements in both df and dj are present in the output. For non-matching pairs, NA values are supplied.\n\nfull_join(df, dj, by=\"y\")\n\n   x y      z\n1  1 a  apple\n2 23 b   pear\n3  4 b   pear\n4 43 b   pear\n5  2 a  apple\n6 17 d   <NA>\n7 NA c orange"
  },
  {
    "objectID": "joins.html#joins-in-a-piping-operation",
    "href": "joins.html#joins-in-a-piping-operation",
    "title": "12  Joining Data Tables",
    "section": "12.5 Joins in a piping operation",
    "text": "12.5 Joins in a piping operation\nThe afrementioned joining functions can be used with pipes. For example:\n\ndf %>% \n  left_join(dj, by = \"y\")\n\n   x y     z\n1  1 a apple\n2 23 b  pear\n3  4 b  pear\n4 43 b  pear\n5  2 a apple\n6 17 d  <NA>"
  },
  {
    "objectID": "joins.html#a-note-about-column-names",
    "href": "joins.html#a-note-about-column-names",
    "title": "12  Joining Data Tables",
    "section": "12.6 A note about column names",
    "text": "12.6 A note about column names\nIf the common columns in both tables have different names, you will need to modify the by = argument as by = c(\"left_col\" = \"right_col\"). For example,\n\nlibrary(dplyr)\n\ndf <- data.frame( x = c(1, 23, 4, 43, 2, 17),\n                  y1 = c(\"a\", \"b\", \"b\", \"b\", \"a\", \"d\"),\n                  stringsAsFactors = FALSE)\n\ndj <- data.frame( z = c(\"apple\", \"pear\", \"orange\"),\n                  y2 = c(\"a\", \"b\", \"c\"),\n                  stringsAsFactors = FALSE)\n\nleft_join(df, dj, by = c(\"y1\" = \"y2\"))\n\n   x y1     z\n1  1  a apple\n2 23  b  pear\n3  4  b  pear\n4 43  b  pear\n5  2  a apple\n6 17  d  <NA>"
  },
  {
    "objectID": "base_plots.html#loading-the-data",
    "href": "base_plots.html#loading-the-data",
    "title": "13  Base plotting environment",
    "section": "13.1 Loading the data",
    "text": "13.1 Loading the data\nThe data files used in this tutorial were created in an earlier exercise. Type the following command to download the objects:\n\nload(url(\"http://mgimond.github.io/ES218/Data/dat1_2.RData\"))\n\nThis should load several data frame objects into your R session (note that not all are used in this exercise). The dat1l dataframe is a long table version of the crop yield dataset.\n\nhead(dat1l, 3)\n\n  Year   Crop    Yield\n1 1961 Barley 16488.52\n2 1962 Barley 18839.00\n3 1963 Barley 18808.27\n\n\nThe dat1w dataframe is a wide table version of the same dataset.\n\nhead(dat1w, 3)\n\n  Year   Barley Buckwheat    Maize     Oats      Rye\n1 1961 16488.52  10886.67 39183.63 15171.26 11121.79\n2 1962 18839.00  11737.50 40620.80 16224.60 12892.77\n3 1963 18808.27  11995.00 42595.55 16253.04 11524.11\n\n\nThe dat2 dataframe is a wide table representation of income by county and by various income and educational attainment levels. The first few lines and columns are shown:\n\ndat2[1:3, 1:7]\n\n   County State B20004001 B20004002 B20004003 B20004004 B20004005\n1 Autauga    al     35881     17407     30169     35327     54917\n2 Baldwin    al     31439     16970     25414     31312     44940\n3 Barbour    al     25201     15643     20946     24201     42629"
  },
  {
    "objectID": "base_plots.html#base-plotting-functions",
    "href": "base_plots.html#base-plotting-functions",
    "title": "13  Base plotting environment",
    "section": "13.2 Base plotting functions",
    "text": "13.2 Base plotting functions\n  \n\n13.2.1 Point and line plots\nThe most commonly used plot function in R is plot() which generates both point and line plots. For example, to plot male population median income (dat2$B20004007) vs female population median income (dat2$B20004013) for each county, type:\n\nplot(B20004007 ~ B20004013, dat = dat2)\n\n\n\n\nThe above plot command takes two arguments: B20004007 ~ B20004013 which is to be read as plot variable B20004007 as a function of B20004013, and dat = dat2 which tells the plot function which data frame to extract the variables from. Another way to call this command is to type:\n\nplot(dat2$B20004007 ~ dat2$B20004013)\n\nThe plot function can take on many other arguments to help tweak the default plot options. For example, we may want to change the axis labels to something more descriptive than the table variable name:\n\nplot(B20004007 ~ B20004013, dat = dat2, \n      xlab = \"Female median income ($)\", \n      ylab = \"Male median income ($)\")\n\n\n\n\nThere are over 3000 observations in this dataset which makes it difficult the see what may be going on in the cloud of points. We can change the symbol type to solid fill,pch = 20, and set its color to 90% transparent (or 10% opaque) using the expression col = rgb(0, 0, 0, 0.10). The rgb() function defines the intensities for each of the display’s primary colors (on a scale of 0 to 1). The primary colors are red, green and blue. The forth value is optional and provides the fraction opaqueness with a value of 1 being completely opaque.\n\nplot(B20004007 ~ B20004013, dat = dat2, \n     xlab = \"Female median income ($)\", \n     ylab = \"Male median income ($)\", \n     pch = 20, col = rgb(0, 0, 0, 0.10) )\n\n\n\n\nThe plot could use additional tweaking, but it may be best to build the plot from scratch as will be demonstrated a few sections down.\nBy default, the plot command will plot points and not lines. To plot lines, add the type=\"l\" parameter to the plot function. For example, to plot oats crop yield as a function of year from our dat1w dataset, type:\n\nplot(Oats ~ Year, dat = dat1w, type=\"l\", \n     ylab = \"Oats yield (Hg/Ha)\" )\n\n\n\n\nTo plot both points and line, set the type parameter to \"b\" (for both). We’ll also set the point symbol to 20.\n\nplot(Oats ~ Year, dat = dat1w, type = \"b\", pch = 20, \n     ylab = \"Oats yield (Hg/Ha)\" )\n\n\n\n\nThe plot command can only graph on variable. If you want to add another variable, you will need to call the lines function. We will assign a different line type to this second variable (lty = 2):\n\nplot(Oats ~ Year, dat = dat1w, type = \"l\", \n     ylab = \"Oats yield (Hg/Ha)\" )\nlines(Barley ~ Year, dat = dat1w, lty = 2)\n\n\n\n\nNote how the plot does not automatically re-scale to accommodate the new line. The plot is a static object meaning that we need to define the axes limits before calling the original plot function. Both axes limits can be set using the xlim and ylim parameters. We don’t need to set the x-axis range since both variables cover the same year range. We will therefore only focus on the y-axis limits. We can grab both the minimum and maximum values for the variables Oats and Barley using the range function, then pass the range to the ylim parameter in the call to plot.\n\ny.rng <- range( c(dat1w$Oats, dat1w$Barley) )\nplot(Oats ~ Year, dat = dat1w, type = \"l\", ylim = y.rng,\n     ylab = \"Oats yield (Hg/Ha)\")\nlines(Barley ~ Year, dat = dat1w, lty = 2)\n\n\n\n\nPoint plots from different variables can also be combined into a single plot using the points function in lieu of the lines function. In the following example, male vs. female income for population having a high school degree (blue dots) and a Bachelor’s degree (red dots) will be overlaid on the same plot. We’ll also add a legend in the top-right corner.\n\ny.rng <- range( c(dat2$B20004009, dat2$B20004011) , na.rm = TRUE) \nx.rng <- range( c(dat2$B20004015, dat2$B20004017) , na.rm = TRUE) \n\n# Plot income for HS degree\nplot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0, 0, 1, 0.10),\n     xlab = \"Female median income ($)\", \n     ylab = \"Male median income ($)\", \n     xlim = x.rng, ylim = y.rng)\n\n# Add points for Bachelor's degree\npoints(dat2$B20004011 ~ dat2$B20004017, dat = dat2, pch = 20, \n       col = rgb(1,0,0,0.10))\n\n# Add legend\nlegend(\"topright\", c(\"HS Degree\", \"Bachelor's\"), pch = 20, \n       col = c(rgb(0, 0, 1, 1), rgb(1, 0, 0, 1) ))\n\n\n\n\nThe na.rm = TRUE option is added as a parameter in the range function to prevent an NA value in the data from returning an NA value in the range.\nPoint symbols are defined by a numeric code. The following figure shows the list of point symbols available in R along with their numeric designation as used with the pch = argument. The symbol’s color can be defined using the col parameter. For symbols 21 through 25, which have a two-color scheme, col applies to the outline color (blue in the following figure) and bg parameter applies to the fill color (red in the following figure).\n\n\n\n\n\nYou can define the color using the rgb() function, or by a color name such as col = \"red\" or col = \"bisque\". For a full list of color names, type colors() at a command prompt.\nLine types can also be customized in the plot function using the lty = parameter. There are six different line types, each identified by a number:\n\n\n\n\n\n\n\n13.2.2 Boxplots\nA boxplot is one of many graphical tools used to summarize the distribution of a data batch. The graphic consists of a “box” that depicts the range covered by 50% of the data (aka the interquartile range, IQR), a horizontal line that displays the median, and “whiskers” that depict 1.5 times the IQR or the largest (for the top half) or smallest (for the bottom half) values.\nFor example, we can summarize the income range for all individuals as follows:\n\nboxplot(dat2$B20004001, na.rm = TRUE)\n\n\n\n\nNote that the boxplot function has no option to specify the data frame as is the case with the plot function; we must therefore pass it both the data frame name and the variable as a single argument (i.e. dat2$B20004001).\nSeveral variables can be summarized on the same plot.\n\nboxplot(dat2$B20004001, dat2$B20004007, dat2$B20004013, \n        names = c(\"All\", \"Male\", \"Female\"), main = \"Median income ($)\")\n\n\n\n\nThe names argument labels the x-axis and the main argument labels the plot title.\nThe outliers can be removed from the plot, if desired, by setting the outline parameter to FALSE:\n\nboxplot(dat2$B20004001, dat2$B20004007, dat2$B20004013, \n        names = c(\"All\", \"Male\", \"Female\"), main = \"Median income ($)\", \n        outline = FALSE)\n\n\n\n\nThe boxplot graph can also be plotted horizontally by setting the horizontal parameter to TRUE:\n\nboxplot(dat2$B20004001, dat2$B20004007, dat2$B20004013, \n        names = c(\"All\", \"Male\", \"Female\"), main = \"Median income ($)\", \n        outline = FALSE, horizontal = TRUE)\n\n\n\n\nThe last two plots highlight one downside in using a table in a wide format: the long series of column names passed to the boxplot function. It’s more practical to store such data in long form. To demonstrate this, let’s switch back to the crop data. To plot all columns in dat1w, we would need to type:\n\nboxplot(dat1w$Barley, dat1w$Buckwheat, dat1w$Maize, dat1w$Oats,dat1w$Rye,\n        names=c(\"Barley\", \"Buckwheat\", \"Maize\", \"Oats\", \"Rye\"))\n\n\n\n\nIf you use the long version of that table, the command looks like this:\n\nboxplot(Yield ~ Crop, dat1l)\n\n\n\n\nwhere ~ Crop tells the function to split the boxplots across unique Crop levels.\nOne can order the boxplots based on the median values. By default, boxplot will order the boxplots following the factor’s level order. In our example, the crop levels are ordered alphabetically. To reorder the levels following the median values of yields across each level, we can use the reorder() function:\n\ndat1l$Crop.ord <- reorder(dat1l$Crop, dat1l$Yield, median)\n\nThis creates a new variable called Crop.ord whose values mirror those in variable Crop but differ in the underlying level order:\n\nlevels(dat1l$Crop.ord)\n\n[1] \"Buckwheat\" \"Rye\"       \"Oats\"      \"Barley\"    \"Maize\"    \n\n\nIf we wanted the order to be in descending order, we would prefix the value parameter with the negative operator as in reorder(dat1l$Crop, -dat1l$Yield, median).\nThe function reorder takes three arguments: the factor whose levels are to be reordered (Crop), the value whose quantity will determine the new order (Yield) and the statistic that will be used to summarize the values across each factor’s level (median).\nThe modified boxplot expression now looks like:\n\nboxplot(Yield ~ Crop.ord, dat1l)\n\n\n\n\nAlternatively, you could have embedded the reorder function directly in the plotting function.\n\nboxplot(Yield ~ reorder(Crop, Yield, median), dat1l)\n\n\n\n\n\n\n13.2.3 Histograms\nThe histogram is another form of data distribution visualization. It consists of partitioning a batch of values into intervals of equal length then tallying their count in each interval. The height of each bar represents these counts. For example, we can plot the histogram of maize yields using the hist function as follows:\n\nhist(dat1w$Maize, xlab = \"Maize\", main = NA)\n\n\n\n\nThe main = NA argument suppresses the plot title.\nTo control the number of bins add the breaks argument. The breaks argument can be passed different types of values. The simplest value is the desired number of bins. Note, however, that you might not necessarily get the number of bins defined with the breaks argument. For example assigning the value of 10 to breaks generates a 14 bin histogram.\n\nhist(dat1w$Maize, xlab = \"Maize\", main = NA, breaks = 10)\n\n\n\n\nThe documentation states that the breaks value “is a suggestion only as the breakpoints will be set to pretty values”. pretty refers to a function that rounds values to powers of 1, 2 or 5 times a power of 10.\nIf you want total control of the bin numbers, manually create the breaks as follows:\n\nn <- 10  # Define the number of bin\nminx <- min(dat1w$Maize, na.rm = TRUE)\nmaxx <- max(dat1w$Maize, na.rm = TRUE)\nbins <- seq(minx, maxx, length.out = n +1)\n\nhist(dat1w$Maize, xlab = \"Maize\", main = NA, breaks = bins)\n\n\n\n\n\n\n13.2.4 Density plot\nHistograms have their pitfalls. For example, the number of bins can drastically affect the appearance of a distribution. One alternative is the density plot which, for a series of x-values, computes the density of observations at each x-value. This generates a “smoothed” distribution of values.\nUnlike the other plotting functions, density does not generate a plot but a list object instead. But the output of density can be wrapped with a plot function to generate the plot.\n\ndens <- density(dat1w$Maize)\nplot(dens, main = \"Density distribution of Maize yields\")\n\n\n\n\nYou can control the bandwidth using the bw argument. For example:\n\ndens <- density(dat1w$Maize, bw = 4000)\nplot(dens, main = \"Density distribution of Maize yields\")\n\n\n\n\nThe bandwidth parameter adopts the variable’s units."
  },
  {
    "objectID": "base_plots.html#customizing-plots",
    "href": "base_plots.html#customizing-plots",
    "title": "13  Base plotting environment",
    "section": "13.3 Customizing plots",
    "text": "13.3 Customizing plots\nSo far, you have learned how to customize point and line symbols, but this may not be enough. You might want to modify other graphic elements such as the axes layout and label formats for publication. Let’s see how we can further customize a plot of median income for male and female population having attained a HS degree.\nFirst, we plot the points but omit the axes and its labels with the parameters axes = FALSE, xlab = NA, ylab = NA. We will want both axes to cover the same range of values, so we will use the range function to find min and max values for both male and female income.\nNext, we draw the x axis using the axis function. The first parameter to this function is a number that indicates which axis is to be drawn (i.e. 1=bottom x, 2=left y, 3=top x and 4=right y). We will then use the mtext function to place the axis label under the axis line.\n\n# Plot the points without the axes\nrng <- range(dat2$B20004009, dat2$B20004015, na.rm = TRUE)\nplot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0,0,0,0.15), \n     xlim = rng, ylim = rng, axes = FALSE, xlab = NA, ylab = NA )\n\n# Plot the x-axis\nlab <- c(\"5,000\", \"25,000\", \"45,000\", \"$65,000\")\naxis(1, at = seq(5000, 65000, length.out = 4),  label = lab)\n\n# Plot x label\nmtext(\"Female median income (HS degree)\", side = 1, line = 2)\n\n\n\n\nNext, we will tackle the y-axis. We will rotate both the tic labels and axis label horizontally and place the axis label at the top of the axis. This will involve a different approach to that used for the x-axis. First, we need to identify each plot region’s corner coordinate values using the par function. Second, we will use the text function instead of the mtext function to place the axis label.\nFirst, let’s plot the y-axis with the custom tic labels.\n\n# Plot the y-axis\naxis(2, las = 1, at = seq(5000,65000, length.out = 4), label = lab)\n\n\n\n\nNow let’s extract the plot’s corner coordinate values.\n\nloc <- par(\"usr\")\nloc\n\n[1]  3850 68650  3850 68650\n\n\nThe corner location coordinate values are in the plot’s x and y units. We want to place the label in the upper left hand corner whose coordinate values are loc[1]= 3850 and loc[2]= 68650.\n\ntext(loc[1], loc[4], \"Male median\\nincome\", pos = 3, adj = 1, xpd = TRUE)\n\n\n\n\nThe string \\n in the text \"Median\\nIncome\" is interpreted in R as being a carriage return–i.e it forces the text that follows this string to be printed on the next line. The other parameters of interest are pos and adj that position and adjust the label location (type ?axis for more information on axis parameters) and the parameter xpd=TRUE allows for the text function to display text outside of the plot region."
  },
  {
    "objectID": "base_plots.html#exporting-plots-to-image-file-formats",
    "href": "base_plots.html#exporting-plots-to-image-file-formats",
    "title": "13  Base plotting environment",
    "section": "13.4 Exporting plots to image file formats",
    "text": "13.4 Exporting plots to image file formats\nYou might need to export your plots as standalone image files for publications. R will export to many different raster image file formats such as jpg, png, gif and tiff, and several vector file formats such as PostScript, svg and PDF. You can specify the image resolution (in dpi), the image height and width, and the size of the margins.\nThe following example saves the last plot as an uncompressed tiff file with a 5”x6” dimension and a resolution of 300 dpi. This is accomplished by simply book-ending the plotting routines between the tiff() and dev.off() functions:\n\ntiff(filename = \"fig1.tif\", width = 5, height = 6, units = \"in\",\n     compression = \"none\", res = 300)\n\n  # Plot the points without the axes\n  rng <- range(dat2$B20004009, dat2$B20004015, na.rm = TRUE)\n  plot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0,0,0,0.10), \n       xlim = rng, ylim = rng, axes = FALSE, xlab = NA, ylab = NA )\n  # Plot the x-axis\n  lab <- c(\"5,000\", \"25,000\", \"45,000\", \"$65,000\")\n  axis(1, at = seq(5000,65000, length.out = 4),  label = lab)\n  # Plot x label\n  mtext(\"Female median income (HS degree)\", side = 1, line = 2)\n  # Plot the y-axis\n  axis(2, las = 1, at = seq(5000,65000, length.out = 4), label = lab)\n  text(loc[1], loc[4], \"Male median\\nincome\", pos = 3, adj = 1, xpd = TRUE)\n\ndev.off()\n\nTo save the same plot to a pdf file format, simply substitute tiff() with pdf() and adjust the parameters as needed:\n\npdf(file = \"fig1.pdf\", width = 5, height = 6)\n\n  # Plot the points without the axes\n  rng <- range(dat2$B20004009, dat2$B20004015, na.rm = TRUE)\n  plot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0,0,0,0.15), \n       xlim = rng, ylim = rng, axes = FALSE, xlab = NA, ylab = NA )\n  # Plot the x-axis\n  lab <- c(\"5,000\", \"25,000\", \"45,000\", \"$65,000\")\n  axis(1, at = seq(5000,65000, length.out=4),  label = lab)\n  # Plot x label\n  mtext(\"Female median income (HS degree)\", side = 1, line = 2)\n  # Plot the y-axis\n  axis(2, las = 1, at = seq(5000,65000, length.out = 4), label = lab)\n  text(loc[1], loc[4], \"Male median\\nincome\", pos = 3, adj = 1, xpd = TRUE)\n\ndev.off()"
  },
  {
    "objectID": "lattice_plot.html#data",
    "href": "lattice_plot.html#data",
    "title": "14  Lattice plotting environment",
    "section": "14.1 Data",
    "text": "14.1 Data\nThe data files used in this tutorial were created in an earlier exercise. Type the following command to download the objects\n\nload(url(\"http://mgimond.github.io/ES218/Data/dat1_2.RData\"))\n\nThis should load several data frame objects into your R session (note that not all are used in this exercise). The first three lines of data frames used in the following sections are shown below:\n\nhead(dat1l, 3)\n\n  Year   Crop    Yield\n1 1961 Barley 16488.52\n2 1962 Barley 18839.00\n3 1963 Barley 18808.27\n\nhead(dat1w, 3)\n\n  Year   Barley Buckwheat    Maize     Oats      Rye\n1 1961 16488.52  10886.67 39183.63 15171.26 11121.79\n2 1962 18839.00  11737.50 40620.80 16224.60 12892.77\n3 1963 18808.27  11995.00 42595.55 16253.04 11524.11\n\nhead(dat2b, 3)\n\n  State  County Level Gender value Region\n1    al Autauga   All    All 35881  South\n2    al Baldwin   All    All 31439  South\n3    al Barbour   All    All 25201  South\n\nhead(dat2c, 3)\n\n  State                 County Level Region   All     F     M\n1    ak Aleutians East Borough   All   West 21953 20164 22940\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885\n3    ak Aleutians East Borough    HS   West 20770 19671 21192"
  },
  {
    "objectID": "lattice_plot.html#the-lattice-package",
    "href": "lattice_plot.html#the-lattice-package",
    "title": "14  Lattice plotting environment",
    "section": "14.2 The lattice package",
    "text": "14.2 The lattice package\nThe lattice package is an implementation of the Trellis display (or multi-panel display) used to visualize multivariate data or, more specifically, to visualize the dependencies and interactions between multiple variables. For example, the following lattice (or Trellis) plot displays the counts of people (grouped by gender, adult/child and economic status or crew) who perished on the Titanic.\n\n\n\n\n\nIn the following example, a scatter plot is produced for different crime types by US region.\n\n\n\n\n\nExamples of some of the most common lattice plot types follow. Note that you will need to load the lattice package to run the following chunks of code.\n\nlibrary(lattice)"
  },
  {
    "objectID": "lattice_plot.html#displaying-univariate-distributions",
    "href": "lattice_plot.html#displaying-univariate-distributions",
    "title": "14  Lattice plotting environment",
    "section": "14.3 Displaying univariate distributions",
    "text": "14.3 Displaying univariate distributions\nLet’s look at a simple example where the density distribution of yields (across all years) is conditioned on crop type.\n\ndensityplot( ~ Yield | Crop, dat1l, plot.points = \"\", layout = c(5, 1))\n\n\n\n\nThe layout=c(5,1) parameter instructs the lattice function to layout the panels along 5 column and 1 row. If you wanted the plots to be displayed in 5 rows and 1 column you would simply type layout=c(1,5).\nThe first thing to note is that the x-scale axis range is the same for all five plots. At first, this may seem as an inefficient use of the range of x values–for example, the buckwheat density plot is squeezed in the left-hand side of the plot, but it is important to understand the purpose of a trellis plot: to facilitate comparison of plots. By ensuring that all scale ranges remain the same, we can easily compare the plot shape. In the above plot, it is clear that the yield of buckwheat remains consistent across the years compared to Maize which displays the greatest variability across the years.\nThe lattice package also has a histogram function:\n\nhistogram( ~ Yield | Crop, dat1l, plot.points = \"\", nint = 20, layout = c(5, 1))\n\n\n\n\nNote the importance of having a table shaped in “long form”. The density plots are conditioned on crop type which requires that crop type be treated as different values of a same variable (i.e. column Crop in dat1l). The above plot could not be generated had crop types been treated as variables (i.e. columns) as in dat1w.\nYou can also compare the distributions to a normal distribution (i.e. a Q-Q plot):\n\nqqmath( ~ Yield | Crop, dat1l, plot.points = \"\", layout = c(5, 1))\n\n\n\n\nBoxplots are another popular way to view the distribution of values. In the following example, we create boxplots of income values as a function of gender (value ~ Gender) and condition this comparison on educational levels (| Level). Since the variable Gender contains three unique values (M, F and All) we will remove all records tied to the whole population (All) by passing a condition, dat2b$Gender != \"All\" (where != is interpreted as “not equal to”), to the dat2b index.\n\nbwplot(value ~ Gender | Level, dat2b[dat2b$Gender != \"All\",] , layout = c(6,1))\n\n\n\n\nNote that bwplot automatically orders the conditional variable lexicographically unless the variable’s order is defined by its levels (the latter being the case in this example).\nWe may remove the outliers by passing the do.out = FALSE parameter to bwplot:\n\nbwplot(value ~ Gender | Level, dat2b[dat2b$Gender != \"All\",] , layout = c(6,1), do.out = FALSE)\n\n\n\n\nBut note that the y-axis range still reflects the outlier values. We will need to explicitly define the y limits using the ylim= parameter.\n\nbwplot(value ~ Gender | Level, dat2b[dat2b$Gender != \"All\",] , layout = c(6,1), do.out = FALSE, ylim=c(0,125000))"
  },
  {
    "objectID": "lattice_plot.html#visualizing-multivariate-data",
    "href": "lattice_plot.html#visualizing-multivariate-data",
    "title": "14  Lattice plotting environment",
    "section": "14.4 Visualizing multivariate data",
    "text": "14.4 Visualizing multivariate data\n  \n\n14.4.1 Basic line plots\nThe Lattice package allows us to plot several variables on a same plot. For example, we can plot yields vs year for each crop on a same plot:\n\nxyplot(Barley + Buckwheat + Maize + Oats + Rye ~ Year , dat1w, type = \"o\", pch=20,\n        auto.key = list(lines = TRUE, space = \"right\"),\n        main = \"Grain yield\", ylab = \"Yield\", xlab = \"Year\")\n\n\n\n\nNote the use of the wide version (dat1w) of the original dat1 dataset. Each crop is treated as its own variable (column) and is added to the plot using the + symbol.\nWe can also split the plots across a Trellis system by conditioning yields on crop type. But this now requires the use of the long form of the data (i.e. dat1l). The next plot also differs from the previous plot in that we substitute a point-line symbol (type=\"o\") with a line symbol only (type=\"l\").\n\nxyplot(Yield ~ Year | Crop, dat1l, type = \"l\", pch=20, layout=c(5,1),\n       main = \"Grain yield\", ylab = \"Yield\", xlab = \"Year\")\n\n\n\n\n\n\n14.4.2 Scatter plots\nOne key benefit of a Trellis system is its ability to generate bivariate scatter plots conditioned on one or more variables. For example, we may wish to compare incomes between female and male for each county and condition this relationship on educational attainment. This requires that male and female be assigned their own columns (since they are now treated as variables) and that educational attainment be assigned as another column whose values are each levels of educational attainment. We will therefore use dat2c.\n\ndat2c$Level <- factor(dat2c$Level, levels = c(\"All\", \"NoHS\", \"HS\", \"AD\", \"BD\", \"Grad\" )) \n\nxyplot( M ~ F | Level, dat2c, type = c(\"p\",\"g\"), pch=20, cex=0.6,\n        col.symbol=rgb(0,0,0,0.1), aspect=\"iso\", abline=c(0,1), layout=c(6,1),\n        xlab=\"Female income ($)\", ylab=\"Male income($)\")\n\n\n\n\nThe xyplot function is passed several parameters. The parameter type = c(\"p\",\"g\") instructs the function to generate both points, \"p\", and a background grid ,\"g\". Parameter type can accept many other options, a few are listed in the following table:\n\n\n\nOption\nAdds…\n\n\n\n\n\"p\"\nPoints\n\n\n\"l\"\nLines\n\n\n\"b\"\nBoth points and lines\n\n\n\"o\"\nBoth points and lines\n\n\n\"r\"\nRegression line\n\n\n\"g\"\nReference grid\n\n\n\"smooth\"\nLOESS fit\n\n\n\"spline\"\nCubic spline fit\n\n\n\nThe parameters pch and cex define the symbol shape and size. The parameter col.symbol defines the point color. The parameter aspect=\"iso\" sets the aspect ratio in such a way that a unit of measure along x matches that of y, this facilitates income comparison between both sexes since the scales match exactly. The parameter abline=c(0,1) generated a 45° line. If median incomes for male and female were identical in each county, the points would line up along the 45° line. Most points are above the line indicating that male income is greater than female income when aggregated at the county level.\nNote that the point distribution seems skewed. We can warp the values in a log() function to help reveal the relationship between male and female income:\n\nxyplot( log(M) ~ log(F) | Level, dat2c, type = c(\"p\",\"g\"), pch=20, cex=0.6,\n        col.symbol=rgb(0,0,0,0.1), aspect=\"iso\", abline=c(0,1), layout=c(6,1),\n        xlab=\"Log of female income ($)\", ylab=\"Log of male income($)\")\n\n\n\n\nWe can take this a step further. We could condition the relationship in income between male and female on both educational attainment and states or regions. We won’t condition the scatterplot on states as this would generate 50 x 6 separate plots, so we will condition the plots on region.\nNote that since the output will consist of 6 (levels) x 4 (regions) we will need to modify the layout parameter to layout=c(6,4). We will also stick with the log transformation of income since it seems to do a good job in helping reveal the relationship between both income values.\n\nxyplot( M ~ F | Level + Region, dat2c, type = c(\"p\",\"g\"), pch=20, cex=0.6,\n        col.symbol=rgb(0,0,0,0.1), aspect=\"iso\", abline=c(0,1), layout=c(6,4),\n        xlab=\"Log of female income ($)\", ylab=\"Log of male income($)\")"
  },
  {
    "objectID": "lattice_plot.html#customizing-trellis-plots",
    "href": "lattice_plot.html#customizing-trellis-plots",
    "title": "14  Lattice plotting environment",
    "section": "14.5 Customizing trellis plots",
    "text": "14.5 Customizing trellis plots\nVarious elements of a trellis plot can be customized by passing the graphic parameters to the trellis.par.set() function. To get the list of graphic parameters and their values call the trellis.par.get() function.\n\ntrellis.par.get()\n\nOutput for three parameters is shown below:\n\n\n $ fontsize         :List of 2\n  ..$ text  : num 12\n  ..$ points: num 8\n $ strip.background :List of 2\n  ..$ col  : chr [1:7] \"#DFDFDF\" \"#BFBFBF\" \"#9F9F9F\" \"#808080\" ...\n $ superpose.polygon:List of 5\n  ..$ alpha : num [1:7] 1 1 1 1 1 1 1\n  ..$ col   : chr [1:7] \"#FBB4AE\" \"#B3CDE3\" \"#CCEBC5\" \"#DECBE4\" ...\n  ..$ border: chr \"black\"\n  ..$ lty   : num [1:7] 1 1 1 1 1 1 1\n  ..$ lwd   : num [1:7] 1 1 1 1 1 1 1\n\n\nEach parameter consists of a list of elements. For example, the graphic parameter fontsize is made up of two modifiable elements: text and points. The following figure shows a few of the parameters and their associated graphical element:\n\nFor example, we will modify the Titanic survival barchart by changing the strip background colors, the bar polygon colors, the text size and color and the label size and color as follows:\n\n# Define an array of colors. The first array generates unique hues while the \n# second generates different shades of grey\ncol.qual <- c(\"#FBB4AE\",\"#B3CDE3\",\"#CCEBC5\",\"#DECBE4\",\"#FED9A6\",\"#FFFFCC\",\"#E5D8BD\")\ncol.grey <- c(\"#DFDFDF\",\"#BFBFBF\",\"#9F9F9F\",\"#808080\",\"#606060\",\"#404040\",\"#202020\")\n\n# Modify the Trellis parameters\ntrellis.par.set(superpose.polygon = list(col = col.qual, border = \"black\"))\ntrellis.par.set(strip.background = list(col = col.grey))\ntrellis.par.set(add.text = list(cex = 0.8, col=\"grey20\"))\ntrellis.par.set(par.xlab.text = list(cex = 0.8, col=\"grey20\"))\n\n# Now generate the strip charts\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), \n         groups = Survived, stack = TRUE, layout = c(4, 1),\n         auto.key = list(title = \"Survived\", columns = 2))\n\n\n\n\nYou’ll note that some of the elements are composed of more than one value such as strip.background’s col element which takes on 7 distinct color values. We did not need to modify all seven since only the first two were used (one for age group and the other for gender). Had we conditioned the plot on a third parameter, a third strip would have been added and the third color in col.grey would have been used. For example:\n\nbarchart(Class ~ Freq | Sex + Age + Survived, data = as.data.frame(Titanic), \n         groups = Survived, stack = TRUE, layout = c(4, 2),\n         auto.key = list(title = \"Survived\", columns = 2))\n\n\n\n\nNote the third strip (Yes vs No) with the third background color (#9F9F9F) in the color array col.grey."
  },
  {
    "objectID": "ggplot2.html#sample-data",
    "href": "ggplot2.html#sample-data",
    "title": "15  ggplot2 plotting environment",
    "section": "15.1 Sample data",
    "text": "15.1 Sample data\nThe data files used in this tutorial were created in an earlier exercise. Type the following command to download the objects.\n\nload(url(\"http://mgimond.github.io/ES218/Data/dat1_2.RData\"))\n\nThis should load several data frame objects into your R session (note that not all are used in this exercise). The dat1l dataframe is a long table version of the crop yield dataset.\n\nhead(dat1l, 3)\n\n  Year   Crop    Yield\n1 1961 Barley 16488.52\n2 1962 Barley 18839.00\n3 1963 Barley 18808.27\n\n\ndat1l2 adds Country to the dat1l dataframe.\n\nhead(dat1l2, 3)\n\n  Year   Crop Country    Yield\n1 2012 Barley  Canada 38894.66\n2 2012  Maize  Canada 83611.49\n3 2012   Oats  Canada 24954.79\n\n\nThe dat1w dataframe is a wide table version of dat1l.\n\nhead(dat1w, 3)\n\n  Year   Barley Buckwheat    Maize     Oats      Rye\n1 1961 16488.52  10886.67 39183.63 15171.26 11121.79\n2 1962 18839.00  11737.50 40620.80 16224.60 12892.77\n3 1963 18808.27  11995.00 42595.55 16253.04 11524.11\n\n\nThe dat2 dataframe is a wide table representation of income by county and by various income and educational attainment levels. The first few lines and columns are shown:\n\ndat2[1:3, 1:7]\n\n   County State B20004001 B20004002 B20004003 B20004004 B20004005\n1 Autauga    al     35881     17407     30169     35327     54917\n2 Baldwin    al     31439     16970     25414     31312     44940\n3 Barbour    al     25201     15643     20946     24201     42629\n\n\ndat2c is a long version of dat2\n\nhead(dat2c, 3)\n\n  State                 County Level Region   All     F     M\n1    ak Aleutians East Borough   All   West 21953 20164 22940\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885\n3    ak Aleutians East Borough    HS   West 20770 19671 21192"
  },
  {
    "objectID": "ggplot2.html#the-ggplot2-package",
    "href": "ggplot2.html#the-ggplot2-package",
    "title": "15  ggplot2 plotting environment",
    "section": "15.2 The ggplot2 package",
    "text": "15.2 The ggplot2 package\nThe ggplot2 package is designed around the idea that statistical graphics can be decomposed into a formal system of grammatical rules. The ggplot2 learning curve is the steepest of all graphing environments encountered thus far, but once mastered it affords the greatest control over graphical design. For an up-to-date list of ggplot2 functions, you may want to refer to ggplot2’s website.\nA plot in ggplot2 consists of different layering components, with the three primary components being:\n\nThe dataset that houses the data to be plotted;\nThe aesthetics which describe how data are to be mapped to the geometric elements (color, shape, size, etc..);\nThe geometric elements to use in the plot (i.e. points, lines, rectangles, etc…).\n\nAdditional (optional) layering components include:\n\nStatistical elements such as smoothing, binning or transforming the variable\nFacets for conditional or trellis plots\nCoordinate systems for defining the plots shape (i.e. cartesian, polar, spatial map projections, etc…)\n\nTo access ggplot2 functions, you will need to load its library:\n\nlibrary(ggplot2)\n\nFrom a grammatical perspective, a scientific graph is the conversion of data to aesthetic attributes and geometric objects. This is an important concept to grasp since it underlies the construction of all graphics in ggplot2.\nFor example, if we want to generate a point plot of crop yield as a function of year using the dat1l data frame, we type:\n\nggplot(dat1l , aes(x = Year, y = Yield)) + geom_point()\n\n\n\n\nwhere the function, ggplot(), is passed the data frame name whose content will be mapped; the aes() function is given data-to-geometry mapping instructions (Year is mapped to the x-axis and Yield is mapped to the y-axis); and geom_line() is the geometry type.\n\nIf we wanted to include a third variable, crop type, to the map, we would need to map its aesthetics: here we’ll map Crop to the color element of the geom.\n\nggplot(dat1l , aes(x = Year, y = Yield, color = Crop)) + geom_point()\n\n\n\n\nThe parameter color acts as a grouping parameter whereby the groups are assigned unique colors.\n\nIf we want to plot lines instead of points, simply substitute the geometry type with the geom_line() geometry.\n\nggplot(dat1l , aes(x = Year, y = Yield, color = Crop)) + geom_line()\n\n\n\n\nNote that the aesthetics are still mapped in the same way with Year mapped to the x coordinate, Yield mapped to the y coordinate and Crop mapped to the geom’s color.\nNote that the parameters x= and y= can be omitted from the syntax reducing the line of code to:\n\nggplot(dat1l , aes(Year, Yield, color = Crop)) + geom_line()"
  },
  {
    "objectID": "ggplot2.html#geometries",
    "href": "ggplot2.html#geometries",
    "title": "15  ggplot2 plotting environment",
    "section": "15.3 Geometries",
    "text": "15.3 Geometries\nExamples of a few available geometric elements follow.\n\n15.3.1 geom_line\ngeom_line generates line geometries. We’ll use data from dat1w to generate a simple plot of oat yield as a function of year.\n\nggplot(dat1w, aes(x = Year, y = Oats)) + geom_line() \n\n\n\n\nParameters such as color and linetype can be passed directly to the geom_line() function:\n\nggplot(dat1w, aes(x = Year, y = Oats)) + \n  geom_line(linetype = 2, colour = \"blue\", size=0.4) \n\n\n\n\nNote the difference in how colour= is implemented here. It’s no longer mapping a variable’s levels to a range of colors as when it’s called inside of the aes() function, instead, it’s setting the line color to blue.\n\n\n15.3.2 geom_point\nThis generates point geometries. This is often used in generating scatterplots. For example, to plot male income (variable B20004013) vs female income (variable B20004007), type:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) \n\n\n\n\nWe modify the point’s transparency by passing the alpha=0.3 parameter to the geom_point function. Other parameters that can be passed to point geoms include colour, pch (point symbol type) and cex (point size as a fraction).\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(colour = \"red\", pch=3 , alpha = 0.3, cex=0.6) \n\n\n\n\n\n\n15.3.3 geom_hex\nWhen a bivariate scatter plot has too many overlapping points, it may be helpful to bin the observations into regular hexagons. This provides the number of observations per bin.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_hex(binwidth = c(1000, 1000))  \n\n\n\n\nThe binwidth argument defines the width and height of each bin in the variables’ axes units.\n\n\n15.3.4 geom_boxplot\nIn the following example, a boxplot of Yield is generated for each crop type.\n\nggplot(dat1l, aes(x = Crop, y = Yield)) + geom_boxplot(fill = \"bisque\") \n\n\n\n\nIf we want to generate a single boxplot (for example for all yields irrespective of crop type) we need to pass a dummy variable to x=:\n\nggplot(dat1l, aes(x = \"\", y = Yield)) + \n  geom_boxplot(fill = \"bisque\") + xlab(\"All crops\")\n\n\n\n\n\n\n15.3.5 geom_violin\nA violin plot is a symmetrical version of a density plot which provides greater detail of a sample’s distribution than a boxplot.\n\nggplot(dat1l, aes(x = \"\", y = Yield)) + geom_violin(fill = \"bisque\") \n\n\n\n\n\n\n15.3.6 geom_histogram\nHistograms can be plotted for single variables only (unless faceting is used) as can be noted by the absence of a y= parameter in aes():\n\nggplot(dat1w, aes(x = Oats)) + geom_histogram(fill = \"grey50\") \n\n\n\n\nThe bin widths can be specified in terms of the value’s units. In our example, the unit is yield of oats (in Hg/Ha). So if we want to generate bin widths that cover 1000 Hg/Ha, we can type,\n\nggplot(dat1w, aes(x = Oats)) + \n  geom_histogram(fill = \"grey50\", binwidth = 1000) \n\n\n\n\nIf you want to control the number of bins, use the parameter bins= instead. For example, to set the number of bins to 8, modify the above code chunk as follows:\n\nggplot(dat1w, aes(x = Oats)) + \n  geom_histogram(fill = \"grey50\", bins = 8) \n\n\n\n15.3.7 geom_bar\nBar plots are used to summaries the counts of a categorical value. For example, to plot the number of counties in each state (note that each record in dat2 is assigned a county):\n\nggplot(dat2, aes(State)) + geom_bar()\n\n\n\n\nTo sort the bars by length we need to rearrange the State factor level order based on the number of counties in each state (which is the number of times a state appears in the data frame). We’ll make use of forcats’s fct_infreq function to reorder the State factor levels based on frequency.\n\nlibrary(forcats)\nggplot(dat2, aes(fct_infreq(State,ordered = TRUE))) + geom_bar()\n\n\n\n\nIf we want to reverse the order (i.e. plot from smallest number of counties to greatest), wrap the fct_infreq function with fct_rev.\n\nggplot(dat2, aes(fct_rev(fct_infreq(State,ordered = TRUE)))) + geom_bar()\n\n\n\n\nThe geom_bar function can also be used with count values (i.e. variable already summarized by count). First, we’ll summaries the number of counties by state using the dplyr package. This will generate a data frame with just 51 records: one for each of the 50 states and the District of Columbia.\n\nlibrary(dplyr)\ndat2.ct <- dat2 %>% group_by(State) %>% \n                summarize(Counties = n())\nhead(dat2.ct)\n\n# A tibble: 6 × 2\n  State Counties\n  <fct>    <int>\n1 ak          28\n2 al          67\n3 ar          74\n4 az          15\n5 ca          57\n6 co          62\n\n\nWhen using summarized data, we must pass the parameter stat=\"identity\" to the geom_bar function. We must also explicitly map the x and y axes geometries. Note that since we are now generating bar heights from a value field and not a frequency, we will need to use another forcats ordering function called fct_reorder. This function takes three parameters: the variable to be ordered (State), the variable whose values will determine the order (Counties) and the function, fun=, which defines the statistic used to summaries the sorting variable. Since there is just one value per state, we can use any summary statistic such as median, mean, sum, min or max.\n\nggplot(dat2.ct, aes(x=fct_reorder(State, Counties, .fun = median), y = Counties)) + \n       geom_bar(stat = \"identity\")\n\n\n\n\nNote that you can replace fct_reorder with the base function reorder for succinctness sake.\n\n\n15.3.8 dot plot\nThe dot plot is an alternative way to visualize counts as a function of a categorical variable. Instead of mapping State to the x-axis, we’ll map it to the y-axis.\n\nggplot(dat2.ct , aes(x = Counties, y = State)) + geom_point()\n\n\n\n\nDot plot graphics benefit from sorting–more so then bar plots. Here, we’ll make use of forcats::fct_reorder function (see last section on geom_bar).\n\nggplot(dat2.ct , aes(x = Counties, y = fct_reorder(State, Counties, .fun = median))) + \n       geom_point()\n\n\n\n\n\n\n15.3.9 Combining geometries\nGeometries can be layered. For example, to overlay a linear regression line to the data we can add the geom_smooth layer:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(alpha = 0.3) + \n  geom_smooth(method = \"lm\")\n\n\n\n\nThe geom_smooth can be used to fit other lines such as a loess:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"loess\")\n\n\n\n\nThe confidence interval can be removed from the smooth geometry by specifying se = FALSE.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"loess\", se = FALSE)\n\n\n\n\n\n\n15.3.10 Combining datasets\nYou can plot different datasets on the same ggplot canvas. This approach usually requires that each geom be assigned its own dataset and aesthetics. In the following example, we’ll compute the High Density Region (HDR) boxplot parameters using the hdrcde package, then use these parameters to construct geom_rect() that will be added to a geom_point element built from the parent dataset. When data and aesthetics are defined in a geom function, the ggplot() function must be explicitly called right from the beginning.\n\nlibrary(hdrcde)\n\n# Compute the HDR boxplot\nhdr1d <- hdr(dat2$B20004013, prob = c(25, 50, 99))\n\n# Build the data frame that will store the geom_rect variables\nhdr.df <- data.frame(hdr1d$hdr)\ncols <- grey(1/(1:nrow(hdr.df)) )\n\n# Build the plot\n  ggplot() +\n  geom_point(data = dat2, aes(x = 0 , y = B20004013), pch = 16, alpha = 0.5) +\n  geom_rect(dat = hdr.df, aes(\n    xmin = -0.2, xmax = 0.2,\n    ymin = X1, ymax = X2),\n    fill = cols,  colour = \"grey\") +\n  theme(axis.title.x = element_blank(),  # Remove x-axis \n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\nThe HBR boxplot is designed to visualize multi-modality on the dataset (i.e. data having more than one peak). In the above example, the boxes are encompassing 25%, 50% and 99% of the data."
  },
  {
    "objectID": "ggplot2.html#tweaking-a-ggplot2-graph",
    "href": "ggplot2.html#tweaking-a-ggplot2-graph",
    "title": "15  ggplot2 plotting environment",
    "section": "15.4 Tweaking a ggplot2 graph",
    "text": "15.4 Tweaking a ggplot2 graph\n \n\n15.4.1 Plot title\nYou can add a plot title using the ggtitle function.\n\nggplot(dat2, aes(State)) + geom_bar() + ggtitle(\"Number of counties by state\")\n\n\n\n\n\n\n15.4.2 Axes titles\nAxes titles can be explicitly defined using the xlab() and ylab() functions.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n    xlab(\"Female income ($)\") + ylab(\"Male income ($)\")\n\n\n\n\nTo remove axis labels, simply pass NULL to the functions as in xlab(NULL) and ylab(NULL).\n\n\n15.4.3 Axes labels\nYou can customize an axis’ label elements. If you are mapping continuous values along the x and y axes, use the scale_x_continuous() and scale_y_continuous() functions. For example, to specify where to place the tics and the accompanying labels, type:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n      xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n      scale_x_continuous(breaks = c(10000, 30000, 50000),\n                         labels = c(\"$10,000\", \"$30,000\", \"$50,000\"))\n\n\n\n\nIf you want to change the label formats whereby the numbers are truncated to a thousandth of their original value, you can make use of unit_format() from the scales package:\n\nggplot(dat2, aes(x=B20004013, y=B20004007)) + geom_point(alpha=0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(labels=scales::unit_format(suffix=\"k\", \n                                                     scale=0.001, \n                                                     sep=\"\")) +\n       scale_y_continuous(labels=scales::unit_format(suffix=\"k\", \n                                                     scale=0.001, \n                                                     sep=\"\"))\n\n\n\n\nThe scales package also has a comma_format() function that will add commas to large numbers:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n        xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n        scale_x_continuous(labels = scales::comma_format()) +\n        scale_y_continuous(labels = scales::comma_format())\n\n\n\n\nYou can rotate axes labels using the theme function.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nThe hjust argument justifies the values horizontally. Its value ranges from 0 to 1 where 0 is completely left-justified and 1 is completely right-justified. Note that the justification is relative to the text’s orientation and not to the axis. So it may be best to first rotate the label values, then to adjust justification based on the plot’s look as needed.\nIf you want the label values rotated 90° you might also need to justify vertically (relative to the text’s orientation) using the vjust argument where 0 is completely top-justified and 1 is completely bottom-justified.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0)) \n\n\n\n\n\n\n15.4.4 Axes limits\nThe axis range can be set using xlim() and ylim().\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       xlim(10000, 75000) + ylim(10000, 75000)\n\n\n\n\nHowever, if you are calling the scale_x_continuous() and scale_y_continuous() functions, you do not want to use xlim and ylim instead, you should add the limit= argument to the aforementioned functions. For example,\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format()) +\n       scale_y_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format())\n\n\n\n\n\n\n15.4.5 Axes breaks\nYou can explicitly define the breaks with the breaks argument. Continuing with the last example, we get:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format(),\n                          breaks = c(10000, 30000, 50000, 70000)) +\n       scale_y_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format(),\n                          breaks = c(10000, 30000, 50000, 70000))\n\n\n\n\nNote that the breaks argument can be used in conjunction with other arguments (as shown in this example), or by itself.\n\n\n15.4.6 Axes and data transformations\nIf you wish to apply a non-linear transformation to either axes (while preserving the untransformed axis values) add the coord_trans() function as follows:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_trans(x = \"log\")\n\n\n\n\nYou can also transform the y-axis by specifying the parameter y=. The log transformation defaults to the natural log. For a log base 10, use \"log10\" instead. For a square root transformation, use \"sqrt\". For the inverse use \"reciprocal\".\nAdvanced transformations can be called via the scales package. For example, to implement the box-cox transformation (with a power of -0.3), type:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_trans(x = scales::boxcox_trans(-0.3))\n\n\n\n\nNote that any statistical geom (such as the regression line) will be applied to the un-transformed data. So a linear model may end up looking non-linear after an axis transformation:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       geom_smooth(method = \"lm\", se = FALSE) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_trans(x = \"log\")\n\n\n\n\nIf a linear fit is to be applied to the transformed data, a better alternative is to transform the values instead of the axes. The transformation can be done on the original data or it can be implemented in ggplot using the scale_x_continuous and scale_y_continuous functions.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       geom_smooth(method = \"lm\", se = FALSE) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(trans = \"log\", breaks = seq(10000,60000,10000))\n\n\n\n\nThe scale_x_continuous and scale_y_continuous functions will accept scales transformation parameters–e.g. scale_x_continuous(trans = scales::boxcox_trans(-0.3)). Note that the parameter breaks is not required but is used here to highlight the transformed nature of the axis.\n\n\n15.4.7 Aspect ratio\nYou can impose an aspect ratio to your plot using the coord_equal() function. For example, to set the axes units equal (in length) to one another set ratio=1:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       geom_smooth(method = \"lm\") +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_equal(ratio = 1)\n\n\n\n\n\n\n15.4.8 Colors\nYou can customize geom colors using one of two sets of color schemes: one for continuous values, the other for categorical (discrete) values.\n\n\n\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nscale_colour_gradient scale_colour_gradient2 scale_color_distiller scale_fill_gradient2 scale_fill_gradient scale_fill_distiller\nscale_colour_hue scale_colour_grey scale_colour_manual scale_colour_brewer\n\n\n\nA few examples follow.\n\n15.4.8.1 Continuous color schemes\nThe following chunk of code summarizes dat2 by tallying the number of counties in each state and by computing the median county income values.\n\ndat2.ct2 <- dat2 %>% group_by(State) %>% \n  summarize(Counties = n(), Income = median(B20004001))\nhead(dat2.ct2)\n\n# A tibble: 6 × 3\n  State Counties Income\n  <fct>    <int>  <dbl>\n1 ak          28 33980.\n2 al          67 28946 \n3 ar          74 26320.\n4 az          15 28799 \n5 ca          57 33438 \n6 co          62 30892.\n\n\nThe following chunk applies a green to red color gradient fill to each bar based on the median county incomes. Note that we are using the summarized count table (and not the original dat2 table). Recall that when plotting bars from counts that are already tabulated we must specify stat=\"identity\" in the geom_bar function.\n\nggplot(dat2.ct2, aes(x = fct_reorder(State, Counties), y = Counties, fill = Income)) + \n       geom_bar(stat = \"identity\") + \n       scale_fill_gradient(low = \"green\", high = \"red\")\n\n\n\n\nThe following chunk applies a divergent color scheme while allowing one to specify the central value of this scheme. Note that the colors are symmetrical about the midpoint which may result in only a partial range of the full possible gradient of colors.\n\nggplot(dat2.ct2, aes(x = fct_reorder(State, Counties), y = Counties, fill = Income)) + \n       geom_bar(stat = \"identity\") + \n       scale_fill_gradient2(low = \"darkred\", mid = \"white\",  high = \"darkgreen\", \n                            midpoint = 30892)\n\n\n\n\nIn the last two chunk, we filled the bars with colors (note the use of functions with the string _fill_). When assigning color to point or line symbols, use the function with the _colour_ string. For example:\n\nggplot(dat2.ct2, aes(y = fct_reorder(State, Counties), x = Counties, col = Income)) +\n       geom_point() +\n       scale_colour_gradient2(low = \"darkred\", mid = \"white\",  high = \"darkgreen\", \n                              midpoint = 30892)\n\n\n\n\n\n\n15.4.8.2 Discrete color schemes\nIn the following chunk, we assign colors manually to each level in the variable Yield. The order of the color names mirror the order of the variable levels.\n\nggplot(dat1l, aes(Year, Yield, col = Crop)) + \n       geom_line() +\n       scale_colour_manual(values = c(\"red\", \"orange\", \"green\", \"blue\", \"yellow\"))\n\n\n\n\nThe following chunk applies a predefined discrete color scheme using one of Brewer’s preset qualitative colors, Dark2, to each level.\n\nggplot(dat1l, aes(Year, Yield, col = Crop)) + \n              geom_line() +\n              scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\nYou can also apply sequential or divergent Brewer color schemes to variables having an implied order.\nLet’s assume that there is an implied order to the crop types. For example, we’ll reorder the crop types based on their median yield (this creates an ordered factor from the Crop variable). We can then use one of Brewer’s sequential color schemes such as Reds.\n\nggplot(dat1l, aes(Year, Yield, col = reorder(Crop, Yield, median))) + \n              geom_line() +\n              guides(colour = guide_legend(title = \"Crops\")) +  \n              scale_colour_brewer(palette = \"Reds\") \n\n\n\n\nNote that we’ve added a guides() function to rename the legend title. This is not needed to generate the sequential colors.\nTo reverse the color scheme, set the direction argument to -1.\n\nggplot(dat1l, aes(Year, Yield, col = reorder(Crop, Yield, median))) + \n              geom_line() +\n              guides(colour = guide_legend(title = \"Crops\")) +  \n              scale_colour_brewer(palette = \"Reds\", direction = -1) \n\n\n\n\nYou can view a list of predifined Brewer color schemes using RColorBrewer::display.brewer.all().\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\n15.4.9 Adding mathematical symbols to a plot\nYou can embed math symbols using plotmath’s mathematical expressions by wrapping these expressions in an expression() function. For example,\n\nggplot(dat2, aes(x = B20004013^0.333, y = sqrt(B20004007))) + geom_point(alpha = 0.3) +\n       xlab( expression( (\"Female income\") ^ frac(1,3) ) ) + \n       ylab( expression( sqrt(\"Male income\") ) )\n\n\n\n\nTo view the full list of mathematical expressions, type ?plotmath at a command prompt."
  },
  {
    "objectID": "ggplot2.html#faceting",
    "href": "ggplot2.html#faceting",
    "title": "15  ggplot2 plotting environment",
    "section": "15.5 Faceting",
    "text": "15.5 Faceting\n  \n\n15.5.1 Faceting by categorical variable\n  \n\n\n15.5.2 facet_wrap\nFaceting (or conditioning on a variable) can be implemented in ggplot2 using the facet_wrap() function.\n\nggplot(dat1l2, aes(x = Year, y = Yield, color = Crop)) + geom_line() + \n       facet_wrap( ~ Country, nrow = 1)\n\n\n\n\nThe parameter ~ Country tells ggplot to condition the plots on country. If we wanted the plots to be stacked, we would set nrow to 2.\nWe can also condition the plots on two variables such as crop and Country. In essence, this stacks the categories. (Note that we will also rotate the x-axis labels to prevent overlaps).\n\nggplot(dat1l2, aes(x = Year, y=Yield)) + geom_line() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    facet_wrap(Crop ~ Country, nrow = 1)\n\n\n\n\n\n15.5.2.1 Wrapping facet headers\nYou can wrap the facet headers using the label_wrap_gen() function as an argument value to labeler. For example, to wrap the United States of America value, we’ll specify the maximum number of characters per line using the width argument:\n\nggplot(dat1l2, aes(x = Year, y=Yield)) + geom_line() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n  facet_wrap(Crop ~ Country, nrow = 1, labeller = label_wrap_gen(width = 12))\n\n\n\n\n\n\n15.5.2.2 facet_grid\nThe above facet_wrap example generated unique combinations of the variables Crop and Country. But such plots are usually best represented in a grid structure where one variable is spread along one axis and the other variable is spread along another axis of the plot layout. This can be accomplished using the facet_grid function:\n\nggplot(dat1l2, aes(x = Year, y = Yield)) + geom_line() + \n       facet_grid( Crop ~ Country)\n\n\n\n\n\n\n\n15.5.3 Faceting by continuous variable\nIn the above examples, we are faceting the plots based on a categorical variable: Country and/or crop. But what if we want to facet the plots based on a continuous variable? For example, we might be interested in comparing male and female incomes across different female income ranges. This requires that a new categorical field (a factor) be created assigning to each case (row) an income group. We can use the cut() function to accomplish this task (we’ll also omit all values greater than 100,000):\n\ndat2c$incrng <- cut(dat2c$F , breaks = c(0, 25000, 50000, 75000, 100000) )\nhead(dat2c)\n\n  State                 County Level Region   All     F     M          incrng\n1    ak Aleutians East Borough   All   West 21953 20164 22940     (0,2.5e+04]\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885     (0,2.5e+04]\n3    ak Aleutians East Borough    HS   West 20770 19671 21192     (0,2.5e+04]\n4    ak Aleutians East Borough    AD   West 26383 26750 26352 (2.5e+04,5e+04]\n5    ak Aleutians East Borough    BD   West 22431 19592 27875     (0,2.5e+04]\n6    ak Aleutians East Borough  Grad   West 74000 74000 71250 (5e+04,7.5e+04]\n\n\nIn this chunk of code, we create a new variable, incrng, which is assigned an income category group depending on which range dat2c$F (female income) falls into. The income interval breaks are defined in breaks=. In the output, you will note that the factor incrng defines a range of incomes (e.g. (0 , 2.5e+04]) where the parenthesis ( indicates that the left-most value is exclusive and the bracket ] indicates that the right-most value is inclusive.\nHowever, because we did not create categories that covered all income values in dat2c$F we ended up with a few NA’s in the incrng column:\n\nsummary(dat2c$incrng)\n\n    (0,2.5e+04] (2.5e+04,5e+04] (5e+04,7.5e+04] (7.5e+04,1e+05]            NA's \n           9419            7520            1425              33               5 \n\n\nWe will remove all rows associated with missing incrng values:\n\ndat2c <- na.omit(dat2c)\nsummary(dat2c$incrng)\n\n    (0,2.5e+04] (2.5e+04,5e+04] (5e+04,7.5e+04] (7.5e+04,1e+05] \n           9419            7520            1425              33 \n\n\nWe can list all unique levels in our newly created factor using the levels() function.\n\nlevels(dat2c$incrng) \n\n[1] \"(0,2.5e+04]\"     \"(2.5e+04,5e+04]\" \"(5e+04,7.5e+04]\" \"(7.5e+04,1e+05]\"\n\n\nThe intervals are not meaningful displayed as is (particularly when scientific notation is adopted). So we will assign more meaningful names to our factor levels as follows:\n\nlevels(dat2c$incrng) <-  c(\"Under 25k\", \"25k-50k\", \"50k-75k\", \"75k-100k\")\nhead(dat2c)\n\n  State                 County Level Region   All     F     M    incrng\n1    ak Aleutians East Borough   All   West 21953 20164 22940 Under 25k\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885 Under 25k\n3    ak Aleutians East Borough    HS   West 20770 19671 21192 Under 25k\n4    ak Aleutians East Borough    AD   West 26383 26750 26352   25k-50k\n5    ak Aleutians East Borough    BD   West 22431 19592 27875 Under 25k\n6    ak Aleutians East Borough  Grad   West 74000 74000 71250   50k-75k\n\n\nNote that the order in which the names are passed must match that of the original breaks.\nNow we can facet male vs. female scatter plots by income ranges. We will also throw in a best fit line to the plots.\n\nggplot(dat2c, aes(x = F, y = M)) + geom_point(alpha=0.2, pch=20) +\n       geom_smooth(method = \"lm\", col = \"red\") +\n       facet_grid( . ~ incrng) \n\n\n\n\nOne reason we would want to explore our data across different ranges of value is to assess the consistency in relationship between variables. In our example, this plot helps assess whether the relationship between male and female income is consistent across income groups."
  },
  {
    "objectID": "ggplot2.html#adding-45-slope-using-geom_abline",
    "href": "ggplot2.html#adding-45-slope-using-geom_abline",
    "title": "15  ggplot2 plotting environment",
    "section": "15.6 Adding 45° slope using geom_abline",
    "text": "15.6 Adding 45° slope using geom_abline\nWe will add a 45° line using geom_abline (where the intercept will be set to 0 and the slope to 1) to help visualize the discrepancy between the batches of values. So if a point lies above the 45° line, then male’s income is greater, if the point lies below the line , then female’s income is greater.\nTo help highlight differences in income, we will make a few changes to the faceted plots. First, we will reduce the y-axis range to $0-$150k (this will remove a few points from the data); we will force the x-axis and y-axis units to match so that a unit of $50k on the x-axis has the same length as that on the y-axis. We will also reduce the number of x tics and assign shorthand notation to income values (such as “50k” instead of “50000”). All this can be accomplished by adding the scale_x_continuous() function to the stack of ggplot elements.\n\nggplot(dat2c,  aes(x = F, y = M)) + geom_point(alpha = 0.2, pch = 20, cex = 0.8) + \n      ylim(0, 150000) +\n      geom_smooth(method = \"lm\", col = \"red\") + \n      facet_grid( . ~ incrng) + \n      coord_equal(ratio = 1) + \n      geom_abline(intercept = 0, slope = 1, col = \"grey50\") +\n      scale_x_continuous(breaks = c(50000, 100000), labels = c(\"50k\", \"100k\"))\n\n\n\n\nNote the change in regression slope for the last facet. The geom_smooth operation is only applied to the data limited to the axis range defined by ylim.\nNow let’s look at the same data but this time conditioned on educational attainment.\n\n# Plot M vs F by educational attainment except for Level == All\nggplot(dat2c, aes(x = F, y = M)) + geom_point(alpha = 0.2, pch = 20, cex = 0.8) +\n       ylim(0, 150000) + \n       geom_smooth(method = \"lm\", col = \"red\") +\n       facet_grid( . ~ Level) +  \n       coord_equal(ratio = 1) + \n       geom_abline(intercept = 0, slope = 1, col = \"grey50\") +\n       scale_x_continuous(breaks = c(50000, 100000), labels  =c(\"50k\", \"100k\"))\n\n\n\n\nWe can also condition the plots on two variables: educational attainment and region.\n\nggplot(dat2c, aes(x = F, y = M)) + geom_point(alpha = 0.2, pch = 20, cex = 0.8) +\n       ylim(0, 150000) + \n       geom_smooth(method = \"lm\", col = \"red\") + \n       facet_grid( Region ~ Level) +  \n       coord_equal(ratio = 1) +\n       geom_abline(intercept = 0, slope = 1, col = \"grey50\") +\n       scale_x_continuous(breaks = c(50000, 100000), labels = c(\"50k\", \"100k\"))"
  },
  {
    "objectID": "ggplot2.html#tile-plots-heat-maps",
    "href": "ggplot2.html#tile-plots-heat-maps",
    "title": "15  ggplot2 plotting environment",
    "section": "15.7 Tile plots (heat maps)",
    "text": "15.7 Tile plots (heat maps)\nYou can create so-called heat maps by tiling the data. This typically requires the use of three variables–two of which are either categorical or have equally spaced continuous values that define a rectangular grid layout, and the third that defines the grid cells’ color. For example, a tile plot can be created showing the median income (for all sexes) as a function of education level and region.\n\ndat2c.med <- dat2c %>% \n  filter(Level != \"All\") %>% \n  group_by(Level, Region) %>% \n  summarise(Income = median(All))\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_gradient(low = \"yellow\", high = \"red\")\n\n\n\n\nThe above example adopts a continuous color scheme. If you want to bin the color swatches using user defined breaks, swap the scale_fill_gradient function with the scale_fill_binned function.\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_binned(low = \"yellow\", high = \"red\",\n                    breaks = c(21000, 28000, 32000, 42000, 53000))\n\n\n\n\nAs of ggplot2 version 3.3, you can use the guide_coloursteps function to control the look of your legend. In the last figure, the breaks are not even, yet the legend splits the color swatches into equal length units. Setting the even.steps argument to FALSE scales the color swatches to match the true interval lengths.\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_binned(low = \"yellow\", high = \"red\",\n                    breaks = c(21000, 28000, 32000, 42000, 53000),\n                    guide = guide_coloursteps(even.steps = FALSE))\n\n\n\n\nThe scale_fill_binned function offers additional control over the legend such as its height (barheight), width (barwidth) and the display of the minimum and maximum values (show.limits).\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_binned(low = \"yellow\", high = \"red\",\n                    breaks = c(21000, 28000, 32000, 42000, 53000),\n                    guide = guide_coloursteps(even.steps = FALSE,\n                                              barheight = unit(2.3, \"in\"),\n                                              barwidth = unit(0.1, \"in\"),\n                                              show.limits = TRUE))"
  },
  {
    "objectID": "ggplot2.html#exporting-to-an-image",
    "href": "ggplot2.html#exporting-to-an-image",
    "title": "15  ggplot2 plotting environment",
    "section": "15.8 Exporting to an image",
    "text": "15.8 Exporting to an image\nYou can export a ggplot figure to an image using the ggsave function. For example,\n\np1 <- ggplot(dat1l2, aes(x = Year, y = Yield, color = Crop)) + geom_line() + \n  facet_wrap( ~ Country, nrow = 1) +\n  scale_y_continuous(labels = scales::comma_format())\n\nggsave(\"fig0.png\", plot = p1, width = 6, height = 2, units = \"in\", device = \"png\")\n\n\nThe width and height arguments are defined in units of inches, in. You can also specify these parameters in units of centimeters by setting units = \"cm\". The device argument controls the image file type. Other file types include \"jpeg\", \"tiff\", \"bmp\" and \"svg\" just to name a few.\nFor greater control of the font sizes, you need to make use of the theme function when buiding the plot.\n\np1 <- ggplot(dat1l2, aes(x = Year, y = Yield, color = Crop)) + geom_line() + \n  facet_wrap( ~ Country, nrow = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme(axis.text    = element_text(size = 8, family = \"mono\"),\n        axis.title   = element_text(size = 11, face = \"bold\"),\n        strip.text   = element_text(size = 11, face=\"italic\", family = \"serif\"),\n        legend.title = element_text(size = 10, family = \"sans\"),\n        legend.text  = element_text(size = 8,  color = \"grey40\"))\n\nggsave(\"fig1.png\", plot = p1, width = 6, height = 2, units = \"in\")\n\n\nThe family argument controls the font type. It does not automatically access all the fonts in your operating system. The three R fonts accessible by default are \"serif\", \"sans\" and \"mono\". These are usually mapped to your system’s fonts.\nTo access other fonts on your operating system, you will need to make use of the showtext package. The package is not covered in this tutorial, instead, refer to the package’s website for instructions on using the package."
  },
  {
    "objectID": "univariate_plots.html#introduction",
    "href": "univariate_plots.html#introduction",
    "title": "16  Visualizing univariate distributions",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nLet’s create two batches of numbers, a and b:\n\na  <- rep(12, 10)\nb  <- rep(15, 10)\ndf <- data.frame(a, b)\n\nHow do the two batches differ?\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n12\n15\n\n\n\n\nIf the difference wasn’t obvious from the table view we can create a jittered point plot from the data.\n\n# To plot across categories, we need the data in a long (tidy) form\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndfl <- pivot_longer(df, names_to = \"Category\", values_to = \"Value\", 1:2)\nggplot(dfl,  aes(y = Value, x = Category, col = Category)) + \n             geom_jitter(position = position_jitter(height = 0, width=0.4))\n\n\n\n\nIt’s clear that both batches differ by their uniform values, batch a is made up of the identical numbers, 12, and batch b is made up of a different set of identical values, 15. Note that because so many values overlapped, we made use of the geom_jitter() function which randomly jitters the data about their actual location.\nNow let’s compare a more complicated batch of numbers.\n\n# Randomly pick 10 values from a uniform distribution\nset.seed(23)\na   <- round(runif(10, 5, 15))\nb   <- round(runif(10, 10, 20))\n\n# Create wide and long data frames\ndf  <- data.frame(a, b)\ndfl <- pivot_longer(df, names_to = \"Category\", values_to = \"Value\", 1:2)\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n11\n19\n\n\n7\n17\n\n\n8\n14\n\n\n12\n13\n\n\n13\n18\n\n\n9\n11\n\n\n15\n15\n\n\n15\n16\n\n\n13\n19\n\n\n15\n16\n\n\n\n\n\nggplot(dfl, aes(y = Value, x = Category, col = Category)) + \n            geom_jitter(position = position_jitter(height = 0, width = 0.2))\n\n\n\n\nSo how do these batches differ? They seem to differ by their center value. For example, each batch’s mean is:\n\nlibrary(dplyr)\ndfl %>% group_by(Category) %>% summarize(mean = mean(Value) )\n\n# A tibble: 2 × 2\n  Category  mean\n  <chr>    <dbl>\n1 a         11.8\n2 b         15.8\n\n\nThe center value (aka location), is one summary statistic we can use to compare batches. Another property of a batch that we might also want to compare is its distribution (aka spread). For example, does the spread between the two batches differ as well? It’s difficult to tell from the above plot given that the batches are offset, so we’ll level the batches by subtracting the means from their respective batches.\n\n# Subtract the batch mean from each batch value\ndfl2 <- dfl %>% \n  group_by(Category) %>%\n  mutate(Spread = Value - mean(Value))\n\n# Now plot the leveled batches\nggplot(dfl2, aes(y = Spread, x = Category, col = Category)) + \n             geom_jitter(position = position_jitter(height = 0, width=0.2))\n\n\n\n\nRemoving the location (or mean in our example) from each value facilitates our comparison of both spreads. From our working example we can, at best, say that the batches share the same range. But a spread can be characterized in many more ways than by its range. Next, we’ll focus on four exploratory tools that will help us explore and quantify a dataset’s spread. These are the histogram, the boxplot, the density plot and the quantile plot."
  },
  {
    "objectID": "univariate_plots.html#histograms",
    "href": "univariate_plots.html#histograms",
    "title": "16  Visualizing univariate distributions",
    "section": "16.2 Histograms",
    "text": "16.2 Histograms\nA histogram bins the values (usually in equal sized bins) and plots the frequency in which each bin is filled. For example, to create a histogram of batch a where each bin size covers one unit, we type:\n\nggplot(df, aes(x = a)) + geom_histogram(breaks = seq(6.5,16.5,by = 1), colour = \"white\")\n\n\n\n\nHere, we are explicitly defining the bin width as 1 unit and the range as 6.5 to 16.5 via the parameter breaks = seq(6.5,16.5,by=1). The colour parameter specifies the outline color. To change the fill color use the fill parameter instead. In our example, we have one value that falls in the first bin (bin ranging from 6.5 to 7.5), another value that falls in the second bin (bin value ranging from 7.5 to 8.5) and so on up to the second to last bin which has 3 values falling in it (bin covering the range 14.5 to 15.5). No values fall in the 15.5 to 16.5 bin range.\nWe can modify the width of each bin. For example, to have each bin cover two units instead of one, type:\n\nggplot(df, aes(x = a)) + geom_histogram(breaks = seq(6.5,16.5,by = 2), colour = \"white\") \n\n\n\n\nYou’ll note that changing bin widths can alter the look of the histogram, this is particularly true when plotting large batches of values.\nYou can also opt to have the function determine the bin ranges by simply specifying the number of bins using the bin = parameter:\n\nggplot(df, aes(x = a)) + geom_histogram(bins = 12, colour = \"white\")"
  },
  {
    "objectID": "univariate_plots.html#density-plots",
    "href": "univariate_plots.html#density-plots",
    "title": "16  Visualizing univariate distributions",
    "section": "16.3 Density plots",
    "text": "16.3 Density plots\nThe histogram is not only sensitive to bin sizes, but it also suffers from discontinuities in its bins. In the following example, two histograms are generated using the same bin sizes and counts but with different starting x values. The orange marks along the x-axis show the location of the values a. The second histogram suggests a slightly bimodal (two peak) distribution while the one on the left suggests a unimodal distribution.\n\n\n\n\n\nThe histogram can be represented as densities as opposed to counts. This is computed as the number of observations per bin divided by the product between the bin width and the total number of observations. For example, the first bin has one observation hence the density of observations within this bin interval is 1 / (2 * 10) or 0.05. The following histogram shows density values instead of counts on its y-axis.\n\nggplot(df, aes(x = a)) + \n  geom_histogram(breaks = seq(5.8, 15.8, by = 2), colour = \"white\", aes(y = ..density..)) + \n  geom_rug(col=\"darkorange\", cex=2, alpha = 0.5)\n\n\n\n\nThe bins map the density of observations along the x-axis. However, as was noted earlier, the discontinuity between bin intervals suggests that the density is constant along the bin width (e.g. the density of observations remains constant between a = 6 and a = 8) which may not necessarily be the case.\nOne solution is to compute density values on overlapping bins. Let’s take the first bin and have it count the number of values between 5 and 9 (exclusive) and divide that number by the total number of values and the bin width–this gives us two observations falling in the bin thus a density value of 2 / (10 * 4) = 0.05. The following plot shows the bin with the orange dot representing the bin’s midpoint.\n\n\n\n\n\nNext, we shift the bin over by one x unit then calculate the density of observations in the same way it was computed for the first bin. The density value is plotted as an orange dot. Note how the bin overlaps with the previous bin.\n\n\n\n\n\nThe same process is repeated for the third bin.\n\n\n\n\n\nThe process is repeated for each bin until the last bin is reached. Note that some of the a values are duplicates, hence the greater density values for the upper-end range.\n\n\n\n\n\nIf we remove the bins and connect the dots, we end up with a density trace.\n\n\n\n\n\nA property associated with the density trace is that the area under the curve sums to one since each density value represents the local density at x.\nThe above density trace applies an equal weight to each point within the bin intervals thus adding some raggedness to the plot. To smooth out the plot, we can apply different weights to each point inside of the bin’s intervals such that points closest to the bin’s midpoint are assigned greater weight than the ones furthest from the midpoint. A Gaussian function can be used to generate the weights. The following figure depicts the difference in weights assigned to any point falling within the first bin whose range covers the interval 4 to 8 centered on 6.\n\n\n\n\n\nWith the rectangular weight, all points within the bin width are assigned equal weight. With the Gaussian weight, points closest to the bin center are assigned greater weight than those furthest from the center.\nYou can generate a density trace of the data using the geom_density function.\n\nggplot(df, aes(x = a)) + geom_density(fill = \"grey60\")\n\n\n\n\nThe function adopts the gaussian weight function and will automatically define the bandwidth (analogous in concept to the bin width). If you want more detail in the density plot, simply reduce the bandwidth argument bw. The bandwidth defines the standard deviation of the gaussian function.\n\nggplot(df, aes(x = a)) + geom_density(fill = \"grey60\", bw = 1)\n\n\n\n\nYou can also add the observations along the x-axis to add to the density effect using the geom_rug function.\n\nggplot(df, aes(x = a)) + geom_density(fill = \"grey60\", col = NA, bw = 1) +\n  geom_rug(col = \"darkred\", size = 2, alpha = 0.4)"
  },
  {
    "objectID": "univariate_plots.html#boxplots",
    "href": "univariate_plots.html#boxplots",
    "title": "16  Visualizing univariate distributions",
    "section": "16.4 Boxplots",
    "text": "16.4 Boxplots\nA boxplot is another popular plot used to explore distributions. In ggplot2 we use the geom_boxplot() function as in,\n\nggplot(df, aes(x = 1, y = a)) + geom_boxplot() + \n           xlab(NULL) + theme(axis.text.y = element_blank()) +\n           coord_flip()\n\n\n\n\nNote that the coord_flip() function flips the boxplot on its side (the boxplot is normally plotted upright). The geom_boxplot function requires that both x and y axes be mapped a variable; usually, the x-axis is assigned a category, but since we are looking at a unique batch, we artificially map a value of 1 to the x-axis (we could have used any value). This requires that we mask the x-axis’s label and text so as not to confuse the reader using the options xlab(NULL) + theme(axis.text.y=element_blank()).\nThe following figure describes the anatomy of a boxplot.\n\nThe boxplot provides us with many meaningful pieces of information. For example, it gives us a center value: the median. It also tells us where the middle 50% of the values lie along the full range of values (in our example, approximately 50% of the values lie between 9.5 and 14.5). This range is referred to as the interquartile range (or IQR for short). Note that this is only an approximation given that some datasets may not lend themselves well to defining exactly 50% of their central values. For example, our batch only has four data points falling within the interquartile range because of tied values.\nThe long narrow lines extending beyond the interquartile range are referred to as the adjacent values or as whiskers. They represent either 1.5 times the width between the median and the nearest interquartile value or the most extreme value, whichever comes first.\nSometimes, you will encounter values that fall outside of the lower and/or upper adjacent values; such values are often referred to as outliers.\n\n16.4.1 Not all boxplots are created equal!\nNot all boxplots are created equal. There are many different ways in which quantiles can be defined. For example, some will compute a quantile as \\(( i - 0.5) / n\\) where \\(i\\) is the nth element of the batch of data and \\(n\\) is the total number of elements in that batch. This is the method implemented by Bill Cleveland and we will refer to this method as Cleveland’s quantile method. This also happens to be the method implemented by the base’s boxplot function which explains the different boxplot output compared to ggplot_boxplot in our working example:\n\nboxplot(a, horizontal = TRUE)\n\n\n\n\nThe upper and lower quartiles differ from those of ggplot since the three 15 values (these are the maximum values in batch a) end up falling inside the interquartile range following the aforementioned quantile definition. This eliminates any upper whiskers. In most cases, however, the difference will not matter as long as you adopt the same boxplot procedure when comparing batches.\n\n\n16.4.2 Implementing different quantile types in geom_boxplot\nIf you wish to implement different \\(f\\)-value calculations offered by the quantile function, you will need to create a custom function as follows:\n\n# Function to extract quantiles given an f-value type\nqtl.bxp <- function(x, type = 5) {\n  qtl <- quantile(x, type = type)\n   df <- data.frame(ymin  = qtl[1], ymax = qtl[5], \n                    upper = qtl[4], lower = qtl[2], middle = qtl[3])\n}\n\n# Plot the boxplot\nggplot(df, aes(x = \"\", y = a)) + \n  stat_summary(fun.data = qtl.bxp, fun.args = list(type = 5),\n               geom = 'boxplot') +\n  xlab(NULL) + theme(axis.text.y = element_blank()) +\n  coord_flip()\n\n\n\n\nNote the use of stat_summary instead of geom_boxplot. The type argument is the quantile type implemented in the quantile function."
  },
  {
    "objectID": "univariate_plots.html#quantile-plots",
    "href": "univariate_plots.html#quantile-plots",
    "title": "16  Visualizing univariate distributions",
    "section": "16.5 Quantile plots",
    "text": "16.5 Quantile plots\nA quantile plot generates a point plot that joins the quantile to each value in a batch. The boxplot is a special case of the \\(f\\)-quantile function in that it only returns the 1st, 2nd (median) and 3rd quartiles. The \\(f\\)-quantile returns the \\(full\\) range of quantile values. The quantile is directly related to the concept of a percentile: it identifies the fraction of the batch of numbers that is less than a value of interest. The following figure describes the anatomy of a quantile plot.\n\nThe x-axis shows the \\(f\\)-values: the full range of fractions. The y-axis is the \\(f\\)-quantile, \\(q(f)\\), which shows the sorted batch values (from smallest to largest). The points in the plot link the values on the y-axis to the \\(f\\)-values on the x-axis. For example, the \\(f\\)-value of 0.25 (~the 25th percentile) is associated with the \\(q(f)\\) value of 9 meaning that 25% of the values in the dataset have values of 9 or less. Likewise, the \\(f\\)-value of 0.5 (the median) is associated with a \\(q(f)\\) value of 12.5 implying that half of the dataset’s values are 12.5 or less. The boxplot is shown alongside the quantile plot to highlight the analogy.\n\n16.5.1 Computing the \\(f\\)-quantile\nComputing \\(f\\) requires that the batch of numbers be ordered from smallest to largest.\n\na.o <- sort(a)\na.o\n\n [1]  7  8  9 11 12 13 13 15 15 15\n\n\nThe concept of sorting values may seem benign, but it is fundamental to many EDA procedures that require robust techniques.\nWith the numbers sorted, we can proceed with the computation of \\(f\\) following Cleveland’s method:\n\\[\nf_i = \\frac{i - 0.5}{n}\n\\]\nwhere \\(i\\) is the nth element of the batch of data and \\(n\\) is the total number of elements in that batch. As noted in the Boxplots section, there are many ways one can compute a quantile, however, the differences may not matter much.\nFor each value in a, the \\(f\\) value is thus:\n\ni     <- 1 : length(a)\nf.val <- (i - 0.5) / length(a)  # Compute the f-value\na.fi  <- data.frame(a.o, f.val)\n\nNote that in the last line of code, we are appending the ordered representation of a to f.val given that f.val assumes an ordered dataset. The data frame a.fi should look like this:\n\n\n\n\n\n\n\n\na.o\nf.val\n\n\n\n\n7\n0.05\n\n\n8\n0.15\n\n\n9\n0.25\n\n\n11\n0.35\n\n\n12\n0.45\n\n\n13\n0.55\n\n\n13\n0.65\n\n\n15\n0.75\n\n\n15\n0.85\n\n\n15\n0.95\n\n\n\n\nIt may be desirable at times to find a value associated with a quantile that might not necessarily match an exact value in our batch. For example, there is no value in a associated with a quantile of \\(0.5\\); this is because we have an even number of values in our dataset. The solution is to interpolate (or extrapolate) a value based on a desired quantile. The quantile() function does just that. For example, to find the value associated with a quantile of \\(0.5\\):\n\nquantile(a, 0.5)\n\n 50% \n12.5 \n\n\nIf we want to get quantile values for a range of fractions, simply wrap the values with the c() function:\n\nquantile(a, c(0.25, 0.5, 0.75))\n\n 25%  50%  75% \n 9.5 12.5 14.5 \n\n\nThe quantile function is designed to accept different quantile algorithms. To see the list of algorithm options, type ?quantile at a command prompt. By default, R adopts algorithm type = 7. To adopt Cleveland’s algorithm, set type = 5. E.g.:\n\nquantile(a, c(0.25, 0.5, 0.75), type = 5)\n\n 25%  50%  75% \n 9.0 12.5 15.0 \n\n\nNote the difference in the upper quartile value.\n\n\n16.5.2 Creating a quantile plot\nA batch’s quantile is best viewed as a plot where we plot the batch values as a function of the \\(f\\)-values:\n\nggplot(a.fi, aes(x = f.val, y = a.o)) + geom_point() + xlab(\"f-value\")\n\n\n\n\n\n16.5.2.1 Using ggplot’s qq geom\nIf you did not want to go through the trouble of computing the \\(f\\)-values and the dataframe a.fi, you could simply call the function stat_qq() as in:\n\nggplot(df, aes(sample = a)) + stat_qq(distribution = qunif) + xlab(\"f-value\")\n\n\n\n\nHowever, ggplot’s stat_qq function does not adopt Cleveland’s \\(f\\)-value calculation. Hence, you’ll notice a slight offset in position along the x-axis. For example, the third-to-last point has an \\(f\\)-value of 0.744 instead of an \\(f\\)-value of 0.75 as calculated using Cleveland’s method.\nAlso note the slight change in mapping values to the aes() function: sample = a where a is the (unsorted) variable being plotted."
  },
  {
    "objectID": "univariate_plots.html#how-quantile-plots-behave-in-the-face-of-skewed-data",
    "href": "univariate_plots.html#how-quantile-plots-behave-in-the-face-of-skewed-data",
    "title": "16  Visualizing univariate distributions",
    "section": "16.6 How quantile plots behave in the face of skewed data",
    "text": "16.6 How quantile plots behave in the face of skewed data\nIt can be helpful to simulate distributions of difference skewness to see how a quantile plot may behave. In the following figure, the top row shows the different density distribution plots and the bottom row shows the quantile plots for each distribution (note that the x-axis maps the f-values)."
  },
  {
    "objectID": "compare_batches.html",
    "href": "compare_batches.html",
    "title": "17  Comparing data distributions",
    "section": "",
    "text": "Let’s compare singer heights between the Bass 2 group and Tenor 1 group. We will work off of a subset of Cleveland’s singer dataset which can be found in the lattice package.\nLet’s plot both datasets side-by-side. We will jitter the points about their category groups to expose overlapping points.\nLet’s get the count and median values for both batches.\nThe basest tend to be slightly taller than the Tenor–this is one way to compare the batches. But how do their distributions differ? We will explore this in the next section."
  },
  {
    "objectID": "compare_batches.html#boxplots",
    "href": "compare_batches.html#boxplots",
    "title": "17  Comparing data distributions",
    "section": "17.1 Boxplots",
    "text": "17.1 Boxplots\nWe can compare the batches using side-by-side boxplots.\n\nggplot(df2) + aes(x = voice.part, y = height) + geom_boxplot()\n\n\n\n\nThe differences in median values is obvious. What’s more, the difference in overall height values is more pronounced with the boxplot than it is with a simple point distributions plot shown earlier.\nAre the spreads between the batches comparable? It’s difficult to tell given that the batches are offset by about four inches. So let’s level the playing field by subtracting the median values from their respective batches.\n\ndf2.med <- df2 %>% \n          group_by(voice.part) %>%\n          mutate( NormHeight = height - median(height)) \n\nggplot(df2.med, aes(x=voice.part, y=NormHeight)) + geom_boxplot()\n\n\n\n\nLeveling the boxplots certainly helps in assessing the difference in spreads. It seems that the Tenor 1 group has a slightly wider range of heights than the Bass 2 group. About 50% of singers in the Tenor 1 group have heights that range five inches whereas the singers in the Bass 2 group have heights that range four inches. We also note that the Tenor 1 height distribution tends to be skewed towards taller singers whereas the Bass 2 height distribution tends to be a little less skewed toward smaller heights."
  },
  {
    "objectID": "compare_batches.html#the-quantile-quantile-q-q-plot",
    "href": "compare_batches.html#the-quantile-quantile-q-q-plot",
    "title": "17  Comparing data distributions",
    "section": "17.2 The quantile-quantile (Q-Q) plot",
    "text": "17.2 The quantile-quantile (Q-Q) plot\nA quantile-quantile plot (or Q-Q plot for short) combines two separate quantile plots from different batches of values by pairing the point values by their common \\(f\\)-value.\n\nFirst, we need to compute the \\(f\\)-values for both batches of singer data (Bass 2 and Tenor 1).\n\ndf.qq <- df2 %>% \n         group_by(voice.part) %>%\n         arrange(height) %>%\n         mutate( f.val = (row_number() - 0.5 ) / n())\n\nNow let’s generate quantile plots for both batches.\n\nggplot(df.qq, aes(x=f.val, y = height)) + geom_line(col=\"grey\") +\n              geom_point() + facet_grid( voice.part ~ .) \n\n\n\n\nWhile the geometric structure of a boxplot lends itself well to side-by-side comparison, the same cannot be said for side-by-side quantile plot comparison hence the need for an amalgamation of these two plots into a single plot called a quantile-quantile (q-q) plot. If the number of values in both batches are equal, then the plotting process is straightforward: sort both batches (from smallest value to largest value), then pair up the sorted values and plot one batch vs. the other batch (see schematic at the beginning of this section)\nIf the two batches differ in size (as is the case with our example where we have 21 tenors and 26 bassists), we won’t be able to match sorted values. For example, in the above graph, you’ll note that one of the singer’s height in the Bass 2 batch is associated with an \\(f\\)-value of 0.75, however, there are no singer heights associated with an \\(f\\)-value of 0.75 in the Tenor 1 batch.\nTo overcome the problem of batch size mismatch, we limit the number of points in the q-q plot to the number of values associated with the smallest sized batch (21 in our working example). This requires that we find matching singer height values to the set of \\(f\\)-values associated with a batch of 21 values. The following chunk of code computes the 21 \\(f\\)-values. We are rounding the output to two decimal places for presentation only.\n\n# Generate and index\ni  <- 1:21\n# Compute the f-values\nfi <- (i - 0.5) / 21\n# view rounded values\nround(fi,2)\n\n [1] 0.02 0.07 0.12 0.17 0.21 0.26 0.31 0.36 0.40 0.45 0.50 0.55 0.60 0.64 0.69 0.74 0.79 0.83 0.88 0.93 0.98\n\n\nTo find the matching singer height values in Bass2, we need to interpolate the original height values. In practice, a linear interpolation is sought unless theory justifies otherwise.\nWe will explore two methods: a manual approach to constructing a q-q plot and the qqplot() function approach. Note that both methods can produce slightly different q-q plots–this is to be expected since they may adopt different quantile algorithms.\nIn all cases, we will add a 45° line (i.e. the Bass 1 \\(=\\) Tenor 1 line) that shows where we would expect the points to be plotted if both distributions were identical.\n\n17.2.1 Manual construction of a q-q plot\nTo find a value in the larger dataset (i.e. Bass 2) that matches the 21 quantiles of the smaller dataset, Tenor 1, we can use the approx() function.\nFirst, we will extract all values associated with Bass 2 from the dataset.\n\ndf.bass  <- df.qq %>%  filter( voice.part == \"Bass 2\") \ndf.tenor <- df.qq %>%  filter( voice.part == \"Tenor 1\") \n\nNext, we will interpolate a new set of height values for Bass 2 that match the 21 quantile values in fi.\n\nb.hgt.interp <- approx(df.bass$f.val, df.bass$height, fi)\n\nThe output generates two columns: the 21 quantiles, fi, and the 21 Bass 2 interpolated height values. Note that we could have substituted fi with df.tenor$f.val. The approach shown here is used to reinforce the idea that we are interpolating height values based on a new set of f-values.\nFinally, we will combine the interpolated Bass 2 values (which is already in an ascending order) with sorted heights for the Tenor 1 singers (these were sorted in df.qq). Then, we will plot the paired values using ggplot. We will also add the 45° line to facilitate comparison.\n\ns.qq <- df.tenor %>% mutate(`Bass 2`  = b.hgt.interp$y) %>% \n                     rename(`Tenor 1` = height) \n\nggplot(s.qq, aes( x= `Tenor 1`, y = `Bass 2`)) + geom_point() + geom_abline( intercept=0, slope=1)\n\n\n\n\nNote that the range of values for both axes may not match; this may influence our visual assessment of the differences. To remedy this, we can force a fixed aspect ratio between both axes.\n\n# Get the range of both datasets\nxylim <- range( c(s.qq$\"Bass\", s.qq$`Tenor 1`) )\n\n# Add a 5% buffer to the xy limits\n# (This avoids having the points plotted on the edge of the plot)\nxylim <- xylim + c(-0.05 * diff(xylim), + 0.05 * diff(xylim))\n\nggplot(s.qq, aes( x= `Tenor 1`, y = `Bass 2`)) + \n             geom_point() + \n             geom_abline( intercept=0, slope=1) +\n             coord_fixed(ratio = 1, xlim=xylim, ylim = xylim)\n\n\n\n\n\n\n17.2.2 Using the base qqplot\nThis approach is quicker and requires fewer lines of code, but it does not afford the insight into the inner workings of a q-q plot as does the manual approach from the previous section.\nqqplot is a base package that will generate a q-q plot using the base plotting environment. It requires that the two batches be loaded as separate vector objects.\n\nTenor <- filter(df, voice.part == \"Tenor 1\") %>%  pull(height)\nBass  <- filter(df, voice.part == \"Bass 2\")  %>%  pull(height)\nqqplot(x=Tenor, y=Bass, asp=1)\nabline( c(0,1))\n\n\n\n\nqqplot offers the option to output the q-q plot values (including the interpolated values) as a list by setting the parameter plot.it=FALSE. We then need to convert this list object to a dataframe using as.data.frame. The output can then be used in a call to ggplot.\n\n# Create the quantile-quantile data table\nqq.out <- qqplot(x=Tenor, y=Bass, plot.it=FALSE)\nqq.out <- as.data.frame(qq.out)\n\n# Set the x and y limits\nxylim <- range( c(qq.out$x, qq.out$y) )\n\n# Generate the QQ plot\nggplot(qq.out, aes( x= x, y = y)) + \n               geom_point() + \n               geom_abline( intercept=0, slope=1) +\n               coord_fixed(ratio = 1, xlim=xylim, ylim = xylim) +\n               xlab(\"Tenor\") + ylab(\"Bass\")\n\n\n\n\nNote that qqplot generates a wide table where the Tenor height values and the interpolated Bass values are side-by-side (note that x and y were assigned when we called the qqplot function in the last code chunk).\n\nhead(qq.out)\n\n   x    y\n1 64 66.0\n2 64 67.0\n3 65 67.5\n4 66 68.0\n5 66 69.0\n6 66 70.0\n\n\n\nNote: The difference in output between our manual approach and qqplot’s approach is small. If you want to faithfully recreate qqplot’s output using the manual approach, compute the f-values as \\((i -1)/(n-1)\\)."
  },
  {
    "objectID": "compare_batches.html#q-q-plots-vs.-traditional-scatter-plots",
    "href": "compare_batches.html#q-q-plots-vs.-traditional-scatter-plots",
    "title": "17  Comparing data distributions",
    "section": "17.3 q-q plots vs. traditional scatter plots",
    "text": "17.3 q-q plots vs. traditional scatter plots\nIt’s important to note the difference between a quantile-quantile plot and a traditional scatter plot whereby the pairing of values between variables is explicitly defined (for example average male and female income values paired by county). The singer data does not assume any pairing of observations (e.g. the height measurement for a singer in Bass 2 is independent for that of a singer in Tenor 2). This is made more evident with our working example which has unequal number of singers in both groups being compared. The pairing of values in a q-q plot is constructed from the ordering of values in each batch and nothing more."
  },
  {
    "objectID": "compare_batches.html#what-can-we-learn-from-a-q-q-plot",
    "href": "compare_batches.html#what-can-we-learn-from-a-q-q-plot",
    "title": "17  Comparing data distributions",
    "section": "17.4 What can we learn from a q-q plot?",
    "text": "17.4 What can we learn from a q-q plot?\nA valuable by-product of an empirical q-q plot is the mathematical relationship between the batches of values. If the distributions are identical (i.e. all the points of a q-q plot fall on the 45° line) then we could characterize the relationship as batch1 = batch2. If the points follow a pattern mimicking a line parallel to the 45° line as in the following plot,\n\n\n\n\n\nthen we say that there is an additive shift between batch1 and batch2. The shift can usually be eyeballed from the plot. In this example, the shift is around 2 or batch2 =  batch1 + 2.\nWhen the points follow a line at an angle to the 45° line as in\n\n\n\n\n\nthen we say that there is a multiplicative shift between the batches. The multiplier can be a bit difficult to glean graphically so trial and error may be the best approach whereby we multiply one of the batches by a multiplier. For example, if we believe that batch2 is 1.5 times greater than batch1, we multiply batch1 by 1.5 (thus generating the relationship batch2 = batch1 * 1.5) then plot the result to see if the points line up.\n\n\n\n\n\nSometimes, you might encounter a relationship that is both additive and multiplicative in which case you should first resolve the multiplicative part of the pattern until the points are close to parallel to the 45° line, then resolve the additive portion. For example, the following plot\n\n\n\n\n\ncan be decomposed into its multiplicative component (multiply batch1 by ~ 1.5),\n\n\n\n\n\nthen by its additive component (add 3.5),\n\n\n\n\n\nto get the approximate relationship of batch2 = batch1 * 1.5 + 3.5.\n\n17.4.1 Is the relationship between tenor and bass additive or multiplicative?\nTurning back to our voice part dataset, an additive shift is apparent, but a multiplicative shift not as much. To check, we’ll add 2.5 to the Bass 2 value.\n\nggplot(s.qq, aes( x= `Tenor 1` + 2.5, y = `Bass 2`)) + \n             geom_point() + \n             geom_abline( intercept=0, slope=1) +\n             coord_fixed(ratio = 1, xlim=xylim, ylim = xylim) + xlab(\"Tenor 1 + 2.5\")\n\n\n\n\nThe bulk of the data appears to follow an additive shift except for one or two outliers at the upper end of the distribution. These outliers should not mislead us into assuming that a multiplicative offset is warranted here."
  },
  {
    "objectID": "compare_batches.html#the-tukey-mean-difference-plot",
    "href": "compare_batches.html#the-tukey-mean-difference-plot",
    "title": "17  Comparing data distributions",
    "section": "17.5 The Tukey mean-difference plot",
    "text": "17.5 The Tukey mean-difference plot\nOur eyes are better suited at judging deviations from a horizontal line than from a 45° line. All that is required is to subtract the y-value (Bass2) from the x-value (Tenor 2) then compare the difference to the mean of the two values:\n\\[\nY = Bass- Tenor\n\\] \\[\nX = \\frac{Bass + Tenor}{2}\n\\]\nThis forces the 45° line in the previous q-q plots to a 0° slope. The final plot is a Tukey mean-difference plot or m-d for short.\n\n17.5.1 Generating a Tukey m-d plot using ggplot\nWe continue with the qq.out dataset generated from the qqplot function.\n\nmd.y  <- (qq.out$y - qq.out$x) \nmd.x  <- (qq.out$y + qq.out$x) * 0.5\ndf.md <- data.frame(md.x, md.y)\n\nggplot(df.md,  aes(x = md.x, y = md.y)) + geom_point() + geom_abline( slope=0 ) +\n               xlab(\"Mean height (in)\") + ylab(\"Difference in height (in)\")\n\n\n\n\nThe units on both axes are in inches. It’s clear from this plot that differences in heights are pretty much consistent across the quantiles with an additive shift of about 2.5 inches, except near the higher quantiles for a few lone points."
  },
  {
    "objectID": "compare_batches.html#pairwise-q-q-plots",
    "href": "compare_batches.html#pairwise-q-q-plots",
    "title": "17  Comparing data distributions",
    "section": "17.6 Pairwise q-q plots",
    "text": "17.6 Pairwise q-q plots\nA pairwise q-q plot allows one to view all combinations of batch pairs. This requires that we first find the smallest batch of height values, then interpolate all other batch values to match the smallest batch quantiles. Note that the pairwise plot is symmetrical about the diagonal. Here, we’ll make use of the lattice package to generate the pairwise q-q plots.\n\nlibrary(lattice)\n# Find smallest batch\nf.rng <- min(tapply(singer$height, singer$voice.part, length))\n\n# Define quantile function\nfun1  <- function(x,f.rng) {approx(1:length(x), sort(x), n = f.rng )$y}\n\n# Compute quantiles for all factors\nt1    <- aggregate(singer$height ~ singer$voice.part, FUN=fun1, f.rng=f.rng )\n\n# Flip table and name columns\nt2    <- as.data.frame(t(t1[,-1]))\nnames(t2) <- t1[,1]\n\n# Label columns of qmat using names of voice parts.\nsplom(~t2,  axis.text.cex=0.6, pch = 20,\n      varname.cex=0.8,\n      pscales=3,\n      xlab=\"Heights (inches)\",\n      prepanel.limits = function(x) (range(t2)), # force all axes the same\n      panel = function(x,y,...){\n        panel.grid()\n        panel.splom(x,y,...)\n        panel.abline(0,1,col=\"grey\")\n      })"
  },
  {
    "objectID": "theoretical_qq.html#introduction",
    "href": "theoretical_qq.html#introduction",
    "title": "18  The theoretical q-q",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\nThus far, we have used the quantile-quantile plots to compare the distributions between two empirical (i.e. observational) datasets. This is sometimes referred to as an empirical Q-Q plot. We can also use the q-q plot to compare an empirical distribution to a theoretical distribution (i.e. one defined mathematically). Such a plot is usually referred to as a theoretical Q-Q plot. Examples of popular theoretical distribution are the normal distribution (aka the Gaussian distribution), the chi-square distribution, and the exponential distribution just to name a few.\n\n\n\n\n\nThere are many reasons we might want to compare empirical data to theoretical distributions:\n\nA theoretical distribution is easy to parameterize. For example, if the shape of the distribution of a batch of numbers can be approximated by a normal distribution we can reduce the complexity of our data to just two values: the mean and the standard deviation.\nIf data can be approximated by certain theoretical distributions, then many mainstream statistical procedures can be applied to the data.\nIn inferential statistics, knowing that a sample was derived from a population whose distribution follows a theoretical distribution allows us to derive certain properties of the population from the sample. For example, if we know that a sample comes from a normally distributed population, we can define confidence intervals for the sample mean using a t-distribution.\nModeling the distribution of the observed data can provide insight into the underlying process that generated the data.\n\nBut very few empirical datasets follow any theoretical distributions exactly. So the questions usually ends up being “how well does theoretical distribution X fit my data?”\nThe theoretical quantile-quantile plot is a tool to explore how a batch of numbers deviates from a theoretical distribution and to visually assess whether the difference is significant for the purpose of the analysis. In the following examples, we will compare empirical data to the normal distribution using the normal quantile-quantile plot."
  },
  {
    "objectID": "theoretical_qq.html#the-normal-q-q-plot",
    "href": "theoretical_qq.html#the-normal-q-q-plot",
    "title": "18  The theoretical q-q",
    "section": "18.2 The normal q-q plot",
    "text": "18.2 The normal q-q plot\nThe normal q-q plot is just a special case of the empirical q-q plot we’ve explored so far; the difference being that we assign the normal distribution quantiles to the x-axis.\n\n18.2.1 Drawing a normal q-q plot from scratch\nIn the following example, we’ll compare the Alto 1 group to a normal distribution. First, we’ll extract the Alto 1 height values and save them as an atomic vector object using dplyr’s piping operations. However, dplyr’s operations will return a dataframe–even if a single column is selected. To force the output to an atomic vector, we’ll pipe the subset to pull(height) which will extract the height column into a plain vector element.\n\nlibrary(dplyr)\n\ndf   <- lattice::singer\nalto <- df %>%  filter(voice.part == \"Alto 1\") %>% pull(height)\n\nNext, we need to sort alto in ascending order.\n\nalto <- sort(alto)\n\nNext, we need to find the matching normal distribution quantiles. We first find the \\(f\\)-values for alto, then use qnorm to find the matching normal distribution values from those same \\(f\\)-values\n\ni      <- 1:length(alto)\nfi     <- (i - 0.5) / length(alto)\nx.norm <- qnorm(fi)\n\nNow we can plot the sorted alto values against the normal values.\n\nplot( alto ~ x.norm, type = \"p\", xlab = \"Normal quantiles\", pch = 20)\n\n\n\n\nWhen comparing a batch of numbers to a theoretical distribution on a q-q plot, we are looking for significant deviation from a straight line. To make it easier to judge straightness, we can fit a line to the points. Note that we are not creating a 45° (x=y) slope as was done with the empirical q-q plot–the range of values between both sets of numbers do not match. Here, we are only seeking the straightness of the point pattern.\nThere are many ways one can fit a line to the data, Cleveland opts to fit a line to the first and third quartile of the q-q plot. The following chunk of code identifies the quantiles for both the alto dataset and the theoretical normal distribution. It then computes the slope and intercept from these coordinates.\n\n# Find 1st and 3rd quartile for the Alto 1 data\ny <- quantile(alto, c(0.25, 0.75), type = 5)\n\n# Find the 1st and 3rd quartile of the normal distribution\nx <- qnorm( c(0.25, 0.75))\n\n# Now we can compute the intercept and slope of the line that passes\n# through these points\nslope <- diff(y) / diff(x)\nint   <- y[1] - slope * x[1]\n\nNext, we add the line to the plot.\n\nabline(a = int, b = slope )\n\n\n\n\n\n\n18.2.2 Using R’s built-in functions\nR has two built-in functions that facilitate the plot building task when comparing a batch to a normal distribution: qqnorm and qqline. Note that the function qqline allows the user to define the quantile method via the qtype= parameter. Here, we set it to 5 to match the \\(f\\)-value calculation adopted in this course.\n\nqqnorm(alto)\nqqline(alto, qtype = 5)\n\n\n\n\nThat’s it. Just two lines of code!\n\n\n18.2.3 Using the ggplot2 plotting environment\n\n18.2.3.1 ggplot2 version 3.0 or greater\nAs of version 3.0, ggplot has the stat_qq_line function (or geom_qq_line) that will generate the interquartile fit. So to generate the theoretical q-q plot, use the stat_qq function (or geom_qq) to generate the point plot, then call stat_qq_line to generate the IQR fit.\n\nlibrary(ggplot2)\n\nggplot() + aes(sample = alto) + stat_qq(distribution = qnorm) + \n           stat_qq_line(line.p = c(0.25, 0.75), col = \"blue\") + ylab(\"Height\")\n\n\n\n\nWe can, of course, make use of ggplot’s faceting function to generate trellised plots. For example, the following plot replicates Cleveland’s figure 2.11 (except for the layout which we’ll setup as a single row of plots).\n\nggplot(df, aes(sample = height)) + stat_qq(distribution = qnorm) + \n           stat_qq_line(line.p = c(0.25, 0.75), col = \"blue\") + ylab(\"Height\") +\n           facet_wrap(~ voice.part, nrow = 1) + ylab(\"Height\")\n\n\n\n\nThe stat_qq_line function uses the built-in quantile function and as such will adopt the default quantile type 7 (i.e. it computes the f-value as \\((i - 1)/(n - 1))\\). This differs from Cleveland’s approach to computing the f-value. This setting cannot be changed in stat_qq_line.\n\n\n18.2.3.2 ggplot2 version older than 3.0\nWith an older version of ggplot2, you will need to construct the interquartile fit from scratch. We can still take advantage of the stat_qq() function to plot the points, but the equation for the line must be computed manually (as was done earlier). Those steps will be repeated here. Note that with this example, we are adopting Cleveland’s f-value algorithm of \\((i - 0.5)/n\\).\n\nlibrary(ggplot2)\n\n# Find the slope and intercept of the line that passes through the 1st and 3rd\n# quartile of the normal q-q plot\n\ny     <- quantile(alto, c(0.25, 0.75), type = 5) # Find the 1st and 3rd quartiles\nx     <- qnorm( c(0.25, 0.75))                 # Find the matching x-axis values\nslope <- diff(y) / diff(x)                     # Compute the line slope\nint   <- y[1] - slope * x[1]                   # Compute the line intercept\n\n# Generate normal q-q plot\n\nggplot() + aes(sample = alto) + stat_qq(distribution = qnorm) + \n           geom_abline(intercept = int, slope = slope, col = \"blue\") + \n           ylab(\"Height\")           \n\n\n\n\nTo generated a faceted normal qq plot you first need to compute the slopes for each singer group. We’ll use dplyr’s piping operations to create a new dataframe with singer group name, slope and intercept.\n\nlibrary(dplyr)\nintsl <- df %>% group_by(voice.part) %>% \n       summarize(q25    = quantile(height, 0.25, type = 5),\n                 q75    = quantile(height, 0.75, type = 5),\n                 norm25 = qnorm( 0.25),\n                 norm75 = qnorm( 0.75),\n                 slope  = (q25 - q75) / (norm25 - norm75),\n                 int    = q25 - slope * norm25) %>%\n       select(voice.part, slope, int) \n\nThe above chunk creates the following table.\n\n\n\n\n\n\n\n\n\n\nvoice.part\nslope\nint\n\n\n\n\nBass 2\n2.965\n72\n\n\nBass 1\n2.224\n70.5\n\n\nTenor 2\n1.483\n70\n\n\nTenor 1\n3.892\n68.62\n\n\nAlto 2\n2.224\n65.5\n\n\nAlto 1\n2.78\n64.88\n\n\nSoprano 2\n2.965\n64\n\n\nSoprano 1\n1.853\n63.75\n\n\n\n\n\nIt’s important that the voice.part names match those in df letter-for-letter so that when ggplot is called, it will know which facet to assign the slope and intercept values to via geom_abline.\n\nggplot(df, aes(sample = height)) + stat_qq(distribution = qnorm) + \n           geom_abline(data = intsl, aes(intercept = int, slope = slope), col = \"blue\") +\n           facet_wrap(~ voice.part, nrow = 1) + ylab(\"Height\")"
  },
  {
    "objectID": "theoretical_qq.html#how-normal-is-my-dataset",
    "href": "theoretical_qq.html#how-normal-is-my-dataset",
    "title": "18  The theoretical q-q",
    "section": "18.3 How normal is my dataset?",
    "text": "18.3 How normal is my dataset?\nSimulations are a great way to develop an intuitive feel for what a dataset pulled from a normal distribution might look like in a normal Q-Q plot. You will seldom come across perfectly normal data in the real world. Noise is an inherent part of any underlying process. As such, random noise can influence the shape of a q-q plot despite the data coming from a normal distribution. This is especially true with small datasets as demonstrated in the following example where we simulate five small batches of values pulled from a normal distribution. The rnorm function is used in this example to randomly pick a number from a normal distribution whose mean is 0 and whose standard deviation is 1. You can, of course, change the mean and standard deviation values to reflect the data being simulated.\nTo streamline the coding, we will make use of the replicate function to generate five sets of randomly generated normal values. The function will assign each randomly generated batch to its own column, thus creating a matrix. We then convert this matrix to a data frame, then pivot it to a long form for use in ggplot.\n\nlibrary(tidyr)\n\n# Create matrix with each row storing randomly generated\n# numbers from a Normal distribution\nset.seed(218)  # Sets random generator seed for consistant output\nsim <- replicate(5 , rnorm(20, mean = 0, sd = 1), simplify = TRUE)\n\n# Create long form version of the sim matrix\nsiml <- pivot_longer(as.data.frame(sim), names_to = \"sim\", \n                     values_to = \"val\", cols = everything())\n\n# Generate q-q plots of the simulated values\nggplot(siml, aes(sample = val)) + stat_qq(distribution = qnorm) +\n  stat_qq_line(line.p = c(0.25, 0.75), col = \"blue\") +\n  facet_wrap(~ sim, nrow = 1) + ylab(\"Simulated normals\")\n\n\n\n\nOf the 5 simulated batches, only V3 and V4 generate q-q plots that reflect a pattern we would expect from a normally distributed batch of values. The other three simulated batches generate plots that could lead one to question whether the data were pulled from a normal distribution, even though we know that they were!\nThis is why you should always interpret such plots with a healthy dose of skepticism, especially with small datasets."
  },
  {
    "objectID": "theoretical_qq.html#how-normal-q-q-plots-behave-in-the-face-of-skewed-data",
    "href": "theoretical_qq.html#how-normal-q-q-plots-behave-in-the-face-of-skewed-data",
    "title": "18  The theoretical q-q",
    "section": "18.4 How normal q-q plots behave in the face of skewed data",
    "text": "18.4 How normal q-q plots behave in the face of skewed data\nIt can be helpful to simulate distributions of difference skewness to see how a normal quantile plot may behave. In the following figure, the top row shows different density distribution plots; the bottom row shows the normal q-q plots for each distribution."
  },
  {
    "objectID": "rf.html",
    "href": "rf.html",
    "title": "19  Univariate analysis: Fits and residuals",
    "section": "",
    "text": "In previous lectures, we’ve determined that the voice.part singer groups differed only by location (central value) and not so much by spread. In this section, we will expand this analysis by fitting a model (the mean) to the data, then we’ll explore the residuals (i.e. the part of the data not explained by the fitted model). This exercise will tackle two objectives:"
  },
  {
    "objectID": "rf.html#fitting-the-data",
    "href": "rf.html#fitting-the-data",
    "title": "19  Univariate analysis: Fits and residuals",
    "section": "19.1 Fitting the data",
    "text": "19.1 Fitting the data\nUnivariate data can be characterized by their location and by their spread. The different groups of singers differ by their central values, we will therefore fit the group means to each group batch and compare the residuals between groups.\nFirst, we’ll load the libraries that will be used in this exercise, then we’ll load the singer data into the df object.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lattice)\n\ndf <- singer\n\nNext, we’ll plot the singer values using jittered points. We’ll also add an orange point to each batch which will represent each group’s mean.\n\nggplot(df, aes(y = height, x = voice.part)) + \n  geom_jitter(width = 0.1, height = 0, alpha = 0.1) +\n  stat_summary(fun = \"mean\", geom = \"point\", cex = 3, pch = 21, col = \"red\", bg = \"orange\") \n\n\n\n\nWe’ve fitted each group with the mean–a mathematical description of the batches. Note that we could have used other measures of location such as the median, but since the data seem to follow a symmetrical distribution, the mean remains an adequate choice."
  },
  {
    "objectID": "rf.html#computing-the-residuals",
    "href": "rf.html#computing-the-residuals",
    "title": "19  Univariate analysis: Fits and residuals",
    "section": "19.2 Computing the residuals",
    "text": "19.2 Computing the residuals\nNow, we’ll subtract the group means from their respective group values: this will give us the residuals for each batch.\n\n# Add residual values to the data\ndf2 <- df %>% \n  group_by(voice.part) %>%\n  mutate(Height.res = height - mean(height))\n\nNext, we will generate a plot of the (jittered) residuals.\n\n# Now plot the data after fitting height with group mean\nggplot(df2) + aes(y=Height.res, x=voice.part)             + \n  geom_jitter(width = 0.1, height=0, alpha=0.1) +\n  stat_summary(fun = \"mean\", geom = \"point\", cex = 3, pch = 21, col = \"red\", bg=\"orange\") \n\n\n\n\nWe’ve normalized the batches to a common location. Note that the values along the y-axis have changed: all values are now spread around 0. Next, we’ll check that the batches of residuals have similar spread."
  },
  {
    "objectID": "rf.html#comparing-the-residuals",
    "href": "rf.html#comparing-the-residuals",
    "title": "19  Univariate analysis: Fits and residuals",
    "section": "19.3 Comparing the residuals",
    "text": "19.3 Comparing the residuals\nThe feature that interests us in the residuals is the spread. We’ve learned that a good way to compare spreads is to plot the quantiles of each batch against one another.\n\n19.3.1 Pairwise q-q plots with the lattice package\nIf we want to compare all batches of residuals, we can create a pairwise q-q plot using the lattice package.\n\nf.rng <- min(tapply(df2$Height.res, df2$voice.part, length))\nfun1  <- function(x,f.rng) {approx(1:length(x), sort(x), n = f.rng )$y}\nt1    <- aggregate(df2$Height.res ~ df2$voice.part, FUN = fun1, f.rng = f.rng )\nt2    <- as.data.frame(t(t1[,-1]))\nnames(t2) <- t1[,1]\nsplom(~t2,  axis.text.cex = 0.4, pch = 20,\n      varname.cex=0.5,\n      pscales=3,\n      prepanel.limits = function(x) (range(t2)), \n      panel = function(x,y,...){\n        panel.grid()\n        panel.splom(x,y,...)\n        panel.abline(0, 1, col=\"grey\")\n      })\n\n\n\n\nNote that the plots are symmetrical about the diagonal. The q-q plots suggest that the spreads are very similar across singer heights given that the points fall almost perfectly along the one-to-one line.\n\n\n19.3.2 Comparing batches to pooled residuals using a q-q plot\nSince the spreads are homogeneous across the batches, we may choose to combine (pool) the residuals and compare the residuals of each batch to the pooled residuals. The advantage with this approach is that we are increasing the size of the reference residual distribution thus reducing uncertainty that results from a relatively small sample size.\n\ndf3 <- df2 %>%\n  group_by(voice.part)  %>%\n  arrange(Height.res)  %>% \n  mutate(f.val    = (row_number() - 0.5) / n())  %>%\n  ungroup()  %>%\n  mutate(Pooled.res = quantile(Height.res, probs = f.val))  %>%\n  select(voice.part, Height.res, Pooled.res)\n\nggplot(df3, aes(y = Height.res, x = Pooled.res)) + geom_point(alpha = 0.5) + \n              geom_abline(intercept = 0, slope = 1) +\n              facet_wrap(~ voice.part, nrow = 1) \n\n\n\n\nAll eight batches seem to have similar spreads. This makes it possible to compare batch means using a residual-fit spread plot (covered in the next section).\n\n19.3.2.1 What to expect if one or more of the batches have different spreads\nThe residual vs. pooled residual plots can be effective at identifying batches with different spreads. In the following example, we combine four batches generated from an identical distribution (V1, V2, V3 and V4) with two batches generated from a different distribution (V5 and V6). Their boxplots are shown next.\n\n\n\n\n\n\n\n\nNow let’s take a look at the residual vs. pooled residual plots.\n\n\n\n\n\nBatches V5 and V6 clearly stand out as having different distributions from the rest of the batches. But it’s also important to note that V5 and V6 contaminate the pooled residuals. This has the effect of nudging the other four batches away from the one-to-one line. Note what happens when batches V5 and V6 are removed from the pooled residuals.\n\n\n\n\n\nThe tightness of points around the one-to-one line suggests nearly identical distributions between V1, V2, V3 and V4 as would be expected given that they were generated from the same underlying distribution."
  },
  {
    "objectID": "rf.html#residual-fit-spread-plot",
    "href": "rf.html#residual-fit-spread-plot",
    "title": "19  Univariate analysis: Fits and residuals",
    "section": "19.4 Residual-fit spread plot",
    "text": "19.4 Residual-fit spread plot\nSo far, we’ve learned that the spreads of singer height are the same across all batches. This makes it feasible to assess whether the differences in means are comparable in magnitude to the spread of the pooled residuals.\n\n19.4.1 A simple example\nFirst, let’s compare the following two plots. Both plots show two batches side-by-side. The difference in location is nearly the same in both plots (group a and b have a mean of 10 and 11 respectively), but the difference in spreads are not.\n\n\n\n\n\nPlot 2 does not allow us to say, with confidence, that the two batches differ significantly despite both means being different. Plot 1 on the other hand, shows a significant difference in batch locations. One cannot make inferences about differences in central values without knowing the batches’ distributions.\nFor example, in Plot 1, the spread (or difference) in mean values is relatively large compared to the spread of the residuals for each group (note that the spreads are nearly identical between both batches a and b). The difference in means spans one unit while the spread of each sets of residuals spans about the same amount. So the difference in location is significant.\nIf we split each batch in Plot 1 into a location component plot (normalized to the overall mean) and a pooled residual component plot, and then compare those values against a quantile, we get a residual-fit spread plot, or r-f spread plot for short.\n\n\n\n\n\nIt’s clear from this r-f spread plot that the spread of the mean distribution (between batches a and b) is important compared to that of its residuals. This suggests that the groups a and b explain much of the variability in the data.\nFor Plot 2, the difference in mean values is also one unit, but the spread of residuals spans almost 5 units. An r-f spread plot makes this difference quite clear.\n\n\n\n\n\nThe spread between each batch’s fitted mean is small compared to that of the combined residuals suggesting that much of the variability in the data is not explained by the differences between groups a and b.\n\n\n19.4.2 Are the fitted voice part values significantly different?\nWe’ll use lattice’s rfs and oneway functions to generate the r-f plot for the singer data. Note that the plot displays the spread of the modeled means after subtracting these means with the overall mean (remember, the goal is to compare the spreads about a common central value and not to compare their absolute values).\n\nrfs(oneway(height ~ voice.part, data = singer, spread = 1), \n    aspect = 1, \n    ylab = \"Height (inches)\")\n\n\n\n\nAfter normalizing the data to the global mean, the r-f spread plot splits the singer height data into two parts: modeled mean and residual. For example, the smallest value in the Bass 2 group is 66. When normalized to the global mean, that value is -1.29. The normalized value is then split between the group (normalized) mean of 4.1 and its residual of -5.39 (i.e. the difference between its value and the Bass 2 group mean). These two values are then each added to two separate plots: the fitted values plot and the residuals plot. This process is repeated for each observation in the dataset to generate the final r-f spread plot.\n\n\n\nThe spread of the fitted heights (across each voice part) is not insignificant compared to the spread of the combined residuals. So height differences between singer groups cannot be explained by random chance alone or, put another way, the voice-parts can explain a good part of the variation in the data."
  },
  {
    "objectID": "rf.html#generating-a-residual-fit-plot-with-ggplot",
    "href": "rf.html#generating-a-residual-fit-plot-with-ggplot",
    "title": "19  Univariate analysis: Fits and residuals",
    "section": "19.5 Generating a residual-fit plot with ggplot",
    "text": "19.5 Generating a residual-fit plot with ggplot\nTo generate the R-F plot using ggplot2, we must first split the data into its fitted and residual components (something the rfs function did for us). We’ll make use of piping operations to complete this task.\n\ndf1 <- singer %>%\n  mutate(norm.hgt = height - mean(height)) %>% \n  group_by(voice.part) %>% \n  mutate( Residuals  = norm.hgt - mean(norm.hgt),\n          `Fitted Values` = mean(norm.hgt))%>% \n  ungroup() %>% \n  select(-height, -voice.part, -norm.hgt) %>% \n  pivot_longer(names_to = \"type\",  values_to = \"value\", cols=everything()) %>% \n  group_by(type) %>% \n  arrange(value) %>% \n  mutate(`f-value` = (row_number() - 0.5) / n()) \n\nNext, we plot the data.\n\nggplot(df1, aes(x = `f-value`, y = value)) + \n  geom_point(alpha = 0.3, cex = 1.5) +\n  facet_wrap(~ type) +\n  ylab(\"Height (inches)\")\n\n\n\n\nAn alternative to the side-by-side R-F plot is one where both fits and residuals are overlapping.\n\nggplot(df1, aes(x = `f-value`, y = value, col = type)) + \n  geom_point(alpha = 0.3, cex = 1.5) +\n  ylab(\"Height (inches)\")"
  },
  {
    "objectID": "rf.html#comparing-pooled-residuals-to-the-normal-distribution",
    "href": "rf.html#comparing-pooled-residuals-to-the-normal-distribution",
    "title": "19  Univariate analysis: Fits and residuals",
    "section": "19.6 Comparing pooled residuals to the normal distribution",
    "text": "19.6 Comparing pooled residuals to the normal distribution\nOur exploration of the singer height batches have been visual thus far. But there may be times when the analysis may need to culminate in a statistical test. Some of these tests reduce the data to mathematically tractable models such as the mean and the standard deviation (which follows a normal distribution).\nWe’ll take advantage of the pooled residuals to give us a larger sample size for comparison with the theoretical normal distribution.\n\n# Find the equation for the line\nggplot(df3, aes(sample = Pooled.res)) + stat_qq(distribution = qnorm) + \n  geom_qq_line(distribution = qnorm)\n\n\n\n\nThis dataset has behaved quite well. Its batches differed only by location, yet its spread remained homogeneous (enough) across the batches to pool them and enable us to confirm, with greater confidence, that the spread follows a normal distribution.\nThis well behaved dataset allows us to model its spread using the sample standard deviation. It’s important to note that had the data not followed a normal distribution, then characterizing its spread using the standard deviation would have been inappropriate. Unfortunately, many ancillary data analysts seldom check the distribution requirements of their data before choosing to characterize its distribution using the standard deviation. In such a situation, you would have to revert to a far less succinct characterization of spread: the quantile.\nYou can compute the standard deviation as:\n\nsd(df2$Height.res)\n\n[1] 2.465049\n\n\nWe can now model singer height by both voice.part means, and the group standard deviation of 2.47."
  },
  {
    "objectID": "sl_plot.html#introduction",
    "href": "sl_plot.html#introduction",
    "title": "20  Spread-level plots",
    "section": "20.1 Introduction",
    "text": "20.1 Introduction\nSome batches of data may show a systematic change in spread vs. location; a good example is stereogram fusion time data in Cleveland’s book where an increase in location translates into an increase in spread. Such dependency is often undesirable (e.g. in an ANOVA for instance) and preferably removed in an analysis. A plot well suited for visualizing this dependency is the spread-level plot, s-l (or spread-location plot as Cleveland calls it)."
  },
  {
    "objectID": "sl_plot.html#constructing-the-s-l-plot",
    "href": "sl_plot.html#constructing-the-s-l-plot",
    "title": "20  Spread-level plots",
    "section": "20.2 Constructing the s-l plot",
    "text": "20.2 Constructing the s-l plot\nThe plot compares a measure of the spread’s residual to the location (usually the median) for each batch of data. The spread is usually distilled down to its residual (what remains after subtracting each batch value by the batch median) then transformed by taking the square root of its absolute value. The following block of code recreates the s-l plot in figure 2.25 from Cleveland’s book.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/fusion.csv\")\n\n# Create two new columns: group median and group residual\ndf1 <- df %>%\n  group_by(nv.vv)   %>%\n  mutate( Median = median(time),\n          Residuals = sqrt( abs(time - Median)))\n\n# Generate the s-l plot\nggplot(df1, aes(x = Median, y = Residuals)) + \n  geom_jitter(alpha = 0.4, width = 0.2) +\n  stat_summary(fun = median, geom = \"line\", col = \"red\") +\n  ylab(expression(sqrt(abs(\" Residuals \"))))\n\n\n\n\nThe red line in the plot helps identify the type of relationship between spread and location (defined by the group median in this plot). If the line slopes upward, there is an increasing spread as a function of increasing location; if it slopes downward, there is a decreasing spread as a function of increasing location; and if the slope is flat, there is no change in spread as a function of location.\nIn the data fusion example, there is an obvious increase in spread as a function of location.   \n\nNote that if you are to rescale the y-axis, you should use the coord_cartesian(ylim = c( .. , .. )) function instead of the ylim() function. The latter will mask the values above its maximum range from the stat_summary() function, the former will not."
  },
  {
    "objectID": "sl_plot.html#other-examples",
    "href": "sl_plot.html#other-examples",
    "title": "20  Spread-level plots",
    "section": "20.3 Other examples",
    "text": "20.3 Other examples\n  \n\n20.3.1 Cleveland’s food web data\nOn page 60, Cleveland compares spread vs location for three different batches of values: two, three and mixed dimensional ecosystems. The values represent the mean chain length of a web (defined by the number of hierarchical prey-predator links in the web) for the three different ecosystem types.\n\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/Food_web.csv\")\n\n# Create two new columns: group median and group residuals\ndf1 <- df %>%\n  group_by(dimension)  %>%\n  mutate( Median = median(mean.length),\n          Residuals = sqrt( abs( mean.length - Median)))   \n\n# Generate the s-l plot\nggplot(df1, aes(x = Median, y = Residuals)) + \n  geom_jitter(alpha = 0.4, width = 0.05, height = 0) +\n  stat_summary(fun = median, geom = \"line\", col = \"red\") +\n  ylab(expression(sqrt( abs(\" Residuals \")))) +\n  geom_text(aes(x = Median, y = 1.75, label = dimension))\n\n\n\n\nA monotonic spread is apparent in this dataset too, i.e. as the median chain length increases, so does the spread.\n\n\n20.3.2 Buoy data\nIn this next example, we will compare water temperature spreads with median temperatures across 12 months using Gulf of Maine buoy data. Since we have many categories to work with, we will do without the jitter and stack the points within each month group.\n\ndf <- read.table(\"http://mgimond.github.io/ES218/Data/buoy_44005_2012.dat\", header=T)\n\n# Create two new columns: group median and group residual\ndf1 <- df %>%\n       group_by(MM)  %>%\n       mutate(Median = median(WTMP),\n              Residuals = sqrt(abs(WTMP - Median)))  \n\n# Generate the s-l plot\nggplot(df1, aes(x = Median, y = Residuals)) + geom_point(alpha = 0.1) +\n  stat_summary(fun = median, geom = \"line\", col = \"red\") +\n  ylab(expression(sqrt(abs( \" Residuals \")))) +\n  geom_text(aes(x = Median, y = 2.5, label = MM), cex = 3)\n\n\n\n\nNote that the discrete appearance of the values reflects the level of precision used to record water temperature data (a tenth of a degree Celsius). Unlike our previous two examples, a systematic spread as a function of increasing water temperature is not immediately apparent."
  },
  {
    "objectID": "sl_plot.html#variations-of-the-s-l-plot",
    "href": "sl_plot.html#variations-of-the-s-l-plot",
    "title": "20  Spread-level plots",
    "section": "20.4 Variations of the S-L plot",
    "text": "20.4 Variations of the S-L plot\nAnother version of the S-L plot (and one that seems to be the most popular) pits the log of the inter-quartile spread vs the log of the median. This approach only works for positive values (this may require that values be adjusted so that the minimum value be no less than or equal to 0).\nThis approach is appealing in that the slope of the best fit line can be used to come up with a power transformation (a topic covered in next week’s lecture) via power = 1 - slope.\nThis variant of the s-l plot can be computed in R as follows (we will use the buoy data as an example).\n\nsl <- df %>%\n  group_by(MM)  %>%\n  summarise (level  = log(median(WTMP)),\n                IQR = IQR(WTMP),  # Computes the interquartile range\n             spread = log(IQR))\n\nggplot(sl, aes(x = level, y = spread)) + geom_point() + \n  stat_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Median (log)\") + ylab(\"Spread (log)\") +\n  geom_text(aes(x = level, y = spread, label = MM), vjust = 1.21, cex = 2.8)\n\n\n\n\nNote how this plot differs from Cleveland’s plot in that we are only displaying each month’s median spread value and we are fitting a straight line to the points.\nThe slope suggests that there may be a monotonic increase in spread vs location, but a close examination of the regression model indicates that the slope may not be significantly different from 0 (the p-value of 0.12 suggests that there is a 12% chance that the computed slope is consistent with a slope we would expect to get if there was no linear relationship between spread and location values).\n\nsummary(lm(spread ~ level, sl))\n\n\nCall:\nlm(formula = spread ~ level, data = sl)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6491 -0.2979 -0.1910  0.6701  1.4585 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -2.2585     1.4327  -1.576    0.146\nlevel         0.9896     0.5907   1.675    0.125\n\nResidual standard error: 0.8912 on 10 degrees of freedom\nMultiple R-squared:  0.2192,    Adjusted R-squared:  0.1411 \nF-statistic: 2.807 on 1 and 10 DF,  p-value: 0.1248"
  },
  {
    "objectID": "re_express.html#introduction",
    "href": "re_express.html#introduction",
    "title": "21  Re-expressing values",
    "section": "21.1 Introduction",
    "text": "21.1 Introduction\nDatasets do not always follow a nice symmetrical distribution nor do their spreads behave systematically across different levels (e.g. medians). Such distributions do not lend themselves well to visual exploration since they can mask simple patterns. They can also be a problem when testing hypotheses using traditional statistical procedures. A solution to this problem is non-linear re-expression (aka transformation) of the values. In univariate analysis, we often seek to symmetrize the distribution and/or equalize the spread. In multivariate analysis, the objective is to usually linearize the relationship between variables and/or to normalize the residual in a regression model.\n\n\nOne popular form of re-expression is the log (natural or base 10). The other is the family of power transformations (of which the log is a special case) implemented using either the Tukey transformation or the Box-Cox transformation."
  },
  {
    "objectID": "re_express.html#the-log-transformation",
    "href": "re_express.html#the-log-transformation",
    "title": "21  Re-expressing values",
    "section": "21.2 The log transformation",
    "text": "21.2 The log transformation\nOne of the most popular transformations used in data analysis is the logarithm. The log, \\(y\\), of a value \\(x\\) is the power to which the base must be raised to produce \\(x\\). This requires that the log function be defined by a base, \\(b\\), such as 10, 2 or exp(1) (the latter defining the natural log).\n\\[\ny = log_b(x) \\Leftrightarrow  x=b^y\n\\]\nIn R, the base is defined by passing the parameter base= to the log() function as in log(x , base=10).\nRe-expressing with the log is particularly useful when the change in one value as a function of another is multiplicative and not additive. An example of such a dataset is the compounding interest. Let’s assume that we start off with $1000 in an investment account that yields 10% interest each year. We can calculate the size of our investment for the next 50 years as follows:\n\nrate <- 0.1                 # Rate is stored as a fraction\ny    <- vector(length = 50) # Create an empty vector that can hold 50 values\ny[1] <- 1000                # Start 1st year with $1000\n\n# Next, compute the investment ammount for years 2, 3, ..., 50.\n# Each iteration of the loop computes the new amount for year i based \n# on the previous year's amount (i-1).\nfor(i in 2:length(y)){\n  y[i] <- y[i-1] + (y[i-1] * rate)  # Or y[i-1] * (1 + rate)\n}\n\nThe vector y gives us the amount of our investment for each year over the course of 50 years.\n\n\n [1]   1000.000   1100.000   1210.000   1331.000   1464.100   1610.510   1771.561   1948.717\n [9]   2143.589   2357.948   2593.742   2853.117   3138.428   3452.271   3797.498   4177.248\n[17]   4594.973   5054.470   5559.917   6115.909   6727.500   7400.250   8140.275   8954.302\n[25]   9849.733  10834.706  11918.177  13109.994  14420.994  15863.093  17449.402  19194.342\n[33]  21113.777  23225.154  25547.670  28102.437  30912.681  34003.949  37404.343  41144.778\n[41]  45259.256  49785.181  54763.699  60240.069  66264.076  72890.484  80179.532  88197.485\n[49]  97017.234 106718.957\n\n\nWe can plot the values as follows:\n\nplot(y, pch = 20)\n\n\n\n\nThe change in difference between values from year to year is not additive, in other words, the difference between years 48 and 49 is different than that for years 3 and 4.\n\n\n\n\n\n\n\nYears\nDifference\n\n\n\n\ny[49] - y[48]\n8819.75\n\n\ny[4] - y[3]\n121\n\n\n\nHowever, the ratios between the pairs of years are identical:\n\n\n\n\n\n\n\nYears\nRatio\n\n\n\n\ny[49] / y[48]\n1.1\n\n\ny[4] / y[3]\n1.1\n\n\n\nWe say that the change in value is multiplicative across the years. In other words, the value amount 6 years out is \\(value(6) = (yearly\\_increase)^{6} \\times 1000\\) or 1.1^6 * 1000 = 1771.561 which matches value y[7].\nWhen we expect a variable to change multiplicatively as a function of another variable, it is usually best to transform the variable using the logarithm. To see why, plot the log of y.\n\nplot(log(y), pch=20)\n\n\n\n\nNote the change from a curved line to a perfectly straight line. The logarithm will produce a straight line if the rate of change for y is constant over the range of x. This is a nifty property since it makes it so much easier to see if and where the rate of change differs. For example, let’s look at the population growth rate of the US from 1850 to 2013.\n\ndat <- read.csv(\"http://personal.colby.edu/personal/m/mgimond/R/Data/Population.csv\", header=TRUE)\nplot(US ~ Year, dat, type=\"l\") \n\n\n\n\nThe population count for the US follows a slightly curved (convex) pattern. It’s difficult to see from this plot if the rate of growth is consistent across the years (though there is an obvious jump in population count around the 1950’s). Let’s log the population count.\n\nplot(log(US) ~ Year, dat, type=\"l\")  \n\n\n\n\nIt’s clear from the log plot that the rate of growth for the US has not been consistent over the years (had it been consistent, the line would have been straight). In fact, there seems to be a gradual decrease in growth rate over the 150 year period (though a more thorough analysis would be needed to see where and when the growth rates changed).\nA logarithm is defined by a base. Some of the most common bases are 10, 2 and exp(1) with the latter being the natural log. The bases can be defined in the call to log() by adding a second parameter to that function. For example, to apply the log base 2 to the 5th value of the vector y, type log( y[5], 2). To apply the natural log to that same value, simply type log( y[5], exp(1)). If you don’t specify a base, R will default to the natural log.\nThe choice of a log base will not impact the shape of the logged values in the plot, only in its absolute value. So unless interpretation of the logged value is of concern, any base will do. Generally, you want to avoid difficult to interpret logged values. For example, if you apply log base 10 to the investment dataset, you will end up with a smaller range of values thus more decimal places to work with whereas a base 2 logarithm will generate a wider range of values and thus fewer decimal places to work with.\n\n\n\n\n\nA rule of thumb is to use log base 10 when the range of values to be logged covers 3 or more powers of ten, \\(\\geq 10^3\\) (for example, a range of 5 to 50,000); if the range of values covers 2 or fewer powers of ten, \\(\\leq 10^2\\)(for example, a range of 5 to 500) then a natural log or a log base 2 log is best."
  },
  {
    "objectID": "re_express.html#the-tukey-transformation",
    "href": "re_express.html#the-tukey-transformation",
    "title": "21  Re-expressing values",
    "section": "21.3 The Tukey transformation",
    "text": "21.3 The Tukey transformation\nThe Tukey family of transformations offers a broader range of re-expression options (which includes the log). The values are re-expressed using the algorithm:\n\\[\n\\begin{equation} T_{Tukey} =\n\\begin{cases} x^p , & p \\neq  0 \\\\\n              log(x), & p = 0  \n\\end{cases}\n\\end{equation}\n\\] The objective is to find a value for \\(p\\) from a “ladder” of powers (e.g. -2, -1, -1/2, 0, 1/2, 1, 2) that does a good job in re-expressing the batch of values. Technically, \\(p\\) can take on any value. But in practice, we normally pick a value for \\(p\\) that may be “interpretable” in the context of our analysis. For example, a log transformation (p=0) may make sense if the process we are studying has a steady growth rate. A cube root transformation (p = 1/3) may make sense if the entity being measured is a volume (e.g. rain fall measurements). But sometimes, the choice of \\(p\\) may not be directly interpretable or may not be of concern to the analyst.\nA nifty solution to finding an appropriate \\(p\\) is to create a function whose input is the vector (that we want to re-express) and a \\(p\\) parameter we want to explore.\n\nRE <- function(x, p = 0) {\n  if(p != 0) {\n    z <- x^p\n  } else{\n    z <- log(x)\n  }\n  return(z)\n}\n\nTo use the custom function RE simply pass two vectors: the batch of numbers being re-expressed and the \\(p\\) parameter.\n\n# Create a skewed distribution of 50 random values\nset.seed(9)\na <- rgamma(50, shape = 1)\n\n# Let's look at the skewed distribution\nboxplot(a, horizontal = TRUE)\n\n\n\n\nThe batch is strongly skewed to the right. Let’s first try a square-root transformation (p=1/2)\n\na.re <- RE(a, p = 1/2)   \nboxplot(a.re, horizontal = TRUE)\n\n\n\n\nThat certainly helps minimize the skew, but the distribution still lacks symmetry. Let’s try a log transformation (p=0):\n\na.re <- RE(a, p = 0)   \nboxplot(a.re, horizontal = TRUE)\n\n\n\n\nThat’s a little too much over-correction; we don’t want to substitute a right skew for a left skew. Let’s try a power in between (i.e. p=1/4):\n\na.re <- RE(a, p = 1/4)   \nboxplot(a.re, horizontal = TRUE)\n\n\n\n\nThat’s much better. The distribution is now nicely balanced about its median."
  },
  {
    "objectID": "re_express.html#the-box-cox-transformation",
    "href": "re_express.html#the-box-cox-transformation",
    "title": "21  Re-expressing values",
    "section": "21.4 The Box-Cox transformation",
    "text": "21.4 The Box-Cox transformation\nAnother family of transformations is the Box-Cox transformation. The values are re-expressed using a modified version of the Tukey transformation:\n\\[\n\\begin{equation} T_{Box-Cox} =\n\\begin{cases} \\frac{x^p - 1}{p}, & p \\neq  0 \\\\\n              log(x), & p = 0\n\\end{cases}\n\\end{equation}\n\\] Just as we can create a custom Tukey transformation function, we can create a Box-Cox transformation function too:\n\nBC <- function(x, p = 0) {\n  if(p == 0) {\n    z <- log(x)\n  } else {\n    z <- (x^p - 1)/p\n  }\n  return(z)\n}\n\nWhile both the Box-Cox and Tukey transformations method will generate similar distributions when the power p is 0 or greater, they will differ in distributions when the power is negative. For example, when re-expressing mtcars$mpg using an inverse power (p = -1), Tukey’s re-expression will change the data order but the Box-Cox transformation will not as shown in the following plots:.\n\nplot(mpg ~ disp, mtcars, main = \"Original data\")\nplot(RE(mtcars$mpg, p = -1) ~ mtcars$disp, main = \"Tukey\")\nplot(BC(mtcars$mpg, p = -1) ~ mtcars$disp, main = \"Box-Cox\")\n\n\n\n\nThe original data shows a negative relationship between mpg and disp; the Tukey re-expression takes the inverse of mpg which changes the nature of the relationship between the y and x variables where whe have a positive relationship between the re-expressed mpg variable and disp variable (note that by simply changing the sign of the re-expressed value, -x^(-1) maintains the nature of the original relationship); the Box-Cox transformation, on the other hand, maintains this negative relationship.\nThe choice of re-rexpression will depend on the analysis context. For example, if you want an easily interpretable transformation then opt for the Tukey re-expression. If you want to compare the shape of transformed variables, the Box-Cox approach will be better suited."
  },
  {
    "objectID": "re_express.html#re-expressing-to-stabilize-spread",
    "href": "re_express.html#re-expressing-to-stabilize-spread",
    "title": "21  Re-expressing values",
    "section": "21.5 Re-expressing to stabilize spread",
    "text": "21.5 Re-expressing to stabilize spread\nA spread vs level plot not only tells us if there is a systematic relationship between spread and level, it can also suggest the power transformation to use. Note that the s-l method discussed here is not the one presented in Cleveland’s book (see the section titled “Spread-location plot” on the course website for an alternative version of the plot).\n\nlibrary(dplyr)\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/Food_web.csv\")\n\n# Create s-l table \ndf.sl <- df %>%\n  group_by(dimension) %>%\n  summarise(med  = log(median(mean.length)),\n            IQR  = IQR(mean.length),  \n            sprd = log(IQR))\n\n# Plot spread vs median\nplot(sprd ~ med, df.sl, pch = 16)\n\n\n\n\nThe plot suggests a monotonic relationship between spread and median. Next, we’ll fit a line to this scatter plot and compute its slope. We’ll use the lm() function, but note that any other line fitting strategies could be used as well.\n\nplot(sprd ~ med, df.sl, pch = 16)\n\n# Run regression model\nM <- lm(sprd ~ med, df.sl)\nabline(M, col = \"red\")\n\n\n\n\nThe slope can be used to come up with the best power transformation to minimize the systematic increase in spread: \\(p = 1 - slope\\).\nThe slope can be extracted from the model M using the coef function:\n\n coef(M)\n\n(Intercept)         med \n  -3.003785    2.969673 \n\n\nThe second value in the output is the slope. So the power to use is 1 - 2.97 or -1.97. We will use the power transformation function BC to re-express the mean.length values. We’ll add the re-expressed values as a new column to df:\n\ndf$re.mean.length <- BC(df$mean.length, -1.97)\n\nLet’s compare boxplots between the original values with the re-expressed values.\n\nboxplot(mean.length ~ dimension, df, main = \"Original data\")\nboxplot(re.mean.length ~ dimension, df, main = \"Re-expressed data\")\n\n\n\n\nRecall that our goal here was to minimize any systematic relationship between spread and median. The re-expression seems to have equalized the spreads across the three groups.\nWe’ll check for a homogeneous spread across fitted medians using the original spread-level plot covered in last week’s lecture.\n\nlibrary(ggplot2)\ndf1 <- df %>%\n  group_by(dimension)  %>%\n  mutate( Median = median(re.mean.length),\n          Residuals = sqrt( abs( re.mean.length - Median)))   \n\n# Generate the s-l plot\nggplot(df1, aes(x = Median, y = Residuals)) + \n  geom_jitter(alpha = 0.3,width = 0.003,height = 0) +\n  stat_summary(fun.y = median, geom = \"line\", col = \"red\") +\n  ylab(expression(sqrt( abs( \" Residuals \")))) +\n  geom_text(aes(x = Median, y = 0.6, label = dimension) )\n\n\n\n\nThe plot suggests that the re-expression does a good job in stabilizing the spread."
  },
  {
    "objectID": "letter_values.html#introduction",
    "href": "letter_values.html#introduction",
    "title": "22  Letter value summaries",
    "section": "22.1 Introduction",
    "text": "22.1 Introduction\nThe boxplot is a five number summary of a batch of values that gives us a handle on the symmetry (or lack thereof) of the data. The five numbers consist of the median, the inter-quartile values and the upper and lower adjacent values (aka whiskers). The letter value summary was introduced by John Tukey and extends the boxplot’s 5 number summary by exploring the symmetry of the batch for depth levels other than the half (median) or the fourth (quartiles)."
  },
  {
    "objectID": "letter_values.html#constructing-the-letter-value-summaries",
    "href": "letter_values.html#constructing-the-letter-value-summaries",
    "title": "22  Letter value summaries",
    "section": "22.2 Constructing the letter value summaries",
    "text": "22.2 Constructing the letter value summaries\n\n\n\nLet’s start with a simple batch of numbers: 24, 3, 5, 10, 13, 6, 16, 22, 4, 19, 17.\n\n22.2.0.1 Order the values\nFirst, we order the numbers from lowest to highest.\n\n\n\n22.2.0.2 Find the median (M)\nNext, we find the median. It’s the location in the enumerated values that splits the batch into two equal sets of values. If the number of values is odd, then the median is the value furthest from the ends. If the number of values in the batch is even, then the median is the average of the two middle values, each furthest from its end. A simple formula to identify the element number (or depth) of the batch associated with the median is:\n\\[\ndepth\\ of\\ median = \\frac{n + 1}{2}\n\\]\nwhere \\(n\\) is the number of values in the batch. In our example, we have 11 values, so the the median is (11 + 1)/2 or 6; it’s the 6th element from the left (or the right) of the sorted values.\n\nIf our batch consisted of an even number of values such as 10, the the median would be the 5.5th value which does not coincide with an existing value. This would require that we find the 5th and 6th element in the batch, then compute their average to find the median.\nThe median is the value furthest from the extreme values; it’s said to have the greatest depth (e.g. a depth of 6 in our example). The minimum and maximum values have the lowest depth with a depth value of 1, each.\n\n\n22.2.0.3 Find the hinges (H)\nNext, we take both halves of our batch of ordered numbers and find the middle of each. These mid points are referred to as hinges. They can be easily computed by modifying the formula used to find the median: we simply substitute the value \\(n\\) with the depth associated with the median, \\(d(M)\\) (i.e. the median becomes an extreme value in this operation).\n\\[\ndepth\\ of\\ hinge = \\frac{d(M) + 1}{2}\n\\]\nIn our working example, the depth of the median is 6, therefore the depth of the hinge is (6+1)/2 = 3.5. So the hinge is the 3.5th element from the left (or right) of the first half of the batch and the 3.5th element from the left (or right) of the second half of the batch. Since the depth does not fall on an existing value, we need to compute it using the two closes values (depth 3 and depth 4). This gives us (8+11)/2=9.5 for the left hinge and (22+24)/2=23 for the right hinge.\n\nIf our batch consisted of even number of values, we would need to drop the ½ fraction from depth of the median before computing the depth of the hinge. For example, if we had 10 values the depth of the median would be 5.5 and the depth of the hinge would be calculated as (5+1)/2.\nNote that the hinges are similar to the quartiles but because they are computed differently, their values may be slightly different.\n\n\n22.2.0.4 Find the other letter summaries (E, D, C, B, A, etc…)\nSO far, we’ve found the median (M) and the hinges (H). We keep computing the depths for each outer group of values delimited by the outer extreme values and the previous depth. For example, the mid-point of the outer quarters gives us our eights (E):\n\\[\ndepth\\ of\\ eights = \\frac{d(H) + 1}{2}\n\\]\nor, after dropping the ½ fraction from the depth of the hinge, (3+1)/2=2.\nThis continues until we’ve exhausted all depths (i.e. until we reach a depth of 1 associated with the minimum and maximum values). Once past the eight, we label each subsequent depths using letters in reverse lexicographic order starting with D (for sixteenth) then C, B, A, Z, Y, etc…\nIn our working example, we stop at a depth of D (though some will stop at a depth of two and only report the extreme values thereafter).\n\n\n22.2.0.5 The mids and spreads\nOnce we’ve identified the values associated with each depth, we compute the middle value for each depth pair. For example, the middle value for the paired hinges is 16.25; the middle value for the paired eights is 14; and so on. We can also compute the spread for each depth by computing the difference between each paired value.\n\nThe letter value summary is usually reported in tabular form:\n\n\n\n\n \n  \n    letter \n    depth \n    lower \n    mid \n    upper \n    spread \n  \n \n\n  \n    M \n    6.0 \n    13.0 \n    13.00 \n    13 \n    0.0 \n  \n  \n    H \n    3.5 \n    5.5 \n    11.75 \n    18 \n    12.5 \n  \n  \n    E \n    2.0 \n    4.0 \n    13.00 \n    22 \n    18.0 \n  \n  \n    D \n    1.5 \n    3.5 \n    13.25 \n    23 \n    19.5 \n  \n  \n    C \n    1.0 \n    3.0 \n    13.50 \n    24 \n    21.0 \n  \n\n\n\n\n\nA custom function is available on the course website that will compute the letter value summaries. To access the function (named lsum) source the R es218.R script as follows.\n\nsource(\"http://mgimond.github.io/ES218/es218.R\")\n\nThen to generate the letter summary function for a batch of values x,\n\nx <- c(22, 8, 11, 3, 26, 1, 14, 18, 20, 25, 24)\nlsum(x)\n\n  letter depth lower   mid upper spread\n1      M   6.0  18.0 18.00  18.0    0.0\n2      H   3.5   9.5 16.25  23.0   13.5\n3      E   2.0   3.0 14.00  25.0   22.0\n4      D   1.5   2.0 13.75  25.5   23.5\n5      C   1.0   1.0 13.50  26.0   25.0"
  },
  {
    "objectID": "letter_values.html#interpreting-the-letter-value-summaries",
    "href": "letter_values.html#interpreting-the-letter-value-summaries",
    "title": "22  Letter value summaries",
    "section": "22.3 Interpreting the letter value summaries",
    "text": "22.3 Interpreting the letter value summaries\nLet’ explore the letter summary values for five simulated distributions. We’ll start with a strong right-skewed distribution then progress to a strong left-skewed distribution with a Gaussian (normal) distribution in between.\n\n\n\n\n\nNote the shape of the letter summaries vis-a-vis the direction of the skew. Note too that the letter value summary plot is extremely sensitive to deviations from perfect symmetry. This is apparent in the middle plot which is for a perfectly symmetrical (Gaussian) distribution. The reason has to do with machine precision: the range of values along the y-axis is extremely small, \\(10^{-16}\\), which is the lower limit of the computer’s precision.\nThis sensitivity has its rewards. Note the second plot from the left and the right. The asymmetry is barely noticeable in both distributions, yet the letter value summaries does a great job in identifying the slight asymmetry. Even the boxplots cannot convey this asymmetry as effectively.\n\n\n\n\n\nThis is not to say that just because asymmetry is present in the letter summary values we necessarily have a problem; but it may warrant further exploration before proceeding with the analysis–especially if statistical procedures warrant it."
  },
  {
    "objectID": "robustness.html#introduction",
    "href": "robustness.html#introduction",
    "title": "24  Robustness",
    "section": "24.1 Introduction",
    "text": "24.1 Introduction\nThe data represent 1,2,3,4-tetrachlorobenzen (TCB) concentrations (in ppb) for two site locations: a reference site free of external contaminants and a cleaned contaminated site (Millard et al., p. 416-417).\n\n# Create the two data objects (TCB concentrations for reference and contaminated sites)\nRef <-  c(0.22,0.23,0.26,0.27,0.28,0.28,0.29,0.33,0.34,0.35,0.38,0.39,\n          0.39,0.42,0.42,0.43,0.45,0.46,0.48,0.5,0.5,0.51,0.52,0.54,\n          0.56,0.56,0.57,0.57,0.6,0.62,0.63,0.67,0.69,0.72,0.74,0.76,\n          0.79,0.81,0.82,0.84,0.89,1.11,1.13,1.14,1.14,1.2,1.33)\nCont <- c(0.09,0.09,0.09,0.12,0.12,0.14,0.16,0.17,0.17,0.17,0.18,0.19,\n          0.2,0.2,0.21,0.21,0.22,0.22,0.22,0.23,0.24,0.25,0.25,0.25,\n          0.25,0.26,0.28,0.28,0.29,0.31,0.33,0.33,0.33,0.34,0.37,0.38,\n          0.39,0.4,0.43,0.43,0.47,0.48,0.48,0.49,0.51,0.51,0.54,0.6,\n          0.61,0.62,0.75,0.82,0.85,0.92,0.94,1.05,1.1,1.1,1.19,1.22,\n          1.33,1.39,1.39,1.52,1.53,1.73,2.35,2.46,2.59,2.61,3.06,3.29,\n          5.56,6.61,18.4,51.97,168.64)\n\n# We'll create a long-form version of the data for use with some of the functions\n# in this exercise\ndf <- data.frame( Site = c(rep(\"Cont\",length(Cont) ), rep(\"Ref\",length(Ref) ) ),\n                  TCB  = c(Cont, Ref ) )\n\nOur goal is to assess if, overall, the concentrations of TCB at the contaminated site are greater than those at the reference."
  },
  {
    "objectID": "robustness.html#a-typical-statistical-approach-the-two-sample-t-test",
    "href": "robustness.html#a-typical-statistical-approach-the-two-sample-t-test",
    "title": "24  Robustness",
    "section": "24.2 A typical statistical approach: the two sample t-Test",
    "text": "24.2 A typical statistical approach: the two sample t-Test\nWe are interested in answering the question: “Did the cleanup at the contaminated site reduce the concentration of TCB down to background (reference) levels?”. If the question being addressed is part of a decision making process such as “Should we continue with the remediation?” we might want to assess if the difference in TCBs between both sites is “significant” enough to conclude that the TCBs are higher than would be expected if chance alone was the process at play.\nA popular statistical procedure used to help address this question is the two sample t-Test. The test is used to assess whether or not the mean concentration between both batches of values are significantly different from one another. The test can be framed in one of three ways: We can see if the batches are similar, if one batch is greater than the other batch, or if one batch is smaller than the other batch. In our case, we will assess if the Cont batch is greater than the Ref batch (this is the alternative hypothesis). We’ll make use of the t.test function and set the parameter alt to \"greater\" (indicating that we are assessing if the Cont mean is significantly greater than that of Ref).\n\nt.test(Cont, Ref, alt=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  Cont and Ref\nt = 1.4538, df = 76.05, p-value = 0.07506\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.4821023        Inf\nsample estimates:\nmean of x mean of y \n3.9151948 0.5985106 \n\n\nThe test suggests that there is just a small chance (7.5%) that the two batches of concentrations are the same. The test outputs the means of each batch: 3.9 ppb for the contaminated site and 0.6 ppb for the reference site.\nMany ancillary data analysts may stop here and proceed with the decision making process. This is not good practice. To see why, let’s deconstruct the t-test.\nFirst, we need to find out how the test is representing the batches of numbers. The t-test characterizes the location of the batch using the mean, and the spread using the standard deviation. Nothing more. In essence, the test is reducing the complexity of the batches down to two pairs of numbers.\n\n\n\nThe t-test uses these pairs of numbers to reconstruct, then compare the distributions. Here are the distributions the t-test thinks it’s comparing:\n\n\n\n\n\nThe red line is the distribution of TCB values for the contaminated site (standard deviation = 20.02) and the black line is the concentration of TCB values for the reference site (standard deviation = 0.28). The red vertical line is the mean concentration of the contaminated site which falls well outside the range of concentrations observed at the reference site.\n\nAs an aside, it’s worth mentioning that the variances are clearly unequal thus violating a basic requirement for the t-test however, R invokes the Welch’s t-test by default to mitigate for unequal variance and sample size. But, Welch’s test does require that the distributions follow a normal theoretical distribution (i.e. they can be different in size but must be normal in shape).\n\nThe spread for the contaminated site is several orders of magnitude greater than that of the reference site. But are these reconstructed distributions really representative of the data?\nLet’s plot the density distributions for both batches:\n\nOP <- par(mfrow = c(1,2), mar=c(3,3,2,2))\nplot(density(Ref))\nplot(density(Cont))\npar(OP)\n\n\n\n\nThese are clearly different looking distributions from those assumed by the t-test which should put the t-test results into question.\nThe t-test is clearly not properly representing the distribution of the two batches. The contaminated site, Cont, has several extreme values that is severely skewing the distribution. We can create a normal q-q plot to see how many values are skewing the distribution.\n\n  qqnorm(Cont, pch=20, col=rgb(0,0,0,0.4))\n  qqline(Cont,lty=3)\n\n\n\n\nIt looks like at least four values (which represent ~5% of the data) are contributing to the strong skew and to a much distorted representation of location and spread. The mean and standard deviation are not robust to extreme values. In essence, all it takes is one single outlier to heavily distort the representation of location and spread in our data. We say that the mean and standard deviation have a breakdown point of 1/n where n is the sample size.\nThe median and interquartile range are less sensitive to extreme values. In fact, the median has a breakdown point of n/2. In other words, half of the values would have to be modified to alter the median.\nThe boxplot makes use of these robust measures of location and spread; let’s compare the batches with and without the extreme (outlier) values.\n\nOP <- par(mfrow=c(1,2), mar=c(3,2,1,1))\n# Boxplot with outliers\nboxplot(TCB ~ Site, df, main=\"With outliers\")\n# Boxplot without outliers\nboxplot(TCB ~ Site, df, main=\"Without outliers\", outline=FALSE)\npar(OP)\n\n\n\n\nNote that because of the robust nature of the median and interquartile range, the boxplot helps us to spot the outliers. In fact, the boxplot has a breakdown point of n/4 (i.e. 25% of the values must be extreme before we see any masking of extreme values). The standard deviation, on the other hand, can be inflated by one extreme value thus masking the potentially problematic values.\nOne observation that can also be gleaned from this plot is the skewed nature of the Cont data within the interquartile range (IQR). This suggests that even if we were to remove the outliers, the data would violate the normal distribution requirements."
  },
  {
    "objectID": "robustness.html#re-expression",
    "href": "robustness.html#re-expression",
    "title": "24  Robustness",
    "section": "24.3 Re-expression",
    "text": "24.3 Re-expression\nIf we are to use the t-test, we need to make sure that the distributional requirements are met. Even Welch’s modification has one requirement about the distribution: both spreads must follow a normal distribution. Let’s compare the batches to a theoretical normal distribution via a theoretical q-q plot:\n\nOP <- par(mfrow=c(1,2), mar=c(2,2,1,1))\nqqnorm(Ref, pch=20, col=rgb(0,0,0,0.3))\nqqline(Ref)\nqqnorm(Cont, pch=20, col=rgb(0,0,0,0.3))\nqqline(Cont)\npar(OP)\n\n\n\n\nThese batches do not follow the straight line suggesting skewness in the distribution (as was noted with the boxplots). A workaround to this problem is to re-express the batches of values in such a way to render them as close to normal as possible. However, in doing so, we must make sure that both batches are re-expressed in an equal way to facilitate comparison. A popular re-expression used with observational data that exhibit skewness towards higher values is the log transformation:\n\nOP <- par(mfrow=c(1,2), mar=c(2,2,1,1))\nqqnorm(log(Ref), pch=20, col=rgb(0,0,0,0.3))\nqqline(log(Ref))\nqqnorm(log(Cont), pch=20, col=rgb(0,0,0,0.3))\nqqline(log(Cont))\npar(OP)\n\n\n\n\nLog-transforming the concentrations seems to have done a good job in creating normally distributed numbers. Let’s re-run the t-Test on the log-transformed values.\n\nt.test(log(Cont), log(Ref), alt=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  log(Cont) and log(Ref)\nt = 0.42589, df = 101.99, p-value = 0.3355\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.2090447        Inf\nsample estimates:\n mean of x  mean of y \n-0.5474262 -0.6195712 \n\n\nNote that the result differs significantly from that with the raw data. This last run gives us a p-value of 0.34 (suggesting little difference in overall concentrations) whereas the first run gave us a p-value of 0.075 (suggesting that the contaminated site may have had, overall, greater concentration values).\nAt this point, it’s important to remind ourselves what we are comparing after having log-transformed the data; we are no longer comparing means but the log of means instead. More specifically, via mathematical construct, it can be shown that a log-transformation of the data results in testing the hypothesis that the ratio of mean concentrations are equal to 1."
  },
  {
    "objectID": "robustness.html#fine-tuning-the-re-expression",
    "href": "robustness.html#fine-tuning-the-re-expression",
    "title": "24  Robustness",
    "section": "24.4 Fine-tuning the re-expression",
    "text": "24.4 Fine-tuning the re-expression\nThe log transformation is one of many re-expressions that can be applied to the data. Let’s explore the skewness across different “depths” of the Cont values to see if the skewness is systematic. We’ll use letter value summary plots to help guide us to a reasonable re-expression.\nFirst, we’ll look at the raw contaminated site data:\n\nsource(\"http://mgimond.github.io/ES218/es218.R\") # Needed to run the lsum() function\nlibrary(ggplot2)\n\n# Generate letter value summary table\nCont.lsum <- lsum(Cont, l=7)\n\n# Plot the letter values\nggplot(Cont.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + \n                    scale_x_reverse() +geom_text(vjust=-.5, size=4)\n\n\n\n\nThe data become strongly skewed for 1/32th of the data (depth letter C). Let’s now look at the reference site.\n\nRef.lsum <- lsum(Ref, l=7)\nggplot(Ref.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + \n                   scale_x_reverse() +geom_text(vjust=-.5, size=4)\n\n\n\n\nA skew is also prominent here but a bit more consistent across the depths with a slight drop between depths D and C (16th and 32nd extreme values).\nNext, we will find a power function that re-expresses the values to satisfy the t-test distribution shape requirement. We’ll first look at the log transformation implemented in the last section. Note that we are using the custom Box-cox function bc from the ES218.R source script to transform the data (recall that the log transformation is one of a large family of Box-Cox transformations).\n\nlibrary(dplyr)\n\ndf  %>% \n  group_by(Site) %>%\n  do(lsum( bc(.$TCB,0), l=7) ) %>%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThe log transformation seems to work well with the reference site, but it’s not aggressive enough for the contaminated site. Recall that to ensure symmetry across all levels of the batches, the letter values must follow a straight (horizontal) line. Let’s try a power of -1/2:\n\ndf  %>% \n  group_by(Site) %>%\n  do(lsum( bc(.$TCB,-0.5), l=7) ) %>%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThis seems to be too aggressive. We are facing a situation where attempting to normalize one batch distorts the other batch. Let’s try a compromise and use -.35.\n\ndf  %>% \n  group_by(Site) %>%\n  do(lsum( bc(.$TCB,-0.35), l=7) ) %>%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThis seems to be a bit better. It’s obvious that we will not find a power transformation that will satisfy both batches, so we will need to make a judgement call and work with a power of -.35 for now.\nLet’s compare the re-expressed batches with a normal distribution.\n\nOP <- par(mfrow=c(1,2), mar=c(2,2,1,1))\nqqnorm(bc(Ref,-0.35), pch=20, col=rgb(0,0,0,0.3), main=\"Reference\")\nqqline(bc(Ref,-0.35))\nqqnorm(bc(Cont,-0.35), pch=20, col=rgb(0,0,0,0.3), main=\"Contaminated\")\nqqline(bc(Cont,-0.35))\npar(OP)\n\n\n\n\nThe distributions do not look too bad when viewed in a q-q plot. Note how the letter values summary plot can pick up on subtle skewness that may not be apparent in a q-q plot. But is the observed skewed distribution after applying a power transformation really significant? I.e. is it small enough not to bias our t-test results?\nLet’s run the t-test.\n\nt.test(bc(Cont,-0.35), bc(Ref,-0.35), alt=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  bc(Cont, -0.35) and bc(Ref, -0.35)\nt = -1.0495, df = 111.68, p-value = 0.8519\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.4845764        Inf\nsample estimates:\n mean of x  mean of y \n-0.9264188 -0.7386209 \n\n\nNote that the result differs significantly from that with the raw data. This last run gives us a p-value of 0.85 whereas the first run gave us a p-value of 0.075 and the log-transformed run gave us a p-value of 0.34. If you tweak the power transformation ever so slightly, you will note that the p-value can change measurably, but fortunately not enough to alter the conclusion that there is no significant difference in concentrations between both sites when comparing the batches using the mean raised to the -0.35th power."
  },
  {
    "objectID": "robustness.html#robust-tests",
    "href": "robustness.html#robust-tests",
    "title": "24  Robustness",
    "section": "24.5 Robust tests",
    "text": "24.5 Robust tests\nIt should be clear by now that many of the popular statistical procedures that reduce the data to a mean and standard deviation are not robust to datasets that don’t fit these models well. In fact, most observational data seldom follow a nice normal distribution. The above exercise demonstrates how a very simple implementation of a t-Test can result in a lengthy detour through exploration and re-expression. This can be time consuming when exploring many different datasests. Fortunately there are several alternative statistics that are far less restrictive than the t-Test but serve the same purpose: comparing batches of numbers. These are covered here very superficially for reference.\n\n24.5.1 Permutation test\nThe idea here is that if concentrations of TCB come from an identical site, then it should not matter which batch (Ref or Cont) a concentration comes from. By mixing up (permuting) the values across batches, we can come up with a distribution of mean (or median) concentration differences between batches we would expect to get if there was no difference, then compare our observed mean (or median) differences to that of the distribution of simulated mean (or median) differences. In the following example, we will we choose the median over the mean because of its robust measure of location.\n\n# Pool the concentrations\nPool <- c(Ref, Cont)\n\n# Create an empty vector that will store the simulated median differences\nmed.dif <- vector()\n\n# Run simulations\nfor (i in 1:999){\n  # Permute the pooled data then assign the resampled data to each batch\n  Pool.rnd <- sample(Pool, replace=FALSE)\n  # Grab the first batch of values\n  Cont.rnd <- Pool.rnd[1:length(Cont)]\n  # Grab the second batch of values\n  Ref.rnd <- Pool.rnd[ (length(Cont)+1):length(Pool)]\n  # Compute median differences\n  med.dif[i] <- median(Cont.rnd) - median(Ref.rnd)\n}\n\n# Plot the distribution of median differences  \nhist(med.dif)\n# Now let's see where our observed difference in median concentration lies\nabline(v = median(Cont) - median(Ref), col=\"red\", lw=2)\n\n\n\n\nWe can compute a pseudo p-value from the above. Note that we are interested in the number of simulated values that are different than our observed value (i.e. we’re not concerned with the direction of our value), we are therefore conducting a two-side test whose p-value can be computed as:\n\nN.greater <- sum( (median(Cont) - median(Ref)) >= med.dif) # Number of simulated differences\n                                                           # greater than the observed value\nn <- length(med.dif) #number of simulated values\np <- 2 * min(N.greater + 1, n + 1 - N.greater) / (n +1)\np\n\n[1] 0.138\n\n\nHere, the p-value gives us the probability that our observed difference in median concentration value is consistent with the expected difference if the two sites were identical. In our example, that probability is around 0.14 suggesting that overall, the concentrations at both sites are relatively the same.\n\n\n24.5.2 Wilcoxon rank sum test\nThis is another popular alternative to the t-Test. The technical implementation and interpretation is identical to that of the t-Test. It differs from the t-Test in that it is based on the observation ranks as opposed to the observation means. Here, we implement a two-sided test addressing the question “are the differences in concentrations between the sites significant”.\n\nwilcox.test(Cont, Ref, alternative = \"two.sided\")\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Cont and Ref\nW = 1582, p-value = 0.2423\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe p-value (which is similar to that found via the permutation technique) suggests that the difference in overall concentrations is not that great between both sites… despite the presence of a few outliers!"
  },
  {
    "objectID": "robustness.html#dont-forget-the-outliers",
    "href": "robustness.html#dont-forget-the-outliers",
    "title": "24  Robustness",
    "section": "24.6 Don’t forget the outliers!",
    "text": "24.6 Don’t forget the outliers!\nBut let’s not loose site of our question which is “Did the cleanup at the contaminated site reduce the concentration of TCB down to background (reference) levels?” It’s obvious once we look at the data that for a few sites, more remediation is needed–particularly for the sites with the three highest concentrations of 6.61, 18.4 and 168.64 ppb and possibly the sites whose concentrations are 5.56 and 6.61. No statistical procedure is needed to come to this conclusion!"
  },
  {
    "objectID": "robustness.html#references",
    "href": "robustness.html#references",
    "title": "24  Robustness",
    "section": "24.7 References",
    "text": "24.7 References\nMillard S.P, Neerchal N.K., Environmental Statistics with S-Plus, 2001."
  },
  {
    "objectID": "bivariate.html",
    "href": "bivariate.html",
    "title": "25  Fits and residuals",
    "section": "",
    "text": "Bivariate data are datasets that store two variables measured from a same observation (e.g. wind speed and temperature at a single location). This differs from univariate data where only one variable is measured for each observation (e.g. temperature at a single location)."
  },
  {
    "objectID": "bivariate.html#scatter-plot",
    "href": "bivariate.html#scatter-plot",
    "title": "25  Fits and residuals",
    "section": "25.1 Scatter plot",
    "text": "25.1 Scatter plot\nA scatter plot is a popular visualization tool used to compare values between two variables. Sometimes one variable is deemed dependent on another variable; the latter being the independent variable. Cleveland refers to the former as the response and the latter as the factor (this is not to be confused with the factor data type used in R as a grouping variable). The dependent variable is usually plotted on the y-axis and the independent variable is usually plotted on the x-axis. Other times, one does not seek a dependent-independent relationship between variables but is simply interested in studying the relationship between them.\nA scatter plot can be generated using the base plotting environment as follows:\n\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/ganglion.csv\")\n\nplot(cp.ratio ~ area, dat = df)\n\n\n\n\nOr, using ggplot2, as follows:\n\nlibrary(ggplot2)\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point()\n\n\n\n\nThe data represent the ratio between the ganglion cell density of a cat’s central retina to that of its peripheral density (variable cp.ratio) and the cat’s retina surface area (area) during its early development (ranging from 35 to 62 days of gestation)."
  },
  {
    "objectID": "bivariate.html#fitting-the-data",
    "href": "bivariate.html#fitting-the-data",
    "title": "25  Fits and residuals",
    "section": "25.2 Fitting the data",
    "text": "25.2 Fitting the data\nScatter plots are a good first start in visualizing the data, but this is sometimes not enough. Our eyes need “guidance” to help perceive patterns. Another visual aid involves fitting the data with a line.\n\n25.2.1 Parametric fit\n\n25.2.1.1 Fitting a straight line\nA straight line is the simplest fit one can make to bivariate data. A popular method for fitting a straight line is the least-squares method. We’ll use R’s lm() function which provides us with a slope and intercept for the best fit line.\nIn the base plotting environment, we can do the following:\n\nM <- lm(cp.ratio ~ area, dat = df)\nplot(cp.ratio ~ area, dat = df)\nabline(M, col = \"red\")\n\n\n\n\nIn the ggplot2 plotting environment, we can make use of the stat_smooth function to generate the regression line.\n\nlibrary(ggplot2)\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n             stat_smooth(method =\"lm\", se = FALSE)\n\n\n\n\nThe se = FALSE option prevents R from drawing a confidence envelope around the regression line.\nThe straight line is a first order polynomial with two parameters, \\(a\\) and \\(b\\), that define an equation that best describes the relationship between the two variables:\n\\[\nCP_{Ratio} = a + b (Area)\n\\]\nwhere \\(a\\) and \\(b\\) can be extracted from the regression model object M as follows:\n\ncoef(M)\n\n(Intercept)        area \n 0.01399056  0.10733436 \n\n\nThus \\(a\\) = 0.014 and \\(b\\) = 0.11.\n\n\n25.2.1.2 Fitting a 2nd order polynomial\nA second order polynomial is a three parameter function (\\(a\\), \\(b\\) and \\(c\\)) whose equation \\(y = a + bx + cx^2\\) defines the curve that best fits the data. We define such a relationship in R using the formula cp.ratio ~ area + I(area^2). The identity function I() preserves the arithmetic interpretation of area^2 as part of the model. Our new lm expression and resulting coefficients follow:\n\nM2 <- lm(cp.ratio ~  area + I(area^2) , dat = df)\ncoef(M2)\n\n  (Intercept)          area     I(area^2) \n 2.8684792029 -0.0118691702  0.0008393243 \n\n\nThe quadratic fit is thus,\n\\[\ny = 2.87 - 0.012 x + 0.000839 x^2\n\\]\nWe cannot use abline to plot the predicted 2nd order polynomial curve since abline only draws straight lines. We will need to construct the line manually using the predict and lines functions.\n\nplot(cp.ratio ~ area, dat=df)\nx.pred <- data.frame( area = seq(min(df$area), max(df$area), length.out = 50) )\ny.pred <- predict(M2, x.pred)\nlines(x.pred$area, y.pred, col = \"red\")\n\n\n\n\nTo do this in ggplot2 simply pass the formula as an argument to stat_smooth:\n\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n  stat_smooth(method = \"lm\", se = FALSE, formula = y ~  x + I(x^2) )\n\n\n\n\n\n\n\n25.2.2 Non-parametric fits\n\n25.2.2.1 Loess\nA more flexible curve fitting option is the loess curve (short for local regression; also known as the local weighted regression). Unlike the parametric approach to fitting a curve, the loess does not impose a structure on the data. The loess curve fits small segments of a regression lines across the range of x-values, then links the mid-points of these regression lines to generate the smooth curve. The range of x-values that contribute to each localized regression lines is defined by the \\(\\alpha\\) parameter which usually ranges from 0.2 to 1. The larger the \\(\\alpha\\) value, the smoother the curve. The other parameter that defines a loess curve is \\(\\lambda\\): it defines the polynomial order of the localized regression line. This is usually set to 1 (though ggplot2’s implementation of the loess defaults to a 2nd order polynomial).\n\n\n\n\n\n25.2.2.2 How a loess is constructed\nBehind the scenes, each point (xi,yi) that defines the loess curve is constructed as follows:\n\nA subset of data points closest to point xi are identified. The number of points in the subset is computed by multiplying the bandwidth \\(\\alpha\\) by the total number of observations. In our current example, the number of points defining the subset is 0.5 * 14 = 7. The points are identified in the light blue area of the plot in panel (a) of the figure below.\nThe points in the subset are assigned weights. Greater weight is assigned to points closest to xi and vice versa. The weights define the points’ influence on the fitted line. Different weighting techniques can be implemented in a loess with the gaussian weight being the most common. Another weighting strategy we will also explore later in this course is the symmetric weight.\nA regression line is fit to the subset of points. Points with smaller weights will have less leverage on the fitted line than points with larger weights. The fitted line can be either a first order polynomial fit or a second order polynomial fit.\nCompute the value yi from the regression line. This is shown a the red dot in panel (d). This is one of the points that will define the shape of the loess.\n\n\n\n\n\n\nThe above steps are repeated for as many xi values practically possible. Note that when xi approaches an x limit, the subset of points becomes skewed to one side of xi. For example, when estimating x10, the seven closest points to the right of x10 are selected.\n\n\n\n\n\nIn the following example, just under 30 loess points are computed at equal intervals. This defines the shape of the loess.\n\n\n\n\n\nIt’s more conventional to plot the line segments than it is to plot the points.\n\n\n\n\n\n\n\n\n25.2.2.3 Plotting a loess in R\nThe loess fit can be computed using the loess() function. It takes as arguments span (\\(\\alpha\\)), and degree (\\(\\lambda\\)).\n\n# Fit loess function\nlo <- loess(cp.ratio ~ area, df, span = 0.5, degree = 1)\n\n# Predict loess values for a range of x-values\nlo.x <- seq(min(df$area), max(df$area), length.out = 50)\nlo.y <- predict(lo, lo.x)\n\nThe predicted loess curve can be added using the lines function.\n\nplot(cp.ratio ~ area, dat = df)\nlines(lo.x, lo.y, col = \"red\")\n\n\n\n\nIn ggplot2 simply pass the method=\"loess\" parameter to the stat_smooth function.\n\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n             stat_smooth(method = \"loess\", se = FALSE, span = 0.5)\n\nHowever, ggplot (up to version 3.3) defaults to a second degree loess (i.e. the small regression line elements that define the loess are defined by a 2nd order polynomial and not a 1st order polynomial). If a first order polynomial (degree=1) is desired, you need to include an argument list in the form of method.args=list(degree=1) to the stat_smooth function.\n\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n             stat_smooth(method = \"loess\", se = FALSE, span = 0.5, \n                         method.args = list(degree = 1) )"
  },
  {
    "objectID": "bivariate.html#residuals",
    "href": "bivariate.html#residuals",
    "title": "25  Fits and residuals",
    "section": "25.3 Residuals",
    "text": "25.3 Residuals\nFitting the data with a line is just the first step in EDA. Your next step should be to explore the residuals. The residuals are the distances (along the y-axis) between the observed points and the fitted line. The closer the points to the line (i.e. the smaller the residuals) the better the fit.\nThe residuals can be computed using the residuals() function. It takes as argument the model object. For example, to extract the residuals from the linear model M computed earlier type,\n\nresiduals(M)\n\n         1          2          3          4          5          6          7          8          9         10 \n 1.3596154  1.3146525  0.5267426 -0.1133131 -0.1421694  0.3998247 -1.2188692 -2.0509148 -1.8622085 -0.1704418 \n        11         12         13         14 \n-0.5788861  0.4484394 -1.0727624  3.1602905 \n\n\n\n25.3.1 Residual-dependence plot\nWe’ll create a residual dependence plot to plot the residuals as a function of the x-values. We’ll do this using ggplot so that we can also fit a loess curve to help discern any pattern in the residuals (the ggplot function makes it easier to add a loess fit than the traditional plotting environment).\n\ndf$residuals <- residuals(M)\nggplot(df, aes(x = area, y = residuals)) + geom_point() +\n             stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                         method.args = list(degree = 1) )\n\n\n\n\nWe are interested in identifying any pattern in the residuals. If the model does a good job in fitting the data, the points should be uniformly distributed across the plot and the loess fit should approximate a horizontal line. With the linear model M, we observe a convex pattern in the residuals suggesting that the linear model is not a good fit. We say that the residuals show dependence on the x values.\nNext, we’ll look at the residuals from the second order polynomial model M2.\n\ndf$residuals2 <- residuals(M2)\nggplot(df, aes(x = area, y = residuals2)) + geom_point() +\n               stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                           method.args = list(degree = 1) )\n\n\n\n\nThere is no indication of dependency between the residual and the area values. The second order polynomial is an improvement over the first order polynomial. Let’s look at the loess model.\n\ndf$residuals3 <- residuals(lo)\nggplot(df, aes(x = area, y = residuals3)) + geom_point() +\n               stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                           method.args = list(degree = 1) )\n\n\n\n\nThe loess model also seems to do a good job in smoothing out any overall pattern in the data.\nYou may ask “if the loess model does such a good job in fitting the data, why bother with polynomial fits?” If you are seeking to generate a predictive model that explains the relationship between the y and x variables, then a mathematically tractable model (like a polynomial model) should be sought. If the interest is simply in identifying a pattern in the data, then a loess fit is a good choice.\nNext we will look for homogeneity in the residuals.\n\n\n25.3.2 Spread-location plot\nThe M2 and lo models do a good job in eliminating any dependence between residual and x-value. Next, we will check that the residuals do not show a dependence with fitted y-values. This is analogous to univariate analysis where we checked if residuals increased or decreased with increasing medians across factors. Here we will compare residuals to the fitted cp.ratio values (think of the fitted line as representing a level across different segments along the x-axis). We’ll generate a spread-level plot of model M2’s residuals (note that in the realm of regression analysis, such plot is often referred to as a scale-location plot). We’ll also add a loess curve to help visualize any patterns in the plot (this reproduces fig 3.14 in Cleveland’s book).\n\nsl2 <- data.frame( std.res = sqrt(abs(residuals(M2))), \n                   fit     = predict(M2))\n\nggplot(sl2, aes(x = fit, y  =std.res)) + geom_point() +\n              stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                          method.args = list(degree = 1) )\n\n\n\n\nThe function predict() extracts the y-values from the fitted model M2 and is plotted along the x-axis. It’s clear from this plot that the residuals are not homogeneous; they increase as a function of increasing fitted CP ratio. The “bend” observed in the loess curve is most likely due to a single point at the far (right) end of the fitted range. Given that we have a small batch of numbers, a loess can be easily influenced by an outlier. We may want to increase the span.\n\nggplot(sl2, aes(x = fit, y = std.res)) + geom_point() +\n              stat_smooth(method = \"loess\", se = FALSE, span = 1.5, \n                          method.args = list(degree = 1) )\n\n\n\n\nThe point’s influence is reduced enough to convince us that the observed monotonic increase is real (Note that we would observe this monotone spread with our loess model as well). At this point, we should look into re-expressing the data.\n\n\n25.3.3 Checking residuals for normality\nIf you are interested in conducting a hypothesis test (i.e. addressing the question “is the slope significantly different from 0”) you will likely want to check the residuals for normality since this is an assumption made when computing a confidence interval and a p-value. We’ll make use of geom_qq and geom_qq_line to compare the residuals to a normal distribution.\n\nggplot(df, aes(sample = residuals2)) + \n  geom_qq(distribution = qnorm) +\n  geom_qq_line(distribution = qnorm, col = \"blue\")\n\n\n\n\nHere, the residuals seem to stray a little from a normal distribution."
  },
  {
    "objectID": "bivariate.html#re-expressing-the-data",
    "href": "bivariate.html#re-expressing-the-data",
    "title": "25  Fits and residuals",
    "section": "25.4 Re-expressing the data",
    "text": "25.4 Re-expressing the data\nThe monotone spread can be problematic if we are to characterize the spread of cp.ratio as being the same across all values of area. To remedy this, we can re-express the cp.ratio values. Ratios are good candidates for log transformation. We will therefore fit a new linear model to the data after transforming the y-value.\n\ndf.log <- data.frame( area = df$area, cp.ratio.log = log(df$cp.ratio))\nM3     <- lm(cp.ratio.log ~ area, dat = df.log)\n\nNext, let’s plot the transformed data and add the fitted line.\n\nggplot(df.log, aes(x = area, y = cp.ratio.log)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nAt first glance, the log transformation seems to have done a good job at straightening the batch of values. Next, let’s look at the residual dependence plot.\n\ndf.log$residuals <- residuals(M3)\nggplot(df.log, aes(x = area, y = residuals)) + geom_point() +\n               stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                           method.args = list(degree = 1) )\n\n\n\n\nLogging the y values has eliminated the residual’s dependence on area. Next, let’s assess homogeneity in the residuals using the s-l plot.\n\nsl3 <- data.frame( std.res = sqrt(abs(residuals(M3))), \n                   fit     = predict(M3))\nggplot(sl3, aes(x = fit, y = std.res)) + geom_point() +\n               stat_smooth(method =\"loess\", se = FALSE, span = 1, \n                           method.args=list(degree = 1) )\n\n\n\n\nWe do not observe a systematic increase in spread, the log transformation seems to have removed the monotone spread.\nFinally, we’ll check for normality of the residuals.\n\nggplot(df.log, aes(sample = residuals)) + \n  geom_qq(distribution = qnorm) +\n  geom_qq_line(distribution = qnorm, col = \"blue\")\n\n\n\n\nAn added benefit of re-expressing the data seems to be a slight improvement in the normality of the residuals."
  },
  {
    "objectID": "resistant_lines.html#introduction",
    "href": "resistant_lines.html#introduction",
    "title": "26  Resistant lines",
    "section": "26.1 Introduction",
    "text": "26.1 Introduction\nOrdinary least squares regression lines (those created with the lm() function) suffer from sensitivity to outliers. Because lm’s best fit line makes use of the mean (which is not a robust measure of location), its breakdown point is \\(1/n\\) meaning that all it takes is for one data point to behave differently from the rest of the points to significantly alter the slope and intercept of the best fit line. For example, let’s start with a well behaved dataset where all points are perfectly aligned and fit this batch with a regression line:\n\nx <- seq(1:11)\ny <- 5 + 2 * x\nplot(y ~ x, pch = 20)\nM <- lm(y ~ x)\nabline( M , col = \"red\")\n\n\n\n\nAs expected, we have a perfect fit. And the regression model’s coefficients match those used to create the data.\n\ncoef(M)\n\n(Intercept)           x \n          5           2 \n\n\nNow, what if one of the points is re-positioned in the plot, what happens to the regression line?\n\ny[11] <- 2\nplot(y ~ x, pch = 20)\nM <- lm(y ~ x)\nabline( M , col = \"red\")\n\n\n\n\nNote the significant change in the line’s characteristics, its intercept and slope are now:\n\ncoef(M)\n\n(Intercept)           x \n  9.5454545   0.8636364 \n\n\nThe slope dropped from 2 to 0.86 because of a single point!\nIf our goal is to explore what the bulk of the data has to say about the phenomena being investigated, we certainly don’t want a small batch of “maverick” values to hijack the analysis. We therefore need a set of fitting tools that minimize the influence of outliers. There are many options out there; most notable are Tukey’s 3-point summary line and the bisquare robust estimation method outlined in Cleveland’s text."
  },
  {
    "objectID": "resistant_lines.html#robust-lines",
    "href": "resistant_lines.html#robust-lines",
    "title": "26  Resistant lines",
    "section": "26.2 Robust lines",
    "text": "26.2 Robust lines\n  \n\n26.2.1 Tukey’s 3-point summary\nThe idea is simple in concept and easy to implement with a pen and paper if the dataset is not too big. It involves dividing the dataset into three approximately equal groups of values and summarizing these groups by computing their respective medians. Two half-slopes are then used to join the three points. Note that points that share the same x value are lumped into the same batch which can lead to unequal group sizes. The motivation behind this plot is to use the three-point summary to provide a robust assessment of the type of relationship between both variables. Such plots are often used to help guide re-expression of the variables x or y or both for the sole purpose of straightening the x-y relationship.\nLet’s look at an example using the last (modified) dataset. First, we’ll divide the plot into three approximately equal batches.\n\n\n\n\n\n\n\n\nNext, compute the median x and y values within each section.\n\n\n\n\n\nNote that we do not include the same point in any two median calculations. This implies that for the mid-third of the data, we do not include the point straddling the left boundary line when computing its median value. Likewise with the right-third of the data.\nThe x median values are 2.5, 6, 9.5 and the y median values are 10, 17, 22.\nFinally, we fit the tail end medians with a straight line.\n\n\n\n\n\nThis gives us the slope of the line (we can, of course, extend the lines to the boundaries of the x range of values). Note that we have not completely eliminated the influence of the outlier. The outlier does take part in helping fit the line, but it doesn’t wield the same disproportionate influence on the line as did the regression line.\nSo what purpose does the middle median serve? It helps us adjust the vertical location of the line. The goal is to nudge the line upward or downward about 1/3 of the way towards the middle median value (parallel to the y-axis). This then defines the line’s intercept (which is where the line crosses the y-axis when x = 0).\n\n\n\n\n\nBut you need not stop here. The next logical step is to look at the residuals. The following two plots compare the residuals from the regression model and the 3-point summary plot.\n\n\n\n\n\nRecall that we are seeking to minimize any non-random pattern in the residual. Even though a monotonic increase in residuals is discernible in both plots, the 3-point summary residual has less of a slope and (ignoring the outlier) has a smaller spread.\nThe 3-point summary fitting process can be extended by adding the residual slope to the original slope. This process is iterated as many times as needed until the residual slope is close to zero.\n\n\n26.2.2 Bisquare\nIn his book (pages 112 - 119), Cleveland uses the bisquare estimation method to come up with a robust line. The first step is to run a linear regression model on the data then to extract the residuals. Next, a weighting scheme is fit to the residuals such that the points associated with outlier residuals are assigned the smallest weight and the points associated with the central residual values are assigned the largest weight. The regression analysis is then re-run using those same weights thus minimizing the influence of the rogue points. This process is repeated several times until the residuals no longer show outliers. The following figure shows three iterations of the bisquare function whereby the weights (shown as grey text next to each point) start off as 1 then are modified following the residuals derived from the most recent regression model.\n\n\n\n\n\n\n# Create the bisquare function\nwt.bisquare <- function(u, c = 6) {\n   ifelse( abs(u/c) < 1, (1-(u/c)^2)^2, 0)\n}\n\n# Assign an equal weight to all points\nwt <- rep(1, length(x))\n\n# Compute the regression, then assign weights based on residual values\nfor(i in 1:10){\n  dat.lm <- lm(y ~ x ,weights=wt)\n  wt <- wt.bisquare( dat.lm$res/ median(abs(dat.lm$res)), c = 6 )\n}\n\n# Plot the data and the resulting line\nplot(x, y, pch = 20)\nabline(dat.lm, col = rgb(1,0,0,0.3))\n\n\n\n\nIn the above example, the bisquare method does a great job in eliminating the outlier’s influence.\n\n26.2.2.1 Sample data\nIn this example, we fit both an un-modified regression line (dashed grey line) and a bisquare modified regression line (red line) to yearly mean temperature values for the Gulf of Maine.\n\nlibrary(lubridate)\nlibrary(dplyr)\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/GoM_hist.csv\")\n\n# Subset the data then compute the yearly mean temperature\ndf2 <- df %>%\n       mutate(Date = mdy(Date),\n              Year = year(Date)) %>%\n       filter(Year > 1950) %>%\n       group_by( Year) %>%\n       summarize( AvgTemp = mean(Temp, na.rm=T)) \n\n# Generate the base regression model\nM1 <- lm( AvgTemp ~ Year, dat=df2)\n\n# Run the bisquare regression model \n# We'll use the wt.bisquare() function from the last chunk of code\nwt <- rep(1,length(df2$Year))\n\nfor(i in 1:10){\n  M2 <- lm(AvgTemp ~ Year,df2 , weights=wt)\n  wt <- wt.bisquare( M2$res/ median(abs(M2$res)), c=6 )\n}\n\n# Plot the points\nplot(AvgTemp ~ Year,df2, pch=16, col=rgb(0,0,0,0.2))\n\n# Add the robust line\nabline( M2, col=\"red\")\n\n# Add the default regression line\nabline( M1 , col=\"grey50\", lty=2)\n\n\n\n\nNote the difference in slopes: The robust regression method computes a slope of 0.0298 while the original regression model computes a slope of 0.0197.\n\n\n26.2.2.2 Built-in implementation of the bisquare\nThe MASS package has a robust linear modeling function called rlm that will implement a variation of the aforementioned bisquare estimation technique. Its results may differ slightly from those presented here, but the difference will be insignificant for the most part.\nNote that if you make use of dplyr in a work flow, loading MASS after dplyr will mask dplyr’s select function. This can be problematic. So you either want to load MASS before dplyr, or you can call the function via MASS::rlm. An example of its use follows.\n\nM2.r <- MASS::rlm( AvgTemp ~ Year, dat=df2, psi=\"psi.bisquare\")\nplot(AvgTemp ~ Year,df2, pch=16, col=rgb(0,0,0,0.2))\n\n# Add the robust line\nabline( M2.r, col=\"red\")\n\n# Add the default regression line\nabline( M1 , col=\"grey50\", lty=2)\n\n\n\n\nThe function rlm can also be called directly from within ggplot.\n\nlibrary(ggplot2)\nggplot(df2, aes(x=Year, y=AvgTemp)) + geom_point() + \n       stat_smooth(method = \"lm\", se = FALSE, col = \"grey50\", lty = 2) +\n       stat_smooth(method = MASS::rlm, se = FALSE, col = \"red\",\n                   method.args = list(psi = \"psi.bisquare\"))"
  },
  {
    "objectID": "resistant_lines.html#robust-loess",
    "href": "resistant_lines.html#robust-loess",
    "title": "26  Resistant lines",
    "section": "26.3 Robust loess",
    "text": "26.3 Robust loess\nThe bisquare estimation method can also be extended to the loess smoothing function. The following chunk of code fits both an uncorrected robust loess (dashed curve) and a bisquare loess (red curve) to the Gulf of Maine temperature data.\n\n# Fit a regular loess model\nlo <- loess(AvgTemp ~ Year, df2, span = 1/3)\n\n# Fit a robust loess model\nwt <- rep(1,length(df2$Year))\n\nfor(i in 1:10){\n  lo2 <- loess(AvgTemp ~ Year,df2, weights = wt, span = 1/3)\n   wt <- wt.bisquare( lo2$res/ median(abs(lo2$res)), c = 6 )\n}\n\n# Plot the data\nplot(AvgTemp ~ Year, df2, pch = 16, col = rgb(0,0,0,0.2))\n\n# Add the robust loess\nlines(df2$Year, predict(lo2), col = \"red\")\n\n# Add the default loess\nlines(df2$Year, predict(lo), col = \"grey50\", lty = 2)\n\n\n\n\nThe two curves overlap for most of the data range except between the years 2000 and 2010 where the un-corrected loess curve is unduly influenced by one or two rogue points incorrectly suggesting that a hump in water temperature occurred between the years 2000 and 2010.\nThe R base loess function and ggplot2’s stat_smooth function have the ability to compute a robust version of the loess by invoking the family = \"symmetric\" parameter as in,\n\nlibrary(ggplot2)\nggplot(df2) + aes(x=Year, y=AvgTemp) + geom_point() + \n              stat_smooth(method = \"loess\", span=1/3,\n                          method.args = list(family=\"symmetric\"), # Robust loess\n                          se=FALSE,  col=\"red\") +  \n              stat_smooth(method = \"loess\", span=1/3, # Regular loess \n                          se=FALSE, col=\"grey50\", lty=2)            \n\n\n\n\nThe red curve is that of the robust loess and the grey dashed curve is that of the standard loess."
  },
  {
    "objectID": "example.html#an-eda-example-co2-analysis",
    "href": "example.html#an-eda-example-co2-analysis",
    "title": "27  The third R of EDA: Residuals",
    "section": "27.1 An EDA example: CO2 analysis",
    "text": "27.1 An EDA example: CO2 analysis\nLet’s start off by plotting the atmospheric CO2 concentrations (in ppm). The original dataset was pulled from NOAA’s website.\n\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndat <- read.csv(\"http://mgimond.github.io/ES218/Data/co2_2020.csv\")\n\ndat2 <- dat %>%\n      mutate( Date = ymd(paste(Year,\"/\", Month,\"/15\")),\n              Year = decimal_date(Date),\n              CO2 = Average)    %>%\n     select(Year, Date, CO2)\n\nNext, let’s look at the data:\n\nggplot(dat2, aes(x = Year , y = CO2)) + geom_point(size = 0.6)\n\n\n\n\nLet’s see if plotting the data using lines instead of points helps identify any underlying patterns.\n\nggplot(dat2, aes(x = Year , y = CO2)) + geom_line()\n\n\n\n\nWe note two patterns of interest: an overall upward trend, and a cyclical trend. We will first smooth out the overall trend by finding a model that best explains the overall variability, then we’ll remove that trend from the data to explore the cyclical component."
  },
  {
    "objectID": "example.html#exploring-the-overall-trend",
    "href": "example.html#exploring-the-overall-trend",
    "title": "27  The third R of EDA: Residuals",
    "section": "27.2 Exploring the overall trend",
    "text": "27.2 Exploring the overall trend\nWe can attempt to model the overall trend by fitting a straight line to the data using a standard regression analysis procedure. The fitted line is displayed in red in the following plot. We will use ggplot2’s built-in smoothing function to plot the regression line.\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n             stat_smooth(method = \"lm\", se = FALSE, col = \"red\") \n\n\n\n\nNext, we subtract the modeled line from the CO2 data and plot the result–this gives us the residuals and can be thought of as representing what the linear model does not explain about the data. But first, we will need to create a regression model object since it will provide us with the residuals needed to generate the residual plot.\n\n# Generate a model object from the regression fit\nM1 <- lm(CO2 ~ Year, dat2) \ndat2$res1 <- M1$residuals\n\n# Plot the residuals from the model object\nggplot(dat2, aes(x = Year, y = res1)) + geom_point()\n\n\n\n\nAn overall trend is still present, despite having attempted to control for it. This suggests that our simple linear model does not do a good job in capturing the shape of the overall trend.\nIt appears that the overall trend is slightly convex and has a small peak around the 1990’s. We can first try to fit the trend using a 2nd order polynomial of the form:\n\\[\nCO2_{trend} = a + b(Year) + c(Year^2)\n\\]\nThe fitted line looks like this:\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n           stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), \n                       se = FALSE, col = \"red\")\n\n\n\n\nNow, let’s look at the residuals. In helping discern any pattern in our residuals, we add a loess smooth to the plot.\n\nM2 <- lm(CO2 ~ Year + I(Year^2), dat2)\ndat2$res2 <- M2$residuals\n\nggplot(dat2 , aes(x = Year, y = res2)) + geom_point() +\n           stat_smooth(method = \"loess\", se = FALSE)\n\n\n\n\nThis is an improvement over the simple line model (which was a 1st order polynomial fit). So what we have learned so far is that the overall trend is not perfectly linear but instead follows a parabolic like trajectory with a small hump halfway across the time span. However, we can still make out a “W” shaped trend in the residuals which can hamper our analysis of the oscillatory patterns in the data. We can try a 3rd order polynomial and see if it can capture the 1980 to 1995 hump.\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n           stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3), \n                       se = FALSE, col = \"red\")\n\n\n\n\nNow, let’s look at the residuals.\n\nM2 <- lm(CO2 ~ Year + I(Year^2) + I(Year^3), dat2)\ndat2$res2 <- M2$residuals\n\nggplot(dat2 , aes(x = Year, y = res2)) + geom_point() +\n           stat_smooth(method = \"loess\", se = FALSE)\n\n\n\n\nThis does not seem to be an improvement over the 2nd order polynomial. At this point, we could play with different order polynomials in an attempt to smooth the trend or, we may opt for a non-parametric fit. When the goal is to peel off one pattern to explore any underlying pattern we should not limit ourselves to parametric fits (which impose a mathematical model on our data) and instead explore non-parametric smoothing techniques that do not impose any structure on the data whatsoever. An example of such a smoothing technique is the loess fit which is shown in the following figure.\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n             stat_smooth(method = \"loess\", span = 1/4, se = FALSE, col = \"red\")\n\n\n\n\nAt first glance, this may not look any different from our 2nd or 3rd order polynomial model. But the resulting residuals suggest that the loess smooth did a better job in removing any decadal patterns in our batch of CO2 values.\n\nlo <- loess(CO2 ~ Year, dat2, span=1/4)\ndat2$res3 <- lo$residuals\n\nggplot(dat2,  aes(x = Year, y = res3)) + geom_point() +\n           stat_smooth(method = \"loess\", se = FALSE, span = 0.2)"
  },
  {
    "objectID": "example.html#exploring-the-seasonal-component",
    "href": "example.html#exploring-the-seasonal-component",
    "title": "27  The third R of EDA: Residuals",
    "section": "27.3 Exploring the seasonal component",
    "text": "27.3 Exploring the seasonal component\nLet’s now explore the cyclical pattern in the residuals. Note the y-axis values: they are three orders of magnitude less than the overall range of CO2 values. This indicates that the oscillating nature along the overall trend is relatively small (the CO2 values have a range of [313.33, 417.31] ppm whereas the residuals have a range of [-4.34, 4.34] ppm).\nNow, we may be tempted to fit a smooth (as Tukey would say) to the residuals but that may not prove to be fruitful. Instead let’s see if the oscillation follows a 12 month cycle. We’ll group all the residual values by month of the year. In other words, we will remove the year tag associated with each value and explore those values as a function of month alone. Each month’s batch of values is distilled into a boxplot.\n\n# Aggregate residuals by year\ndat2$Month <- month(dat2$Date, label=TRUE) \n\nggplot(dat2, aes(x = Month, y = res3)) + geom_boxplot()         \n\n\n\n\nIt’s clear from the plot that the oscillation follows a yearly cycle: a peak in the spring and a dip in the fall. This cycle is explained in part by the increased land mass in the northern hemisphere relative to the southern hemisphere. Because plants (and by extension photosynthesis) go dormant during the winter months in the northern hemisphere, atmospheric CO2 is no longer being photosynthesized; this despite the southern hemisphere’s photosynthesis peak during the October-March period (a result of the southern hemisphere’s smaller land mass). Other factors such as increased CO2 emissions during the winter months may also contribute to the oscillatory nature of atmospheric CO2 concentrations.\nThus far, we have uncovered three patterns of interest: an overall trend, a “hump” around the 1980-1995 time period, and a cyclical component. Note that to effectively explore the cyclical pattern we had to de-trend (or smooth) the data. Next, we should smooth the seasonal component of the data to see if another pattern emerges. We may, for example, smooth the data by subtracting the monthly median from each residual batch leaving us with the next batch of residual values to explore:\n\nd3 <- dat2 %>% \n  group_by(month(Date)) %>%\n  mutate(Resid.lev = res3 - median(res3)) \n\nggplot(d3, aes(x = Month, y = Resid.lev)) + geom_boxplot()     \n\n\n\n\nAll the boxplots are lined up along their median values. We are now exploring the data after having accounted for both overall trend and seasonality. What can be gleaned from this dataset? We may want to explore the skewed nature of the residuals in March, or the narrower range of CO2 values for the fall months, for example.\nWe may also be interested in seeing if any trend in CO2 concentrations exists within each month. Let’s explore this by fitting a first order polynomial to the monthly data.\n\nggplot(d3, aes(x = Year, y = Resid.lev)) + geom_point(col = \"grey\", cex = .6)  + \n     stat_smooth(se = FALSE, method = \"lm\") + facet_wrap(~ Month, nrow = 1) +\n     theme(axis.text.x=element_text(angle=45, hjust = 1.5, vjust = 1.6,  size = 7))\n\n\n\n\nNote the overall increase in CO2 for the winter and spring months followed by a similar (but not as persistent) decrease in CO2 loss in the summer and fall months. This suggests either an increase in cyclical amplitude with the deviation from a central value being possibly greater for the winter/spring months. It could also suggest a gradual offset in the periodicity over the 60 year period. More specifically, a leftward shift in periodicity.\nLet’s fit a loess to see if the trends within each month are monotonic. We’ll adopt a robust loess (family = \"symmetric\") to control for possible outliers in the data.\n\nggplot(d3, aes(x = Year, y = Resid.lev)) + geom_point(col = \"grey\", cex = .6)  + \n     stat_smooth(method = \"loess\", se = FALSE, \n                 method.args = list(family = \"symmetric\")) + \n     facet_wrap(~ Month, nrow = 1) +\n     theme(axis.text.x=element_text(angle=45, hjust = 1.5, vjust = 1.6,  size = 7))\n\n\n\n\nThe trends are clearly not monotonic suggesting that the processes driving the cyclical concentrations of CO2 are not systematic. However, despite the curved nature of the fits, the overall trends observed with the straight lines still remain."
  },
  {
    "objectID": "example.html#exploring-what-remains",
    "href": "example.html#exploring-what-remains",
    "title": "27  The third R of EDA: Residuals",
    "section": "27.4 Exploring what remains",
    "text": "27.4 Exploring what remains\nNow that we’ve fitted the monthly medians to the seasonal component of the data, let’s plot the residuals across the years. We’ll make use of the robust version of the loess given that the data may be quite noisy.\n\nggplot(d3, aes(x = Year, y = Resid.lev)) +  geom_line(col = \"grey\") + \n     stat_smooth(method = \"loess\", se = FALSE, span = 0.1 ,\n                 method.args = list(family = \"symmetric\", degree = 2))    \n\n\n\n\nRecall that at this points, we have accounted for the overall trend (which has by far the greatest effect with a range of over 90 ppm) and the seasonal component (which has a smaller effect with a range of about 6 ppm). The 3rd set of residuals account for about 2 ppm of the total variability in the dataset. Note that we can still make out the seasonal fluctuations if we look closely at the data. But we can also make out broader patterns of interest not consistent with random noise and not explained by seasonal variability. Some of the patterns could be explained by the southern oscillation index (SOI), for example, which is a measure of the difference in atmospheric pressure across the pacific basin. When the index is positive the resulting winds drive upwelling currents that release additional CO2 into the atmosphere.\nWe’ll create a simple overlay the SOI data on top of our last plot, but note that a scatter plot of the two variables with a lagging component would be best suited for this analysis.\n\nlibrary(tidyr)\n\nsoi <- read.csv(\"http://mgimond.github.io/ES218/Data/Southern_oscillation.csv\")\n\nsoi <- gather(soi, key = Month, value = Index, -1)\nDate <- ymd(paste(soi$YEAR, soi$Month, 15) )\nsoi$Yr <- decimal_date(Date)\n\nggplot(d3, aes(x = Year, y = Resid.lev)) +  \n             geom_line(col = \"blue\", alpha = 0.2) + \n             stat_smooth(method = \"loess\", se = FALSE, span = 0.1, lwd = 0.5,\n                         method.args = list(family = \"symmetric\", degree = 2)) +\n             geom_line(dat = soi, aes(Yr, Index/2, colour = \"red\"), alpha = 0.3) +\n             stat_smooth(dat = soi, aes(Yr, Index/2, colour = \"red\"), lwd = 0.5,\n                         method = \"loess\", se = FALSE, span = 0.2,\n                         method.args = list(family = \"symmetric\", degree = 2)) +\n             guides(color = FALSE)\n\n\n\n\nThe blue lines represent the residuals (with the bold blue line showing the loess fit) and the red lines represent the SOI trend (with the bold line showing its loess fit). Note that we halved the SOI values to match the range along the y-scale; the intent is to facilitate the comparison of SOI trend to CO2 trend. There appears to be matching maxima (notably around 1975 and 1988) but we also note some opposing trends such as in the period after 2010. When the index drops below 0, “El Nino” events become more prominent and upwelling flows are suppressed. Though this may prevent CO2 trapped in the deeper Pacific oceans from reaching the surface, “El Nino” events can also have a measurable impact on land climate thus influencing the rate of vegetation growth and, by extension, CO2 uptake. El nino is also believed to increase the rate of forest and brush fires around the world which may also contribute to atmospheric CO2 concentrations.\nIt’s important to realize that this is one of many ways in which we could have proceeded with the analysis. For example, we could have started off the analysis by removing the seasonal component from the data, then analyzed the long term trend. This is the approach taken by William Cleveland on page 165 of the course’s text."
  },
  {
    "objectID": "discontinuity.html#introduction",
    "href": "discontinuity.html#introduction",
    "title": "28  Slicing data",
    "section": "28.1 Introduction",
    "text": "28.1 Introduction\nLet’s start off by downloading a dataset.\n\nlibrary(ggplot2)\n\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/Sample1.csv\")\n\nNext, we’ll plot the data and fit a straight line.\n\nggplot(df, aes(x = x,y = y)) + geom_point(alpha = 0.3) +\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nThe line seems to do a decent job in depicting the overall trend but the relationship does not appear perfectly linear. Let’s check the residuals via a residual-dependence plot.\n\nM1    <- lm( y ~ x, df)\ndf$residuals <- residuals(M1)\n\nggplot(df, aes(x = x, y = residuals)) + geom_point(alpha = 0.3) +\n  stat_smooth(method = \"loess\", se = FALSE, span = 0.2, method.args = list(degree = 1))\n\n\n\n\nThere appears to be a dip in residual values between ~95 and ~107 followed by an increase beyond ~107. The kinks in the residuals seem to delineate three perfectly straight line segment suggesting that the raw data may be modeled using three distinct lines (i.e. with differing slopes and intercepts). Note the use of a small loess span in the above code to reveal the kinks in the pattern.\nSometimes, the data may represent outcomes from different processes given different ranges in independent (x) values. Since the residual plot seems to suggest that the kinks occur are x=95 and x=106, we’ll split the data into three groups: less than 95, between 95 and 106 and greater than 106. These groups will be labeled 1, 2, and 3 and will be assigned to a new column called grp. This new column will then be used to facet the scatter plots and associated loess curve. We’ll use of the case_when() function to perform this task.\n\nlibrary(dplyr)\ndf2 <- df %>% mutate(grp = case_when(x < 95 ~ 1,\n                                     x < 106 ~ 2,\n                                     x >= 106 ~ 3))\n\nggplot(df2, aes(x = x,y = y)) + geom_point(size = 0.5, alpha = 0.3) +\n  stat_smooth(method = \"loess\", se = FALSE) + facet_grid(. ~ grp)\n\n\n\n\nThe segmented plots seem to confirm our earlier suspicion that the data followed three separate linear processes. We can extract the slope and intercept for each segment using the following chunk of code:\n\nlibrary(tidyr)\ndf2  %>% \n  group_by(grp) %>% \n  do( M1 = (lm(y ~ x, . ) ) )  %>% \n  mutate(intercept = coef(M1)[1],\n         slope = coef(M1)[2]) %>% \n  select(-M1)\n\n# A tibble: 3 × 3\n# Rowwise: \n    grp intercept slope\n  <dbl>     <dbl> <dbl>\n1     1      1.41  2.09\n2     2    103.    1.04\n3     3    -53.4   2.50"
  },
  {
    "objectID": "discontinuity.html#example",
    "href": "discontinuity.html#example",
    "title": "28  Slicing data",
    "section": "28.2 Example",
    "text": "28.2 Example\n\nDisclaimer: the analysis presented here is only exploratory and does not mirror the complete analysis conducted by Vincent et al. nor the one conducted by Stone.\n\n\n28.2.1 Original analysis\nThe following data are pulled from the paper titled “Observed Trends in Indices of Daily Temperature Extremes in South America 1960-2000” (Vincent et al., 2005) and represent the percentage of nights with temperatures greater than or colder than the 90th and 10th percentiles respectively within each year. The percentiles are calculated for the 1961 to 2000 period.\n\nlibrary(tidyr)\n\nYear <- 1960:2000\nPerC <- c(11.69,9.33,14.35,10.73,14.15,11.16,13,12.13,14.25,10.01,11.94,14.35,\n          10.83,9.38,11.5,10.44,12.66,7.55,9.77,9.81,8.9,8.51,7.02,6.83,9.67,\n          7.84,7.11,8.56,10.59,7.93,8.85,8.8,8.75,8.18,7.16,9.91,10.15,6.58,\n          6.44,9.43,8.03)\nPerH <- c(8.62,10.1,6.67,11.13,5.71,9.48,7.63,8.12,7.2,9.64,8.42,5.71,11.72,\n          11.32,7.2,7.17,7.46,13.17,9.28,8.75,12.38,10,13.83,17.59,10.14,\n          9.84,11.23,14.39,9.44,8.26,12.15,12.45,13.14,13.67,15.22,11.79,11.16,\n          20.37,17.56,11.13,11.49)\ndf2 <- data.frame(Year, PerC, PerH)\n\ndf2.l <- gather(df2, key = Temp, value = Percent, -Year)\n\nLet’s plot the data and fit a straight line to the points.\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) + facet_wrap(~ Temp, nrow = 1)\n\n\n\n\nThe plot on the left shows percent cold nights and the one on the right shows percent hot nights. At first glance, the trends seem real and monotonic.\nNext we’ll fit a loess to see if the trends are indeed monotonic. To minimize the undue influence of end values in the plot, we’ll implement loess’ bisquare estimation method via the family=symmetric option. We’ll also use a small span to help identify any “kinks” in the patterns.\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"loess\", se = FALSE, span = 0.5, \n              method.args = list(degree = 1, family = \"symmetric\"))  + \n  facet_wrap(~ Temp, nrow = 1)\n\n\n\n\nThe patterns seem to be segmented around the 1975-1980 period for both plots suggesting that the observed trends may not be monotonic. In fact, there appears to be a prominent kink in the percent cold data around the mid to late 1970`s. A similar, but not as prominent kink can also be observed in the percent hot data at around the same time period.\n\n\n28.2.2 Changepoint\nIn a comment to Vincent et al.’s paper, R.J. Stone argues that the trend observed in the percent hot and cold dataset is not monotonic but segmented instead. In other words, there is an abrupt change in patterns for both datasets that make it seem as though a monotonic trend exists when in fact the data may follow relatively flat patterns for two different segments of time. He notes that the abrupt change (which he refers to as a changepoint) occurs around the 1976 and 1977 period. He suggests that this time period coincides with a change in the Pacific Decadal Oscillation (PDO) pattern. PDO refers to an ocean/atmospheric weather pattern that spans multiple decades and that is believed to impact global climate.\nThe following chunk of code loads the PDO data, then summarizes the data by year before plotting the resulting dataset.\n\ndf3  <- read.table(\"http://mgimond.github.io/ES218/Data/PDO.dat\", \n                   header = TRUE, na.strings = \"-9999\")\npdo <- df3 %>%\n  gather(key = Month, value = PDO, -YEAR) %>%\n  group_by(YEAR) %>%\n  summarise(PDO = median(PDO) )\n\nggplot(pdo, aes(x = YEAR, y = PDO)) + geom_line() + \n  stat_smooth(se = FALSE, span = 0.25) +\n  geom_vline(xintercept = c(1960, 1976, 2000), lty = 3)\n\n\n\n\nThe contrast in PDO indexes between the 1960-1976 period and the 1976-2000 period is obvious with the pre-1977 index values appearing to remain relatively flat over a 15 year period and with the post-1977 index appearing to show a gradual increase towards a peak around the early 1990’s.\nTo see if distinct patterns emerge from the percent hot and cold data before and after 1976, we’ll split the data into two segments using a cutoff year of 1976-1977. Values associated with a period prior to 1977 will be assigned a seg value of Before and those associated with a post-1977 period will be assigned a seg value of After.\n\ndf2.l$seg <- ifelse(Year < 1977, \"Before\", \"After\")\n\nNext, we’ll plot the data across four facets:\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) + facet_grid(seg ~ Temp)\n\n\n\n\nWe can also choose to map seg to the color aesthetics which will split the points by color with the added benefit of fitting two separate models to each batch.\n\nggplot(df2.l, aes(x = Year, y = Percent, col = seg)) + geom_point() +\n  stat_smooth(method = \"lm\", se=FALSE) + facet_wrap( ~ Temp, nrow = 1)\n\n\n\n\nTo test for “straightness” in the fits, we’ll fit a loess to the points.\n\nggplot(df2.l, aes(x = Year, y = Percent, col = seg)) + geom_point() +\n  stat_smooth(method = \"loess\", se = FALSE, method.args = list(degree = 1)) + \n  facet_wrap( ~ Temp, nrow = 1)\n\n\n\n\nThere is a clear “stair-step” pattern for the percent cold nights. However, there seems to be an upward trend in the percent of hot nights for the post-1977 period which could imply that in addition to the PDO effect, another process could be at play.\nIt should be noted that in a followup to Stone’s comment, Vincent et al. defend their analysis results. However, this little exercise highlights the ease in which an analysis can follow different (and seemingly sound) paths."
  },
  {
    "objectID": "discontinuity.html#reference",
    "href": "discontinuity.html#reference",
    "title": "28  Slicing data",
    "section": "28.3 Reference",
    "text": "28.3 Reference\nOriginal paper: Vincent, L. A., et al., 2005. Observed trends in indices of daily temperature extremes in South America 1960–2000. J. Climate, 18, 5011–5023.\nComment to the paper Stone, R. J., 2011. Comments on “Observed trends in indices of daily temperature extremes in South America 1960–2000.” J. Climate, 24, 2880–2883.\nThe reply to the comment Vincent, L. A., et al., 2011. Reply, J. Climate, 24, 2884-2887."
  },
  {
    "objectID": "two_way.html#introduction",
    "href": "two_way.html#introduction",
    "title": "29  Analyzing two-way tables",
    "section": "29.1 Introduction",
    "text": "29.1 Introduction\nBefore tackling two-way tables, let’s explore a simpler one-way table. The following dataset shows mean annual income by occupation for the year 2014 (src: Bureau of Labor Statistics)\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n\n\nArchitecture and Engineering\n\n\n112490\n\n\n\n\nArts, Design, Entertainment, Sports, and Media\n\n\n72410\n\n\n\n\nBuilding and Grounds Cleaning and Maintenance\n\n\n83970\n\n\n\n\nBusiness and Financial Operations\n\n\n81520\n\n\n\n\nCommunity and Social Service\n\n\n70070\n\n\n\n\nComputer and Mathematical\n\n\n45310\n\n\n\n\nConstruction and Extraction\n\n\n101110\n\n\n\n\nEducation, Training, and Library\n\n\n52210\n\n\n\n\nFarming, Fishing, and Forestry\n\n\n55790\n\n\n\n\nFood Preparation and Serving Related\n\n\n76010\n\n\n\n\nHealthcare Practitioners and Technical\n\n\n28820\n\n\n\n\nHealthcare Support\n\n\n43980\n\n\n\n\nInstallation, Maintenance, and Repair\n\n\n21980\n\n\n\n\nLegal\n\n\n26370\n\n\n\n\nLife, Physical, and Social Science\n\n\n24980\n\n\n\n\nManagement\n\n\n38660\n\n\n\n\nOffice and Administrative Support\n\n\n35530\n\n\n\n\nPersonal Care and Service\n\n\n25160\n\n\n\n\nProduction\n\n\n46600\n\n\n\n\nProtective Service\n\n\n45220\n\n\n\n\nSales and Related\n\n\n35490\n\n\n\n\nTransportation and Material Moving\n\n\n34460\n\n\n\n\n\nSo, what can we glean from this dataset other than the raw values associated with each occupation type? For starters, we may be interested in coming up with a summary of income values such as the median, or $45265 in our working example. This is a single value that can be used to characterize the overall income value. However, the income values in our batch are not all equal to $45265–they vary above or below this median value. A good way to measure the variability about the median is to subtract the median income value from each occupation income value. We can do this in a table, but will choose a dotplot chart instead to facilitate comparison.\n\n\n\n\n\nWe have separated the income values into two parts: an overall value and the difference between each value and the overall (which we will refer to as the residuals). This can be written in the following algebraic form:\n\\[\nIncome_{Occupation} = Income_{median} + Income_{residual}\n\\]\nFor example, ‘Healthcare Support’ average income of $43980 can be separated into the sum of the median, $45265, and the residual, $-1285.\nSo in addition to gleaning a median income value from this batch of values, we have also devised a way to facilitate the comparison of the income values to one another. Alternatively, we could have assigned colors to each cell of the table reflecting their values relative to the overall median.\n\n\n\n\n\nThe income value can be thought of as the response to the occupation type (also known as a factor). In other words, the factor determines the value in our table. We can re-write the last equation in a more formal way:\n\\[\ny_{i} = \\mu + \\epsilon_i\n\\]\nwhere \\(y_{i}\\) is the response variable (e.g. income) for factor \\(i\\) (e.g. occupation), \\(\\mu\\) is the overall typical value (hereafter referred to as the common value) and is usually the mean or median, and \\(\\epsilon_i\\) is the residual. Next, we will see how the idea of separating the values into components can be expanded to two-way (aka two-factor) tables."
  },
  {
    "objectID": "two_way.html#anatomy-of-a-two-way-table",
    "href": "two_way.html#anatomy-of-a-two-way-table",
    "title": "29  Analyzing two-way tables",
    "section": "29.2 Anatomy of a two-way table",
    "text": "29.2 Anatomy of a two-way table\nLet’s decompose the following table which represents infant mortality rates (per 1000 live births) by region and by a father’s educational attainment for the years 1964 through 1966 (Hoaglin et al.).\n\nA two-way table is composed of three variables:\n\na row factor which has four levels in our working example: Northeast, North Central, South and West,\na column factor which has five levels in our working example: under 8, 9-11, 12, 13-15 and 16 and greater years of education,\nresponse variables of which we have 4 (rows) x 5 (columns) = 20 all together.\n\nA two-way table is an extension to the one-way table described in the introduction where instead of grouping a continuous variable by a single factor category we explore the variable across two factor categories.\nWe can represent the relationship between the response variable, \\(y\\), and the two factors as:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\n\\]\nwhere \\(y_{ij}\\) is the response variable for row \\(i\\) and column \\(j\\), \\(\\mu\\) is the overall typical value (hereafter referred to as the common value), \\(\\alpha_i\\) is the row effect, \\(\\beta_j\\) is the column effect and \\(\\epsilon_{ij}\\) is the residual or value left over after all effects are taken into account.\nThe goal of this analysis is to decompose the response variable into its respective effects–i.e. the two contributing factors: education of father and region–via an iterative process where row medians and column medians are removed from the response variables repeatedly until the row and column medians approach zero."
  },
  {
    "objectID": "two_way.html#analysis-workflow",
    "href": "two_way.html#analysis-workflow",
    "title": "29  Analyzing two-way tables",
    "section": "29.3 Analysis workflow",
    "text": "29.3 Analysis workflow\nLet’s first create the dataframe.\n\ndf <- data.frame(row.names = c(\"NE\",\"NC\",\"S\",\"W\"),\n                 ed8       = c(25.3,32.1,38.8,25.4), \n                 ed9to11   = c(25.3,29,31,21.1),\n                 ed12      = c(18.2,18.8,19.3,20.3),\n                 ed13to15  = c(18.3,24.3,15.7,24),\n                 ed16      = c(16.3,19,16.8,17.5)\n                 )\n\n\n29.3.1 Visualizing the data\nIt’s often easier to look at a graphical representation of the data than a tabular one. Even a table as small as this can benefit from a plot.\nWe will adopt Bill Cleveland’s dotplot for this purpose. R has a built-in dotplot function called dotchart. It requires that the table be stored as a matrix instead of a dataframe; we will therefore convert df to a matrix by wrapping it with the as.matrix() function.\n\ndotchart( as.matrix(df), cex=0.7)\n\n\n\n\nThe plot helps visualize any differences in mortality rates across different father educational attainment levels. There seems to be a gradual decrease in child mortality with increasing father educational attainment.\nBut the plot does not help spot differences across regions (except for the ed12 group). We can generate another plot where region becomes the main grouping factor. We do this by wrapping the matrix with the transpose function t().\n\ndotchart( t(as.matrix(df)), cex=0.7)\n\n\n\n\nAt first glance, there seems to be higher death rates for the NC and S regions and relatively lower rates for the W and NE regions. But our eyes may be fooled by outliers in the data.\nNext, we’ll generate side-by-side boxplots to compare the effects between both categories. Note that we’ll need to create a long version of the table using the tidyr package.\n\nlibrary(tidyr)\nlibrary(dplyr)\n\ndf.l <- df %>%\n  mutate(Region = as.factor(row.names(.)) )  %>%\n  gather(key=Edu, value = Deaths, -Region) %>% \n  mutate(Edu = factor(Edu, levels=names(df)))\n\n# side-by-side plot\nOP <- par(mfrow=c(1,2))\nplot(Deaths ~ Region + Edu, df.l)\npar(OP)\n\n\n\n\nSo at this point the plots suggest that there may be a father’s educational attainment effect as well as a regional effect on infant mortality with the former effect being possibly more important.\nNext we will attempt to quantify these effects using Tukey’s median polish.\n\n\n29.3.2 Median polish\n\n29.3.2.1 Step 1: Compute overall median and residual table\nFirst, we will compute the median value for all values in the dataset; this will be our first estimate of the common value. The resulting median value is placed in the upper left-hand margin of the table. A residual table is created by taking the difference between the original value and the overall median.\n\nYou’ll also note that the row and column effect values are initially populated with 0 values.\n\n\n29.3.2.2 Step 2: Compute the row medians\nNext, row medians are computed (shown in red in the following figure) from the residual table.\n\nNote that in addition to computing the row median values for the response variable, we are also computing the median value for the column effects in the upper margin (which happens to be zero since we have no column effect values yet).\n\n\n29.3.2.3 Step 3: Create a new residual table from the row medians\nThe row medians are added to the left hand margin of the new residual table (shown in green). These values represent the row effects in this first iteration. A new set of residual values is created from the row medians where each cell takes on the value of the subtraction of the row median from each response variable in that row.\n\nThe subtraction is also done for the column effect values (even though all values remain zero) and the global median (common effect value).\n\n\n29.3.2.4 Step 4: Compute the column medians\nNext, column medians are computed (shown in red in the following figure) from the residual table. Note that you are NOT computing the column medians from the original response variables. Note too that we are also computing the median value for the row effect values (left margin).\n\n\n\n29.3.2.5 Step 5: Create a new residual table from the column medians\nA new residual table is created from the column median where each new cell takes on the value of the subtraction of the column median from each initial residual value in that column. For example, the first upper-left cell’s residual is \\(7.0 - 7.4 = -0.4\\).\nThe column medians are also added to the column effect margin. Note that we are also adding the row effect median value, -0.5, to the common effect cell in the upper left-hand corner.\n\nWe have now completed our first row and column smoothing iteration. However, there may be more row and column effects that can be pulled from the residuals. We therefore move on to a second iteration.\n\n\n29.3.2.6 Step 6: Second iteration – row effects\nNext, we compute the row medians from the residuals, then add the column effect median to the top margin (and the common value) and subtract the row medians from the residuals. as in step 3.\n\n\n\n29.3.2.7 Step 7: Second iteration – column effects\nTo wrap-up the second iteration, we compute the column median values from the residuals then subtract the medians from those residuals. We also add the medians to the row effect values and the common value.\n\n\n\n29.3.2.8 When do we stop iterating?\nThe goal is to iterate through the row and column smoothing operations until the row and column effect medians are close to 0. However, Hoaglin et al. (1983) warn against “using unrestrained iteration” and suggest that a few steps should be more than adequate in most instances. In our working example, a third iteration may be warranted. The complete workflow for this third iteration follows.\n\nThe final version of the table (along with the column and row values) is shown below:\n\nA progression of each iteration is shown side-by-side in the next figure. Note how, at the end of the iterations, the common factor ends up contributing the largest effect. Also note the one abnormally high residual in the southern region associated with 8th grade education.\n\n\n\n\n\n\n\n\n29.3.3 Interpreting the median polish\nAs noted earlier, a two-way table represents the relationship between the response variable, \\(y\\), and the two categories as:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\n\\]\nIn our working example, \\(\\mu\\) = 20.6; \\(\\alpha_i\\) = -1.5, 2.6 -0.4, and 0.4 for \\(i\\) = NE, Nc, S and W respectively; \\(\\beta_j\\) = 7.6, 6.0, -0.9, 0.2 and -3.5 for \\(j\\) = ed8, ed9to11, ed12, ed13to15 and ed16 respectively.\nThe residuals represent the portion of the mortality rates that can’t be explained by either factors.\nSo the mortality rate in the upper left-hand cell from the original table can be deconstructed as:\n\nThe examination of the table suggests that the infant mortality rate is greatest for fathers who did not attain more than 8 years of school (i.e. who has not completed high school) as noted by the high column effect value of 7.6. This is the rate of infant mortality relative to the overall median (i.e. on average, 20.6 infants per thousand die every year and the rate goes up to 7.6 + 20.6 for infants whose father has not passed the 8th grade). Infants whose father has completed more than 16 years of school (i.e. who as completed college) have a lower rate of mortality as indicated by the low effect value of -3.5 (i.e. 3.5 fewer depths than average). The effects from regions also show higher infant mortality rates for North Central and Western regions (with effect values of 2.6 and 0.4 respectively) and lower rates for the northeastern and southern regions; however the regional effect does not appear to be as dominant as that of the father’s educational attainment.\nIt’s also important to look at the distribution of the residual numbers across the two-way table. One should identify unusually high or low residuals in the table. Such residuals may warrant further investigation (e.g. the high southern region residual value of 10.8 may need further exploration).\n\n\n29.3.4 Test for non-additivity\nThus far, we have assumed an additive relationship between the effects (factors). But this additive model may not be the best fit for our data. A good way to test this is by generating a Tukey Additivity Plot where we plot residuals vs. the comparison value, \\(cv_{ij}\\), defined as \\(\\alpha_i \\beta_j / \\mu\\). If the plot is devoid of any obvious trend or pattern we can conclude that our dataset is consistent with an additive model. Such seems to be the case with our working example as shown in the following plot.\n\n\n\n\n\nIf the diagnostic plot revealed a trend, its shape–or more specifically, its slope–could be used in helping define an appropriate transformation for the data. A rule of thumb is to apply a \\((1 – slope)\\) power transformation to the original data. If the resulting power is not appropriate for the dataset then the \\(cv_{ij}\\) can be added to the additive model as follows:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\alpha_i \\beta_j / \\mu  + \\epsilon_{ij}\n\\]"
  },
  {
    "objectID": "two_way.html#implementing-the-median-polish-in-r",
    "href": "two_way.html#implementing-the-median-polish-in-r",
    "title": "29  Analyzing two-way tables",
    "section": "29.4 Implementing the median polish in R",
    "text": "29.4 Implementing the median polish in R\nThe steps outlined in the previous section can be easily implemented using pen and paper or a spreadsheet environment for larger datasets. R has a built-in function called medpolish() which does this for us. We can define the maximum number of iterations by setting the maxiter= parameter but note that medpolish will, by default, automatically estimate the best number of iterations for us. We’ll set it to 3 to match the three iterations conducted in the previous selection.\n\ndf.med <- medpolish( df , maxiter=3)\n\n1: 47.1\n2: 42.9\n3: 42.45\n\n\nThe three values printed to the console gives us the sum of absolute residuals at each iteration; we will ignore these numbers. The output from the model is stored in the df.med object. To see its contents , simply type it at the command line.\n\ndf.med\n\n\nMedian Polish Results (Dataset: \"df\")\n\nOverall: 20.6\n\nRow Effects:\n   NE    NC     S     W \n-1.50  2.55 -0.35  0.35 \n\nColumn Effects:\n     ed8  ed9to11     ed12 ed13to15     ed16 \n   7.575    6.025   -0.925    0.175   -3.450 \n\nResiduals:\n      ed8 ed9to11   ed12 ed13to15  ed16\nNE -1.375   0.175  0.025   -0.975  0.65\nNC  1.375  -0.175 -3.425    0.975 -0.70\nS  10.975   4.725 -0.025   -4.725  0.00\nW  -3.125  -5.875  0.275    2.875  0.00\n\n\nAll three effects are displayed as well as the residuals (note that the precision returned is greater than that used in our earlier analysis).\nTo generate the Tukey additivity plot, simply wrap the median polish object with the plot command as in:\n\nplot( df.med )"
  },
  {
    "objectID": "two_way.html#what-if-we-use-the-mean-instead-of-the-median",
    "href": "two_way.html#what-if-we-use-the-mean-instead-of-the-median",
    "title": "29  Analyzing two-way tables",
    "section": "29.5 What if we use the mean instead of the median?",
    "text": "29.5 What if we use the mean instead of the median?\nThe procedure is similar with some notable differences. First, we compute the global mean instead of the medians (the common effect) then subtract it from all values in the table. Next, we compute the row means (the row effect) then subtract each mean from all values in its associated row. We finally compute the column means (from the residuals) to give us the column effect. That’s it, unlike the median polish, we do not iterate the smoothing sequences. An example of a “mean” polish applied to our data follows:\n\nThe results differ slightly from those produced using the median polish. Recall that the mean is sensitive to outliers and medians are not. If a robust estimate of the effects is sought, stick with the median polish.\nSo what can we gain from the “mean” polish? Well, as it turns out, it serves as the basis of the two-way ANOVA.\n\n29.5.1 Two-way ANOVA\nA two-way ANOVA assesses whether the factors (categories) have a significant effect on the outcome. Its implementation can be conceptualized as a regression analysis where each row and column level is treated as a dummy variable. The computed regression coefficients are the same as the levels computed using the “mean” polish outlined in the previous step except for one notable difference: the ANOVA adds one of the column level values and one of the row level values to the grand mean then subtracts those values from their respective row/column effects.\n\nThe resulting row/column levels are the regression coefficients:\n\nM <- lm(Deaths ~ Edu + Region, df.l)\ncoef(M)\n\n(Intercept)  Edued9to11     Edued12 Edued13to15     Edued16    RegionNE     RegionS     RegionW \n     32.215      -3.800     -11.250      -9.825     -13.000      -3.960      -0.320      -2.980 \n\n\nThe regression formula takes on the form:\n\\[\nDeaths = 32.21 - 3.8(ed9to11) -11.25(ed12) -9.83(ed13to15) -13(ed16) \\\\\n         -3.96(RegionNE) -0.320(RegionS)  -2.980(RegionW) + \\epsilon_{ij}\n\\]\nSo, for example, to compute the first value in the raw table (death rate = 25.3) from the formula, substitute the variables as follows:\n\\[\nDeaths = 32.21 - 3.8(0) -11.25(0) -9.83(0) -13(0) \\\\\n         -3.96(1) -0.320(0)  -2.980(0) - 3.0 \\\\\n       = 32.21 -3.96 -3.0 \\\\\n       = 25.3\n\\]\nTo assess if any of the factors have a significant effect on the Death variables, simply wrap the regression with an anova() function.\n\nanova(M)\n\nAnalysis of Variance Table\n\nResponse: Deaths\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nEdu        4 478.52 119.630  7.7539 0.00251 **\nRegion     3  57.44  19.146  1.2410 0.33801   \nResiduals 12 185.14  15.428                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results suggest that the father’s educational attainment has a significant effect on infant mortality (p = 0.0025) whereas the region does not (p = 0.338)."
  },
  {
    "objectID": "two_way.html#references",
    "href": "two_way.html#references",
    "title": "29  Analyzing two-way tables",
    "section": "29.6 References",
    "text": "29.6 References\nUnderstanding Robust and Exploratory Data Analysis, D.C. Hoaglin, F. Mosteller and J.W. Tukey, 1983."
  },
  {
    "objectID": "rmarkdown.html#introduction",
    "href": "rmarkdown.html#introduction",
    "title": "30  R markdown document",
    "section": "30.1 Introduction",
    "text": "30.1 Introduction\nAn R markdown document is a text file usually ending with an .Rmd extension. It allows one to embed R code chunks and their output into a comprehensive report thus eliminating the possibility of loading the wrong figure into the document, or forgetting to update a statistical summary in the text when the original data file was revised.\nCreating an R markdown output from an Rmd file requires knitting the file as opposed to running the code as you would an R script. The RStudio interface provides you with a knit button at the top of its interface. The knit button also allows you to choose the output format (HMTL, Word or PDF). You can also knit an Rmd file in R using the render function from the rmarkdown package. For example,\n\nrmarkdown::render(\"HW16.Rmd\")"
  },
  {
    "objectID": "rmarkdown.html#the-yaml-header",
    "href": "rmarkdown.html#the-yaml-header",
    "title": "30  R markdown document",
    "section": "30.2 The YAML header",
    "text": "30.2 The YAML header\nThe YAML header controls the look and feel of your document. At the very least, your R markdown document should contain the following YAML header sandwiched between two sets of ---:\n ---\n title: \"Your document title\"\n author: \"ES 218\"\n output:\n   html_document: default\n ---\n\nMake sure that the html_document: default line is indented at least two spaces. If you intend on creating a Word document, substitute html_document with word_document.\n ---\n title: \"Your document title\"\n author: \"ES 218\"\n output:\n   word_document: default\n ---\n\nThe YAML header can take on several parameters. For example, to add the current date, add:\ndate: '`r format(Sys.time(), \"%d %B, %Y\")`'\n\nThe above chunk makes use of an inline code chunk that will be discussed later in this tutorial. Note the mix of single quotes and back ticks that wrap the inline code. The %d, %B and %y parameters specify the date format. You can read more on date formats here.\nTo have the document automatically generate a table of contents add toc: true to the html_document or word_document header. Make sure that the toc parameter is indented at least two spaces from the xxx_document header:\n...\noutput:\n  html_document: default\n    toc: true\n\nThe above generates a static TOC. If you want to generate a floating TOC, add toc_float: true.\n...\noutput:\n  html_document: default\n    toc: true\n    toc_float: true"
  },
  {
    "objectID": "rmarkdown.html#code-folding",
    "href": "rmarkdown.html#code-folding",
    "title": "30  R markdown document",
    "section": "30.3 Code folding",
    "text": "30.3 Code folding\nRmarkdown offers the option to interactively collapse the code chunks in a knitted document. This may not be an option to have in a final report, but it may prove useful for a technical document where both code and output are to be shared. Code folding option is set with code_folding: .... The options are hide to collapse the code chunks by default and show to reveal the code chunks by default.\n...\noutput:\n  html_document: default\n    toc: true\n    toc_float: true\n    code_folding: hide"
  },
  {
    "objectID": "rmarkdown.html#section-headers",
    "href": "rmarkdown.html#section-headers",
    "title": "30  R markdown document",
    "section": "30.4 Section headers",
    "text": "30.4 Section headers\nYou can add section headers to your document by preceding the header with one or more hashtags. Each hashtag represents one heading level. For example, the top heading level is # and the third heading level is ###.\nThe top header hashtag is usually avoided because its default font size tends to be too big. It’s not uncommon to see R markdown files assign the top level to ##.\n## Use this as a top section level \n\nSome text\n\n### Use this as the second section level \n\nSome text\n\n#### Use this as the third section level \n\netc..."
  },
  {
    "objectID": "rmarkdown.html#use-this-as-a-top-section-level",
    "href": "rmarkdown.html#use-this-as-a-top-section-level",
    "title": "30  R markdown document",
    "section": "Use this as a top section level",
    "text": "Use this as a top section level\nSome text\n\nUse this as the second section level\nSome text\n\nUse this as the third section level\netc…"
  },
  {
    "objectID": "rmarkdown.html#text-formats",
    "href": "rmarkdown.html#text-formats",
    "title": "30  R markdown document",
    "section": "30.5 Text formats",
    "text": "30.5 Text formats\nThe markdown language has several built-in text formatting options. A brief summary of some their syntax follows:\n\nItalic: To italicize text, wrap it in asterisks as in *this is italicized*. Note that you do not want spaces between the asterisks and the text.\nBold: To bold text, wrap it with a pair of asterisks **this is bold**.\nWeb links: To create web links wrap the text with [ ] followed by the web link wrapped with ( ) as in [ES 218 website](https://mgimond.github.io/ES218). Make sure that there are no spaces between [] and ().\nLists: To create lists in your document, precede each list item with an asterisk followed by a space. For example:\n\n* First list element\n* Second list element\n* Third list element\n\n\n\nFirst list element\nSecond list element\nThird list element\n\n\n\nBlock equations: You can embed Latex block equations using double dollar signs,\n\n$$\nx = \\frac{1 + x}{x}\n$$\n\nwhich generates,\n\n\\[\nx = \\frac{1 + x}{x}\n\\]\n\n\nInline equations: You can also add inline Latex equations using single dollar signs,\n\nThe equation $x(1 + x)$ can be re-written as $x + x^2$. \n\nwhich generates,\n\nThe equation \\(x(1 + x)\\) can be re-written as \\(x + x^2\\)."
  },
  {
    "objectID": "rmarkdown.html#code-chunks",
    "href": "rmarkdown.html#code-chunks",
    "title": "30  R markdown document",
    "section": "30.6 Code chunks",
    "text": "30.6 Code chunks\nTo embed a code chunk, simply wrap the code between ```{r} and ```.\n```{r} \nplot(hp ~ mpg, mtcars)\n```\nCode chunks can take on many options. Examples of a few common options follow:\n\necho: If you don’t want the code chunks to appear in the ouput, set echo=FALSE.\ninclude: If you want neither the code chunk nor its ouput displayed in the output, set include=FALSE.\nfig.width and fig.height: These parameters control a figure’s height and width (in inches).\nwarning and message: Some functions will output warnings or messages, most of which you probably do not want in your output document. To hide these, set warning and message to FALSE.\n\nAn example of a code chunk with a few of the aformentioned parameter follows:\n```{r message=FALSE, warning=FALSE, echo=TRUE, fig.width=3, fig.height=2} \nplot(hp ~ mpg, mtcars)\n```\nHere’s the output (note that echo was set to TRUE in this example):\n\n\nplot(hp ~ mpg, mtcars)"
  },
  {
    "objectID": "rmarkdown.html#document-wide-code-chunk-options",
    "href": "rmarkdown.html#document-wide-code-chunk-options",
    "title": "30  R markdown document",
    "section": "30.7 Document wide code chunk options",
    "text": "30.7 Document wide code chunk options\nYou can apply document wide code chunk options. For example, to avoid adding message=FALSE and warning=FALSE to each chunk of code, you can add this single chunk of code to the beginning of your Rmd file.\n```{r include=FALSE}\nknitr::opts_chunk$set(message=FALSE, warning=FALSE)\n```"
  },
  {
    "objectID": "rmarkdown.html#tables",
    "href": "rmarkdown.html#tables",
    "title": "30  R markdown document",
    "section": "31.1 Tables",
    "text": "31.1 Tables\nYou can create two types of tables: static tables where you manually populate the cell values, and dynamic tables which are populated with R data tables.\n\n31.1.1 Static tables\nHere’s an example of a static table syntax:\ncolumn 1        Column 2    column3 \n-----------  -----------  ------------\nval1                 2.3  apple\nval2                   5  orange\nval3                0.34  kiwi\n\n\n\n\n\ncolumn 1\nColumn 2\ncolumn3\n\n\n\n\nval1\n2.3\napple\n\n\nval2\n5.0\norange\n\n\nval3\n0.34\nkiwi\n\n\n\n\nNote how the left and right adjusted columns in the output reflect the left and right adjusted columns in the above syntax. It’s important that the column elements not extend beyond the dashed line extents.\n\n\n31.1.2 Dynamic tables\nThere are many R packages that specialize in table output formats such as xtable and stargazer. However, decent tables can be created with knitr’s kable function in conjunction with kableExtra. Note that this requires the magrittr package (if the pipe %>% is used). However, if dplyr is used elsewhere in the Rmd document, the magrittr package can be omitted. Here’s an example:\nlibrary(magrittr)\nknitr::kable( head(mtcars), format=\"html\" ) %>% \n  kableExtra::kable_styling(bootstrap_options = \"striped\", \n                            full_width = FALSE, position = \"left\")\n\n\n\n\n\n\n\n \n  \n      \n    mpg \n    cyl \n    disp \n    hp \n  \n \n\n  \n    Mazda RX4 \n    21.0 \n    6 \n    160 \n    110 \n  \n  \n    Mazda RX4 Wag \n    21.0 \n    6 \n    160 \n    110 \n  \n  \n    Datsun 710 \n    22.8 \n    4 \n    108 \n    93 \n  \n  \n    Hornet 4 Drive \n    21.4 \n    6 \n    258 \n    110 \n  \n  \n    Hornet Sportabout \n    18.7 \n    8 \n    360 \n    175 \n  \n  \n    Valiant \n    18.1 \n    6 \n    225 \n    105 \n  \n\n\n\n\n\n\n\nIf the output file format is a Word document, substitute format = \"html\" with format = \"pandoc\".\nFor more kableExtra options, visit its website."
  },
  {
    "objectID": "rmarkdown.html#a-complete-example",
    "href": "rmarkdown.html#a-complete-example",
    "title": "30  R markdown document",
    "section": "31.2 A complete example",
    "text": "31.2 A complete example\nHere’s what a complete Rmd file might look like:\n\n ---\n title: \"A simple example\"\n author: \"ES 218\"\n output:\n   html_document: default\n   toc: true\n editor_options: \n   chunk_output_type: console\n ---\n\n\n```{r include=FALSE}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE)\n```\n\n## A basic plot\n\n\n```{r fig.width = 3, fig.height = 2.5} \nlibrary(ggplot2)\n\nggplot(mtcars, aes(mpg, hp)) + geom_point() + \n  geom_smooth(method = lm, se = FALSE)\n```\n\n\n## Here's a glimpse of the data table\n\n\n```{r echo = FALSE}\nknitr::kable(head((mtcars), format = \"html\"))\n```\n\n\n## A basic analysis\n\n\n```{r include = FALSE}\nM <- lm(mpg ~ hp, mtcars)\nr.sq <- round(summary(M)$r.square, 2)\n```\n\n\nThe modeled r-square between miles-per-gallon and engine horsepower is `r r.sq`."
  },
  {
    "objectID": "spatial.html",
    "href": "spatial.html",
    "title": "31  Mapping variables",
    "section": "",
    "text": "Variables can be mapped in R using packages such as ggplot2 and tmap. The focus of this tutorial will be on mapping polygon (aka areal) data. For more information on mapping spatial data (both vector and raster) visit the GIS and Spatial Analysis pages."
  },
  {
    "objectID": "spatial.html#loading-a-sample-dataset",
    "href": "spatial.html#loading-a-sample-dataset",
    "title": "31  Mapping variables",
    "section": "31.1 Loading a sample dataset",
    "text": "31.1 Loading a sample dataset\nThe following chunk loads a spatial dataset that is in an sf spatial format. The data represent the 2018 census tracts for the state of Maine (USA).\n\nlibrary(sf)\n\nshp <- readRDS(gzcon(url(\"https://github.com/mgimond/ES218/blob/gh-pages/Data/maine_tracts.Rds?raw=true\")))\n\nThis next chunk is a data table of the average commute time (in minutes) for each Maine census tract.\n\ndat <- read.csv(\"http://mgimond.github.io/ES218/Data/maine_commute.csv\")"
  },
  {
    "objectID": "spatial.html#joining-a-table-to-a-spatial-object",
    "href": "spatial.html#joining-a-table-to-a-spatial-object",
    "title": "31  Mapping variables",
    "section": "31.2 Joining a table to a spatial object",
    "text": "31.2 Joining a table to a spatial object\nMany mapping operations require that the data table be joined to an already created spatial dataset. If your data are aggregated at some administrative boundary level, you can find boundary files for most countries (and at different aggregate levels) in an sf format at https://gadm.org/.\nHere, we’ll join the dat table to the shp spatial object using Geo_FIPS as the common key.\n\nlibrary(dplyr)\n\nshp2 <- left_join(shp, dat, by = \"Geo_FIPS\")"
  },
  {
    "objectID": "spatial.html#mapping-the-data",
    "href": "spatial.html#mapping-the-data",
    "title": "31  Mapping variables",
    "section": "31.3 Mapping the data",
    "text": "31.3 Mapping the data\nThere are at least two mapping solutions in R: ggplot2 and tmap. Examples using both mapping environments follow.\n\n31.3.1 Mapping with ggplot2\nThe ggplot grammar is no different here from what you’ve learned earlier in the course. We simply make use of the geom_sf() function to generate the map.\n\nlibrary(ggplot2) \n\nggplot(shp2) + geom_sf(aes(fill = Commute)) \n\n\n\n\nThe commute time variable, commute, is mapped to the map’s fill aesthetic. You can overcome many of the plot features’ default behavior. For example, you can bin the color schemes by assigning ranges of commute time values to a unique set of color swatches using one of the scale_fill_steps* family of functions. You can also remove the border colors by setting col = NA. This is demontrated next.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35))\n\n\n\n\nHere, we make use of hex defined colors. Note that you can use the built-in color names or the rgb() function to define the palette colors.\nIt might help to darken the background panel using the theme function. We’ll assign the grey70 color to the background and grey80 to the grid lines.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35)) +\n  theme(panel.grid = element_line(color = \"grey80\"),\n        panel.background = element_rect(fill = \"grey70\"))\n\n\n\n\nCoordinate systems can play an important role in both analyzing and visualizing spatial data. If you want to control the coordinate system used in your map, add the coord_sf function. Here we’ll adopt a UTM NAD83 Zone 19N coordinate system using its EPSG code of 26919. Note that the grid axes will still adopt the lat/long designation.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  coord_sf(crs = 26919) +\n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35)) +\n  theme(panel.grid = element_line(color = \"grey80\"),\n        panel.background = element_rect(fill = \"grey70\"))\n\n\n\n\nFinally, we’ll modify the bin intervals by generating a non-uniform classification scheme. As such we’ll scale the legend bar so as to reflect the non-uniform intervals using the guide_coloursteps() function and its even.steps = FALSE argument (note that this feature is only available in ggplot2 ver 3.3 or greater). We’ll also modify the legend bar dimensions and title.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  coord_sf(crs = 26919) +\n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35),\n                    values = scales::rescale(c(5, 15, 20, 30, 35), c(0,1)),\n                    guide = guide_coloursteps(even.steps = FALSE,\n                                              show.limits = TRUE,\n                                              title = \"Commute time \\n(min)\",\n                                              barheight = unit(2.3, \"in\"),\n                                              barwidth = unit(0.15, \"in\"))) +\n  theme(panel.grid = element_line(color = \"grey80\"),\n        panel.background = element_rect(fill = \"grey70\"))\n\n\n\n\n\n\n31.3.2 Mapping with tmap\nWhile ggplot2 can handle basic mapping operations, it does not offer the ease and flexibility of the tmap package. Much like ggplot2, tmap adopts the same layering strategy. For example, to construct the above plot we first specify the layer to plot via the call to tm_shape, then the geometry to symbolize the spatial feature, tm_fill().\n\nlibrary(tmap)\n\ntm_shape(shp2) +  tm_fill(col = \"Commute\")\n\n\n\n\nTo change the projection, pass the PROJ4 string to the projection parameter. To move the legend box outside of the map, set tm_legend(outside = TRUE).\n\ntm_shape(shp2, projection = 26919) + \n  tm_fill(col = \"Commute\") +\n  tm_legend(outside = TRUE)\n\n\n\n\nYou can also modify the classification breaks and colors. For example, to break the values following quantiles, and assigning the same color scheme used with the ggplot output in an earlier code chunk, you would write:\n\ntm_shape(shp2, projection = 26919) + \n  tm_fill(col = \"Commute\", n = 6, style = \"quantile\",  \n          palette = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                      \"#D9EF8B\", \"#91CF60\", \"#1A9850\")) +\n  tm_legend(outside = TRUE)\n\n\n\n\nTo explore additional mapping options with tmap Click here."
  },
  {
    "objectID": "spatial.html#another-example",
    "href": "spatial.html#another-example",
    "title": "31  Mapping variables",
    "section": "31.4 Another example",
    "text": "31.4 Another example\nIn this section we explore a more complicated workflow that requires additional data manipulation steps to allow a proper join between US counties and Census data.\nFirst, we’ll load the cnty spatial object to your current session environment.\n\nlibrary(sf)\n\nload(url(\"https://github.com/mgimond/ES218/blob/gh-pages/Data/counties48.RData?raw=true\"))\n\nNext, we’ll plot the data. Since the data spans the continental US, we’ll change the projection to an equal area projection. This projection will be defined using a string (ea object)–note that a string is another way to define a projection. To learn more about defining projections in R, click here.\nSince we have no polygons to fill just yet, we’ll simply draw the outline using tm_polygons()\n\n# Define the projection using PROJ4 syntax\nea <- \"+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 \n       +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\"\n\n# Draw the map\ntm_shape(cnty, projection = ea) + \n  tm_polygons() +\n  tm_layout(outer.margins = c(.1,.1,.1,.1))"
  },
  {
    "objectID": "spatial.html#joining-tables-to-spatial-objects",
    "href": "spatial.html#joining-tables-to-spatial-objects",
    "title": "31  Mapping variables",
    "section": "31.5 Joining tables to spatial objects",
    "text": "31.5 Joining tables to spatial objects\nIn the examples that follow, you will learn how to join census income data to a spatial object in one of two ways: First, by state and county names; Then, by FIPS (Federal Information Processing Standards) ids.\n\n31.5.1 Joining income table to the map by county and state names\nWe will use the income-by-gender-education dataset used earlier in the course and limit the data to median income per person (B20004001 column).\n\nlibrary(dplyr)\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/Income_education.csv\")\ndf1 <- df %>% select(subregion = County, region = State, B20004001 )\n\nNext, we need to join the census data to the county map using two columns: state name and county name (or region and subregion columns). Let’s compare the two dataframes by viewing the first few rows of each.\n\nhead(cnty)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -88.01778 ymin: 30.24071 xmax: -85.06131 ymax: 34.2686\nGeodetic CRS:  WGS 84\n               ID                       geometry\n1 alabama,autauga MULTIPOLYGON (((-86.50517 3...\n2 alabama,baldwin MULTIPOLYGON (((-87.93757 3...\n3 alabama,barbour MULTIPOLYGON (((-85.42801 3...\n4    alabama,bibb MULTIPOLYGON (((-87.02083 3...\n5  alabama,blount MULTIPOLYGON (((-86.9578 33...\n6 alabama,bullock MULTIPOLYGON (((-85.66866 3...\n\nhead(df1)\n\n  subregion region B20004001\n1   Autauga     al     35881\n2   Baldwin     al     31439\n3   Barbour     al     25201\n4      Bibb     al     29016\n5    Blount     al     32035\n6   Bullock     al     26408\n\n\nWe note two problems. First, the cnty object encodes states using their full name and not their two letter abbreviation. Second, the county names in cnty are in all lower case. Before attempting to join the dataframes, we must first fix these discrepancies. We will choose to modify df so that its state and county names match those in the cnty (map) dataframe.\nWe will first create a state name/abbreviation look-up table using the built-in state.name and state.abb objects. Note that using these pre-existing objects will require that we add the District of Columbia to the table since it’s not present in either data objects.\n\nst <- data.frame(region=tolower(state.name), State = tolower(state.abb)) %>% \n      bind_rows( data.frame(region=\"district of columbia\", State=\"dc\") ) \n\nNext, we join the st look-up table to df then make the additional changes needed to join the census data to the counties map. Note that we are overwriting the earlier instance of df1.\n\ndf1 <- df %>% \n  inner_join(st, by=\"State\") %>%\n  mutate(ID = paste(region,tolower(County), sep = \",\"))  %>%\n  select(ID, B20004001 )\n\nWe can now join df1 to cnty.\n\ncnty.df1 <- inner_join(cnty, df1, by=\"ID\" )\n\nNow let’s map the income distribution.\n\ntm_shape(cnty.df1) + tm_fill(col = \"B20004001\", palette = \"Greens\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nYou may notice that the counties map does not seem complete; most notable is the absence of all Louisiana counties. Let’s look at the rows in cnty.df1 associated with the state of Louisiana.\n\nlibrary(stringr)\ntable(str_detect(cnty.df1$ID,  \"louisiana\") )\n\n\nFALSE \n 2971 \n\n\nThere are no rows returned (i.e. all cases returned FALSE). This suggests that the Louisiana counties did not properly join. Let’s compare the Louisiana county names between df1 and cnty.\n\ndf1 %>% filter(str_detect(ID,  \"louisiana\")) %>% head()\n\n                           ID B20004001\n1     louisiana,acadia parish     28678\n2      louisiana,allen parish     29870\n3  louisiana,ascension parish     43464\n4 louisiana,assumption parish     34269\n5  louisiana,avoyelles parish     26483\n6 louisiana,beauregard parish     33983\n\ncnty %>% filter(str_detect(ID,  \"louisiana\")) %>% head()\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -93.74163 ymin: 29.62765 xmax: -90.63619 ymax: 31.35225\nGeodetic CRS:  WGS 84\n                    ID                       geometry\n1     louisiana,acadia MULTIPOLYGON (((-92.61863 3...\n2      louisiana,allen MULTIPOLYGON (((-92.5957 30...\n3  louisiana,ascension MULTIPOLYGON (((-91.12894 3...\n4 louisiana,assumption MULTIPOLYGON (((-91.23207 3...\n5  louisiana,avoyelles MULTIPOLYGON (((-92.32642 3...\n6 louisiana,beauregard MULTIPOLYGON (((-93.12856 3...\n\n\nIt turns out that Louisiana does not divide its administrative areas into counties but parishes instead. The income data (originally downloaded from the Census Bureau) follows through with that designation convention by adding the word “parish” to each of its administrative area names. We therefore need to remove all instances of parish in the subregion names associated with Louisiana. We will therefore need to recreate the df1 object as follows:\n\nlibrary(stringr)\ndf1 <- df %>% \n  inner_join(st, by=\"State\") %>%\n  mutate(ID = paste(region,tolower(County), sep = \",\"),\n         ID = ifelse(region==\"louisiana\", \n                     str_replace(ID, \" parish\", \"\"), ID))  %>%\n  select(ID, B20004001 )\n\nLet’s re-join the dataframes and re-plot the map.\n\ncnty.df1 <- inner_join(cnty, df1, by=\"ID\" )\n\ntm_shape(cnty.df1) + tm_fill(col = \"B20004001\", palette = \"Greens\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nMost of Louisiana’s parishes are now mapped, but we are still missing a few parishes as well as a few counties. This is a result of differences in spelling for two-word county and parish names. For example, df1 encodes St. Lucie county (Florida) as st. lucie whereas cnty omits the dot and encodes it as st lucie. Fixing these discrepancies will require additional labor. At this point, it may prove more fruitful to do away with state/county names as joining keys and use FIPS county codes instead.\nFIPS (Federal Information Processing Standards) codes assign each county/state a unique five number designation thus making it easier to join data to spatial features. However, neither our census data table nor the built-in counties map have FIPS codes. We will download another version of the income data (one that has FIPS codes). Note that FIPS codes are normally part of datasets provided by the Census Bureau. The maps package has a dataset called county.fips that can be used to match state/county names in the county map to FIPS codes.\n\n\n31.5.2 Joining income table by FIPS code\nLet’s download the FIPS version of the dataset. Note that we will not need to modify/add columns to the data since we will be joining this table to the county map using the FIPS code.\n\ndf <- read.csv(\"http://mgimond.github.io/ES218/Data/Income_education_with_FIPS.csv\")\ndf1 <- df %>% select(FIPS, B20004001 )\n\nNext, we’ll load the county.fips dataset from the maps package.\n\n# Load the county.fips dataset\ncounty.fips <- maps::county.fips\nhead(county.fips)\n\n  fips        polyname\n1 1001 alabama,autauga\n2 1003 alabama,baldwin\n3 1005 alabama,barbour\n4 1007    alabama,bibb\n5 1009  alabama,blount\n6 1011 alabama,bullock\n\n\n\ncnty2 <- cnty %>%\n         left_join(county.fips, by=c(\"ID\" = \"polyname\"))\n\nNow that the FIPS codes are part of the county map dataframe, let’s join the income data table to the county map, then map the income values.\n\ncnty2.df1 <- inner_join(cnty2, df1, by=c(\"fips\" = \"FIPS\") )\n\ntm_shape(cnty2.df1) + tm_fill(col = \"B20004001\", palette = \"Greens\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nThis is an improvement over the last map. But there are still a few counties missing. After a closer scrutiny of the data, it seems that the county.fips table splits some counties into two or more sub-regions. For example, Accomack county (Virginia) is split into two regions (and thus into two different names):\n\ncounty.fips %>% filter(str_detect(polyname,  \"virginia,accomack\"))\n\n   fips                       polyname\n1 51001         virginia,accomack:main\n2 51001 virginia,accomack:chincoteague\n\n\nFixing these discrepancies would require manual intervention and time. Another work around is to use the Census Bureau’s map files instead of maps’s built-in map. We will cover this in the next section."
  },
  {
    "objectID": "spatial.html#working-with-external-gis-files",
    "href": "spatial.html#working-with-external-gis-files",
    "title": "31  Mapping variables",
    "section": "31.6 Working with external GIS files",
    "text": "31.6 Working with external GIS files\n  \n\n31.6.1 Reading a shapefile into R\nThe Census Bureau maintains its own library of administrative boundary shapefiles (shapefiles are popular map data formats). This may prove to be the best map source to use since its areal units are designed to match the records in the income data table. Loading a shapefile can easily be accomplished using the st_read() function from the sf package. One option is to download the shapefile from the Census Bureau’s website (usually in a zipped format) then unpack it in a local project folder before reading it into the current session with st_read(). But we can maximize the replicability of the workflow by writing a chunk of code that will download the zipped shapefile into a temporary directory before unzipping it and loading the shapefile into an active R session as shown below.\n\nlibrary(sf)\n\ntmp <- tempdir()\nlink <- \"http://www2.census.gov/geo/tiger/GENZ2010/gz_2010_us_050_00_20m.zip\"\nfilename <- basename(link)\ndownload.file(link, filename)\nunzip(filename, exdir = tmp )\nshapefile <- st_read(dsn = tmp, layer = tools::file_path_sans_ext(filename))\n\nReading layer `gz_2010_us_050_00_20m' from data source `C:\\Users\\mgimond\\AppData\\Local\\Temp\\RtmpgfvWMf' using driver `ESRI Shapefile'\nSimple feature collection with 3221 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 17.88481 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\n\nThe shapefile is unzipped into a temporary folder, tmp, then read from that same directory and stored in an sf object called shapefile. The code tools::file_path_sans_ext(filename) extracts the name of the zip file (minus the extension) which turns out to also be the name of the shapefile. Note that if the zip file was manually unpacked in a project folder then the only command that would have needed to be executed is the st_read function as in st_read(\"gz_2010_us_050_00_20m.shp\" ).\nThe Census shapefile has a field called GEO_ID that includes the county FIPS codes along with other superfluous county ID values. We only need the last five digit county FIPS code so we’ll extract these codes into a new column called FIPS.\n\nshapefile$FIPS <- as.numeric(str_sub(shapefile$GEO_ID, 10, 14))\n\nNext we can append the income table to the shapefile object.\n\ncnty2.cs <- shapefile %>%\n            inner_join(df1, by=\"FIPS\")\n\nNow, we can plot the income distribution map. Note that the Census Bureau shapefile includes the 48 states, Alaska and Hawaii. If you want to limit the extent to the 48 states, define the boundary limits using the bbox parameter in the tm_shape function.\n\ntm_shape(cnty2.cs, \n         bbox = st_bbox(c(xmin = -125, xmax = -67 , ymin = 25, ymax = 50))) +\n  tm_fill(col = \"B20004001\", palette = \"Greens\", title = \"Income\") +\n  tm_legend(outside = TRUE) \n\n\n\n\n\n\n31.6.2 Modifying the color scheme\nThe classification scheme can be customized. For example, to split the income values across six intervals (e.g. $0 to $20000, $20000 to $30000, $30000 to $50000, and $50000 to $100000) and to label the values as dollars, type the following:\n\ntm_shape(cnty2.cs, \n         bbox = st_bbox(c(xmin = -125, xmax = -67 , ymin = 25, ymax = 50))) +\n  tm_fill(col = \"B20004001\", palette = \"Greens\",\n          style=\"fixed\", breaks=c(0, 20000 , 30000, 50000, 100000 ),\n          labels = c(\"under $20k\", \"$20k - $30k\", \"$30k - $50k\", \"above $50k\"),\n          title = \"Income\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nWe might choose to map the income distribution using a divergent color scheme where we compare the counties to some overall value such as the counties’ median income value. We can use the quantile() function to break the income data into its percentiles. This will be helpful in generating symmetrical intervals about the median.\n\ninc.qt <- quantile(df1$B20004001, probs = c(0, 0.125, .25, 0.5, .75, .875 , 1 ))\n\nA divergent color scheme consists of two different hues whose lightness values converge towards some central value (the median income in our case). We’ll use a red-to-green color palette, RdYlGn, for this next map where red hues highlight counties below the county median income and green hues highlight counties above the county median value.\n\ntm_shape(cnty2.cs,\n         bbox = st_bbox(c(xmin = -125, xmax = -67 , ymin = 25, ymax = 50))) +\n  tm_fill(col = \"B20004001\", palette = \"RdYlGn\",\n          style=\"fixed\", breaks = inc.qt,\n          title = \"Income\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nThe divergent map gives us a different handle on the data. Here, we can identify the poorer than average counties such as the southeast and parts of the Mississippi river region as well as the wealthier counties such as the mid-Atlantic region and the west coast."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "32  Data used in ES218",
    "section": "",
    "text": "CO2 (1959-2018): file, README\nCO2 (1959-2020): file, README\nSouthern oscillation index (1960-2014): file, README\nPacific decadal oscillation index (1900-2001): file, README\nACS Income and Education (2008-2012): file (CSV), file (RDS), Code book\nIncome by educational attainment (2008 - 2012): file (CSV), file with FIPS (CSV), Code book\nPopulation (1850-2013): file, README\nColby courses (Spring 2020): file, Department/Division key\nSeeded clouds: file, README\nGHCND temperature data for five US locations: file, README, Documentation\nHistorical temperature data for New York: file, README\nHistorical temperature data for Bombay: file, README\nHistorical temperature data for Shanghai: file, README\nGulf of Maine buoy data (2012): file, README\nGulf of Maine buoy data (1905 - 2012): file\nKennebec River daily discharge (2004 - 2014): file (csv),file (xlsx), README\nNFL Combine: file, README\nUninsured by county: file, README\nGreenhouse gas emissions for the northeast (2008 & 2011): file, README\nNorth american grain production (1961-2012): file, README\nLogan airport (BOS) flight data (2014): file (CSV),file (rds), README\nConsumer expenditure (1995-2012): file (csv), file (RDS), README\nWilliam Cleveland’s fusion time data: file\nWilliam Cleveland’s food web data: file\nWilliam Cleveland’s ganglion data: file\nWilliam Cleveland’s figure 3.6 data: file\nWilliam Cleveland’s carbon dating data: file\nIn-class sample data 1: file"
  },
  {
    "objectID": "dplyr.html#sec-pipe",
    "href": "dplyr.html#sec-pipe",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.2 Combining data manipulation functions using the pipe %>%",
    "text": "9.2 Combining data manipulation functions using the pipe %>%\nIn most cases, you will find yourself wanting to combine several of dplyr’s data manipulation functions. For example,\n\ndat.yield  <- filter(dat, Information == \"Yield (Hg/Ha)\", \n                          Crop == \"Oats\",\n                          Year == 2012)\ndat.rename <- mutate(dat.yield, Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))\ndat.final  <- select(dat.rename, Country, Value)\n\nhead(dat.final, 3)\n\n  Country    Value\n1     CAN 24954.79\n2     USA 21974.70\n\n\nThe downside to this approach is the creation of several intermediate objects (e.g. dat.yield and dat.rename). This can make the workflow difficult to follow and clutter your R session with needless intermediate objects.\nAnother approach to combining dplyr operations is to use the piping operator ,%>%, which daisy chains dplyr operations. So our previous workflow could look like:\n\ndat.final <- dat %>%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Crop == \"Oats\",\n         Year == 2012)  %>% \n  mutate(Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\")) %>%\n  select(Country, Value)\n\nhead(dat.final, 3)\n\n  Country    Value\n1     CAN 24954.79\n2     USA 21974.70\n\n\nThe chunk of code can be read as “… with the dat table, filter by …, then mutate …., then select …” with the result from one operation being passed on to the next using the %>% operator. Note that the filter, mutate and select functions do not include the data table name making the chunk of code less cluttered and easier to read. The input data table dat appears just once at the beginning of the pipe.\n\n9.2.1 R has a native pipe too\nR has recently (as of version 4.1) added its own native pipe to its base function. Its infix operator is written as |>. In most code chunks covered in these tutorials, you can substitute %>% with |>. For example, you can write the previous code chunk as:\n\ndat.final <- dat  |> \n  filter(Information == \"Yield (Hg/Ha)\", \n         Crop == \"Oats\",\n         Year == 2012)   |>  \n  mutate(Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))  |> \n  select(Country, Value)\n\nThere are, however, a few subtle differences between the two. A deeper dive in how they differ can be found here.\nIn this course, we’ll stick with the %>% operator given that |> is new and is not yet as widely adopted as %>%."
  },
  {
    "objectID": "strings.html#searching-for-non-alphanumeric-strings",
    "href": "strings.html#searching-for-non-alphanumeric-strings",
    "title": "6  Working with string objects",
    "section": "6.2 Searching for non alphanumeric strings",
    "text": "6.2 Searching for non alphanumeric strings\nThe following characters need specialized syntax when sought in a regular expression: . , + , * , ? , ^ , $ , ( , ) , [ , ] , { , } , | , \\ . For example, when searching for a parenthesis ( in a string, the following code will generate an error:\n\nstr_detect(\"Some text (with parenthesis)\", \"(with\")\n\nError in stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)): Incorrectly nested parentheses in regex pattern. (U_REGEX_MISMATCHED_PAREN, context=`(with`)\n\n\nTo resolve this, you need to precede the special character with the escape characters \\\\ (two backslashes) as in:\n\nstr_detect(\"Some text (with parenthesis)\", \"\\\\(with\")\n\n[1] TRUE\n\n\nLikewise, when searching for an asterisk, *, type:\n\nstr_detect(\"x * y\", \"\\\\*\")\n\n[1] TRUE\n\n\nNote that not all special characters will generate an error. For example, if you are looking for a period . in a string, the following will not generate the desired outcome:\n\nstr_detect(\"x * y\", \".\")\n\n[1] TRUE\n\n\nThis should have outputted FALSE given that no period is present in the string. The dot has a special role in a regular expression in that it seeks out any one character in the string–hence the reason it returned TRUE given that we had at least one character in the string x * y.\nThe generate the desired outcome, add \\\\:\n\nstr_detect(\"x * y\", \"\\\\.\")\n\n[1] FALSE\n\n\nTo learn more about regular expressions, see this wikipedia entry on the topic."
  }
]