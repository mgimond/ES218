[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Data Analysis in R",
    "section": "",
    "text": "Preface\nThis book is a compilation of lecture notes used in an Exploratory Data Analysis in R course taught to undergraduates at Colby College. The course assumes little to no background in quantitative analysis nor in computer programming and was first taught in Spring, 2015. The course introduces students to data manipulation in R, data exploration (in the spirit of John Tukey’s EDA) and the R markdown language. Many of the visualization techniques are adopted from William Cleveland’s Data Visualization book.\nThe base R plotting environment and the ggplot2 ecosystem are used throughout this book. While a chapter is dedicated to the lattice plotting package, its functions are not used outside of that chapter given that ggplot2 offers many of lattice’s functionality.\nWhile great effort is made to adopt a consistent plotting environment throughout this book (this being ggplot2, for the most part), a few topics (including the q-q plot and the median polish) will benefit from custom plotting functions available in the tukeyedar package. The package can be downloaded from GitHub via the command:\n\ndevtools::install_github(\"mgimond/tukeyedar\")\n\nNote that installing the GitHub package will require that the devtools package be installed first.\nFunctions making use of the tukeyedar package will be highlighted in a peach/pink code block as opposed to the default light yellow code block used for all other code blocks. For example, if tukeyedar’s eda_qq function is used, the code block will take on the following appearance:\n\nlibrary(tukeyedar)\neda_qq(Tenor, Bass)\n\nThe tukeyedar functions are built off of base R graphics and require R vesion 4.1 or greater.\n\nManuel “Manny” Gimond"
  },
  {
    "objectID": "intro.html#what-is-exploratory-data-analysis-eda",
    "href": "intro.html#what-is-exploratory-data-analysis-eda",
    "title": "1  EDA",
    "section": "1.1 What is Exploratory Data Analysis (EDA)?",
    "text": "1.1 What is Exploratory Data Analysis (EDA)?\nTraditional approaches to data analysis tend to be linear and unidirectional. It often starts with the acquisition or collection of a dataset, then ends with the computation of some inferential or confirmatory procedure.\n\nUnfortunately, such practice can lead to faulty conclusions. The following datasets generate identical regression analysis results shown in the above figure yet, they are all completely different!\n\n\n\n\n\nThe four plots represent Francis Anscombe’s famous quartet which he used to demonstrate the importance of visualizing the data before proceeding with traditional statistical analysis. Of the four plots, only the first is a sensible candidate for the regression analysis; the second dataset highlights a nonlinear relationship between X and Y; the third and fourth plots demonstrate the disproportionate influence of a single outlier on the regression procedure.\nThe aforementioned example demonstrates that a sound data analysis workflow must involve data visualization and exploration techniques. Exploratory data analysis seeks to extract salient features about the data (that may have otherwise gone unnoticed) and to help formulate hypotheses. Only then should appropriate statistical tests be applied to the data to confirm a hypothesis.\nHowever, not all EDA workflows result in a statistical test: We may not be seeking a hypothesis or, if a hypothesis is sought we may not have the statistical tools necessary to test the hypothesis. It’s important to realize that many statistical procedures found in commercial software make restrictive assumptions about the data and the type of hypothesis being tested; data sets seldom meet those stringent requirements.\n\n “Exploratory data analysis is an attitude, a flexibility, and a reliance on display, NOT a bundle of techniques.”\n–John Tukey\n\nJohn Tukey is credited with having coined the term Exploratory Data Analysis and with having written the first comprehensive book on that subject (Tukey, 19771). The book is still very much relevant today and several of the techniques highlighted in the book will be covered in this course."
  },
  {
    "objectID": "intro.html#the-role-of-graphics-in-eda",
    "href": "intro.html#the-role-of-graphics-in-eda",
    "title": "1  EDA",
    "section": "1.2 The role of graphics in EDA",
    "text": "1.2 The role of graphics in EDA\nThe preceding example highlights the importance of graphing data. A core component of this course is learning how to construct effective data visualization tools for the purpose of revealing patterns in the data. The graphical tools must allow the data to express themselves without imposing a story.\n\n “Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way.”\n–William S. Cleveland\n\nWilliam Cleveland has written extensively about data visualization and has focused on principles founded in the field of cognitive neuroscience to improve data graphic designs. His book, Visualizing Data, is a leading authority on statistical graphics and, despite its age, is as relevant today as it was two decades ago. It focuses on graphical techniques (some newer than others) designed to explore the data. This may differ from graphics generated for public dissemination which benefits from another form of data visualization called information visualization (or infovis for short). Infovis will not be covered in this course (though there is some overlap between the two techniques). For a good discussion on the differences between statistical graphics and infovis see the 2013 article Infovis and Statistical Graphics: Different Goals, Different Looks2\nCleveland has also contributed a very important tool to EDA: the LOESS curve. The LOESS curve will be used extensively in this course. It is one of many fitting options used in smoothing (or detrending) the data. Others include parametric models such as the family of linear polynomials and Tukey’s suite of smoothers notably the running median and the 3RS3R."
  },
  {
    "objectID": "intro.html#we-need-a-good-data-analysis-environment",
    "href": "intro.html#we-need-a-good-data-analysis-environment",
    "title": "1  EDA",
    "section": "1.3 We need a good data analysis environment",
    "text": "1.3 We need a good data analysis environment\nEffective EDA requires a flexible data analysis environment that does not constrain one to a limited set of data manipulation procedures or visualization tools. After all, would any good writer limit herself to a set of a hundred pre-built sentences? Of course not–we would be reading the same novels over and over again! So why would we limit ourselves to a limited set of pre-packaged data analysis procedures? EDA requires an arsenal of data analysis building blocks much like a good writer needs an arsenal of words. Such an environment must provide us with flexible data manipulation capabilities, a flexible data visualization environment and access to a wide range of statistical procedures. A scripting environment, like R, offers such an environment.\nThe data analysis environment should also be freely available, and its code open to the public. Free access to the software allows anyone with the right set of skills to share in the data analysis, regardless of any budgetary constraints. The open source nature of the software ensures that any aspect of the code used for a particular task can be examined when additional insight into the implementation of an analytical/numerical method if needed. However, deciphering code may not be a skill available to all researchers; if the need to understand how a procedure is implemented is important enough, an individual with the appropriate programming skills can be easy to come by, even if it’s for a small fee. Open source software also ensures that the underlying code used to create the executable application can be ported to different platforms or different operating systems (even though this too may require some effort and modest programming skills).\n\n1.3.1 The workhorse: R\nR is an open source data analysis and visualization programming environment whose roots go back to the S programming language developed at Bell Laboratories in the 1970’s by John Chambers. It will be used almost exclusively in this course.\n\n\n1.3.2 The friendly interface: RStudio\nRStudio is an integrated development environment (IDE) to R. An IDE provides a user with an interface to a programming environment (like R) by including features such as a source code editor (with colored syntax). RStudio is not needed to use R (which has its own IDE environment–albeit not as nice as RStudio’s), but makes using R far easier. RStudio is an open source software, but unlike R, it’s maintained by a private entity which also distributes a commercial version of RStudio for businesses or individuals needing customer support.\n\n\n1.3.3 Data manipulation\nBefore one can begin plotting data, one must have a data table in a form ready to be plotted. In cases where the data table consists of just two variables (columns), little data manipulation may be needed, but in cases where data tables consist of tens or scores of variables, data manipulation, subsetting and/or reshaping may be required. Tackling such a task can be challenging in a point and click spreadsheet environment and can introduce clerical error. R offers an array of data table manipulation tools and packages such as tidyr and dplyr. Furthermore, R’s scripting environment enables one to read through each step of a manipulation procedure in a clear and unambiguous way. Imagine the difficulty in properly documenting all the point-and-click steps followed in a spreadsheet environment.\nFor example, a data table of grain production for North America may consist of six variables and 1501 rows. The following table shows just the first 7 lines of the 1501 rows.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nCrop\nInformation\nYear\nValue\nSource\n\n\n\n\nCanada\nBarley\nArea harvested (Ha)\n2012\n2060000.00\nOfficial data\n\n\nCanada\nBarley\nYield (Hg/Ha)\n2012\n38894.66\nCalculated data\n\n\nCanada\nBuckwheat\nArea harvested (Ha)\n2012\n0.00\nFAO estimate\n\n\nCanada\nCanary seed\nArea harvested (Ha)\n2012\n101900.00\nOfficial data\n\n\nCanada\nCanary seed\nYield (Hg/Ha)\n2012\n12161.92\nCalculated data\n\n\nCanada\nGrain, mixed\nArea harvested (Ha)\n2012\n57900.00\nOfficial data\n\n\n\n\n\nThere are many ways in which we may want to summarize the data table. We could, for example, want to compute the total Barley yield for Canada by year for the years ranging from 2005 and 2007. In R, this would be done in just a few lines of code:\n\nlibrary(dplyr) \ndat2 &lt;- fao %&gt;% \n    filter(Information == \"Yield (Hg/Ha)\",  Crop==\"Barley\", Country==\"Canada\",\n           Year &gt;= 2005, Year &lt;=2010) %&gt;%  \n    group_by(Year) %&gt;% \n    summarise(Barley_yield = round(median(Value))) \n\n\n\n\n\n\nYear\nBarley_yield\n\n\n\n\n2005\n32134\n\n\n2006\n29703\n\n\n2007\n27476\n\n\n2008\n33646\n\n\n2009\n32620\n\n\n2010\n31859\n\n\n\n\n\n\n\nOn the other hand, creating the same output in a spreadsheet environment would take a bit more effort and its workflow would be less transparent.\n\n\n1.3.4 Reproducible analysis\nData table manipulation is inevitable in any data analysis workflow and, as discussed in the last section, can be prone to clerical errors if performed in a point-and-click environment. Furthermore, reproducing a workflow in a spreadsheet environment can be difficult unless each click and each copy-and-paste operations are meticulously documented. And even if the documentation is adequate, there is no way of knowing if the analyst followed those exact procedures (unless his mouse and keyboard moves were recorded). However, with a scripting environment, each step of a workflow is clearly and unambiguously laid out as demonstrated with the FAO grain data above. This leads to another basic tenet of the scientific method: reproducibility of the workflow.\nReproducible research lends credence to scientific work. The need for reproducibility is not limited to data collection or methodology but includes the actual analytical workflow that generated the results including data table output and statistical tests.\nData analysis can be complex. Each data manipulation step that requires human interaction is prone to clerical error. But error can also manifest itself in faulty implementation of an analytical procedure—both technical and theoretical. Unfortunately, workflows are seldom available in technical reports or peer-reviewed publications where the intended audience is only left with the end product of the analysis.\n\n “… a recent survey of 18 quantitative papers published in Nature Genetics in the past two years found reproducibility was not achievable even in principle for 10.”\n–Keith A. Baggerly & Kevin R. Coombes3\n\n\nUnfortunately, examples of irreproducible research are all too common. An example of such was reported by the New York Times in an article titled How Bright Promise in Cancer Testing Fell Apart. In 2006, researchers at Duke had published a paper in Nature Medicine on a breakthrough approach to fighting cancer. The authors’ research suggested that genomic tests of a cancer cell’s DNA could be used to target the most effective chemotherapy treatment. This was heralded as a major breakthrough in the fight against cancer. Unfortunately, the analysis presented by the authors was flawed. Two statisticians, Dr. Baggerly and Dr. Coombes, sought to replicate the work but discovered instead that the published work was riddled with problems including mis-labeling of genes and confounding experimental designs. The original authors of the research did not make the analytical workflow available to the public thus forcing the statisticians to scavenge for the original data and techniques. It wasn’t until 5 years later, in 2011, that Nature decided to retract the paper because they were “unable to reproduce certain crucial experiments”.\nMany journals now require or strongly encourage authors to “make materials, data and associated protocols promptly available to readers without undue qualifications” (Nature, 2014). Sharing data file is not too difficult, but sharing the analytical workflow used to generate conclusions can prove to be difficult if the data were run though many different pieces of software and point-and-click procedures. An ideal analytical workflow should be scripted in a human readable way from beginning (the moment the data file(s) is/are read) to the generation of the data tables or data figures used in the report of publication. This has two benefits: elimination of clerical errors (associated with poorly implemented point-and-click procedures) and the exposition of the analytical procedures adopted in the workflow."
  },
  {
    "objectID": "intro.html#creating-dynamic-documents-using-r-markdown",
    "href": "intro.html#creating-dynamic-documents-using-r-markdown",
    "title": "1  EDA",
    "section": "1.4 Creating dynamic documents using R Markdown",
    "text": "1.4 Creating dynamic documents using R Markdown\nAnother source of error in the write-up of a report or publication is the linking of tables, figures and statistical summaries to the write-up. Typically, one saves statistical plots as image files then loads the image into the document. However, the figures may have gone through many different iterations resulting in many different versions of the image file in a working folder. Add to this many other figures, data table files and statistical results from various pieces of software, one quickly realizes the potential for embedding the wrong image files in the document or embedding the wrong statistical summaries in the text. Furthermore, the researcher is then required to properly archive and document the provenance of each figure, data table or statistical summary resulting in a complex structure of files and directories in the project folder thus increasing the odds of an irreproducible analysis.\nConfining all of the analysis to a scripting environment such as R can help, but this still does not alleviate the possibility of loading the wrong figure into the document, or forgetting to update a statistical summary in the text when the original data file was revised. A solution to this potential pitfall is to embed the actual analysis and graphic generation process into the document–such environments are called dynamic documents. In this course, we will use the R Markdown authoring tool which embeds R code into the document. An example of an R Markdown document is this course website which was entirely generated in RMarkdown! You can view the R Markdown files on this author’s GitHub repository."
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  EDA",
    "section": "",
    "text": "Tukey, John W. Exploratory Data Analysis. 1977. Addison-Wesley.↩︎\nGelman A. and Unwin A. Infovis and Statistical Graphics: Different Goals, Different Looks Journal of Computational and Graphical Statistics. Vol 22, no 1, 2013.↩︎\nBaggerly, Keith A. and Coombes, Kevin R. Deriving Chemosensitivity from Cell Lines: Forensic Bioinformatics and Reproducible Research in High-Throughput Biology. The Annals of Applied Statistics, vol.3, no.4, pp. 1309-1334. 2009.↩︎"
  },
  {
    "objectID": "The_R_environment.html#r-vs-rstudio",
    "href": "The_R_environment.html#r-vs-rstudio",
    "title": "2  The R and RStudio Environments",
    "section": "2.1 R vs RStudio",
    "text": "2.1 R vs RStudio\nR and RStudio are two distinctly different applications that serve different purposes. R1 is the software that performs the actual instructions. It’s the workhorse. Without R installed on your computer or server, you would not be able to run any commands.\nRStudio2 is a software that provides a nifty interface to R. It’s sometimes referred to as an Integrated Development Environment (IDE). Its purpose is to provide bells and whistles that can improve your experience with the R software.\n\nRStudio comes in two flavors:\n\nA desktop application that installs directly on your computer;\nA server application that is accessible via a web browser.\n\nBoth platforms offer nearly identical experiences. The former runs on top of R installed on your computer, the latter runs off of an instance of R running on a remote server.\n\n2.1.1 Do I need RStudio to interface with R?\nThe answer is No! Many new users to the R environment conflate R with RStudio. R has been around for decades, long before RStudio was developed. In fact, when you install R on your Windows or Mac computer, you are offered a perfectly functional barebones IDE for R.\n\nR can even be run in a shell environment like Linux:\n\nNote that while you do not need RStudio to run R on your computer, the reverse cannot be said. In other words, RStudio is not functional without an installation of R. You therefore need to install R regardless of whether or not you use RStudio.\n\n\n2.1.2 Which software do I cite?\nYou will normally cite R and not RStudio since RStudio does not contribute to the execution of the code (i.e. an R script will run independently of the version of RStudio or of any other IDE used to interface with R).\nYou can access citation information for R via:\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for\n  Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it when using it for data analysis.\nSee also 'citation(\"pkgname\")' for citing R packages."
  },
  {
    "objectID": "The_R_environment.html#command-line-vs.-script-file",
    "href": "The_R_environment.html#command-line-vs.-script-file",
    "title": "2  The R and RStudio Environments",
    "section": "2.2 Command line vs. script file",
    "text": "2.2 Command line vs. script file\n\n2.2.1 Command line\nR can be run from a R console or RStudio command line environment. For example, we can assign four numbers to the object x then have R read out the values stored in x:\n\nx &lt;- c(1,2,3,4)\nx\n\n[1] 1 2 3 4\n\n\n\n\n2.2.2 R script files\nIf you intend on typing more than a few lines of code in a command prompt environment, or if you wish to save a series of commands as part of a project’s analysis, it is probably best that you write and store the commands in an R script file. Such a file is usually saved with a .R extension.\nIn RStudio, you can run a line of code of a R script file by placing a cursor anywhere on that line (while being careful not to highlight any subset of that line) and pressing the shortcut keys Ctrl+Enter on a Windows keyboard or Command+Enter on a Mac.\nYou can also run an entire block of code by selecting all lines to be run, then pressing the shortcut keys Ctrl+Enter/Command+Enter. Alternatively, you can run the entire R script by pressing Ctrl+Alt+R in Windows or Command+Option+R on a Mac.\nIn the following example, the R script file has three lines of code: two assignment operations and one regression analysis. The lines are run one at a time using the Ctrl+Enter keys and the output is displayed in the console window."
  },
  {
    "objectID": "The_R_environment.html#the-assignment-operator--",
    "href": "The_R_environment.html#the-assignment-operator--",
    "title": "2  The R and RStudio Environments",
    "section": "2.3 The assignment operator <-",
    "text": "2.3 The assignment operator &lt;-\nWhen assigning values or output from operations to a variable, the assignment operator, &lt;-, is placed between the variable name and the value(s) being assigned to that variable. In the preceding example, the values 1,2,3,4 were being assigned to x. The assignment operator is constructed by combining the less then character, &lt;, with the dash character, -. Given that the assignment operator will be used frequently in an R script, it may be worthwhile to learn its shortcut: Alt+- on Windows and Option + - on a Mac.\nNote that, in most cases, you can also use the = to assign values as in:\nx = c(1,2,3,4)\nHowever, this option is not widely adopted in the R community. An advantage in using &lt;- instead of = is in readability. The &lt;- operator makes it easier to spot assignments during a quick visual scan of an R script, more so than the = operator which is also used in functions when assigning variables to function parameters as in:\nM &lt;- lm(y ~ x, data = dat, weights = wt)  \nThe alternative would be:\nM = lm(y ~ x, data = dat, weights = wt)   \nNotice how the assignment of M does not stand out as well in the second example given the recurrence of = on the same line of code (unless, of course, you benefit from colored syntax)."
  },
  {
    "objectID": "The_R_environment.html#understanding-directory-structures",
    "href": "The_R_environment.html#understanding-directory-structures",
    "title": "2  The R and RStudio Environments",
    "section": "2.4 Understanding directory structures",
    "text": "2.4 Understanding directory structures\nBecause a data file may reside in a different directory than that which houses the R script calling it, you need to explicitly instruct R on how to access that file from the R session’s working directory.\nIn the example that follows, user Jdoe has a project folder called Project1 in which reside a ./Data folder and an ./Analysis folder.\n\nHe opens the R script called Data_manipulation.R from the Analysis folder which contains the following line of code:\n\ndat &lt;- read.csv(\"ACS.csv\")\n\nHe runs that line of code and R returns the following error message:\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nThe error message states that the file ACS.csv cannot be found. This is because the session’s working directory is probably set to a directory other than the D:/Jdoe/Project1/Data directory which houses the data file. An R session’s working directory can be verified by typing the following command:\n\ngetwd()\n\n[1] \"D:/jdoe/Project1/Analysis\"\nThe working directory is used to instruct R where to look for a file (or where to create one) if the directory path is not explicitly defined. So in the above example, user Jdoe is asking R to open the file ACS.csv without explicitly telling R in which directory to look, so R is defaulting to the current working directory which is D:/jdoe/Project1/Analysis which does not contain the file ACS.csv.\nThere are two options to resolving this problem. The first is to set the working directory to the folder that contains the ACS.csv file using the setwd() function.\n\nsetwd(\"D:/Jdoe/Project1/Data\")\n\nThe second is to modify the read.csv call by specifying the path to the ACS.csv file.\n\ndat &lt;- read.csv(\"D:/Jdoe/Project1/Data/ACS.csv\")\n\nHowever, this approach makes it difficult to share the project folder with someone else who may choose to place the project folder under a different folder such as C:\\User\\John\\Documents\\Project1\\. In such a scenario, the user would need to modify every R script that references the directory D:\\Jdoe\\Project1\\. A better solution is to specify the location of the data folder relative to the location of the Analysis folder such as,\n\ndat &lt;- read.csv(\"../Data/ACS.csv\")\n\nThe two dots, .., tells R to move up the directory hierarchy relative to the current working directory. So in our working example, ../ tells R to move out of the Analysis/ folder and up into the Project1/ folder. The relative path ../Data/ACS.csv tells R to move out of the Analysis/ directory and over into the Data/ directory before attempting to read the contents of the ACS.csv data file.\nUsing relative paths makes your project folder independent of the full directory structure in which it resides thus facilitating the reproducibility of your work on a different computer or root directory environment. This assumes that the user of your code will set the working directory to the project folder location."
  },
  {
    "objectID": "The_R_environment.html#packages",
    "href": "The_R_environment.html#packages",
    "title": "2  The R and RStudio Environments",
    "section": "2.5 Packages",
    "text": "2.5 Packages\nOne of R’s attractive features is its rich collection of packages designed for specific applications and techniques. Packages allow researchers and scientists to share R functions and data with other users. Some packages come already installed with R, others must be downloaded separately from a CRAN repository or other locations such as GitHub or personal websites.\n\n2.5.1 Base packages\nR comes installed with a set of default packages. A snapshot of a subset of the installed base packages is shown below:"
  },
  {
    "objectID": "The_R_environment.html#installing-packages-from-cran",
    "href": "The_R_environment.html#installing-packages-from-cran",
    "title": "2  The R and RStudio Environments",
    "section": "2.6 Installing packages from CRAN",
    "text": "2.6 Installing packages from CRAN\nThere are thousands of R packages to choose from. Most can be accessed from the CRAN repository. To install a CRAN package from within RStudio, click on the Packages tab, select Install and choose Repository (CRAN) as the source location. In the following example, the library ggplot2 is installed from CRAN.\n\nPackage installation from CRAN’s repository can also be accomplished by typing the following line of code in a console:\n\ninstall.packages(\"ggplot2\")\n\nThe installation is usually straightforward and if any other packages need to be installed, R will install those as well as long as the Install dependencies option is checked. In the previous example, ggplot2 requires that a dozen or so packages be present on your computer (such as RColorBrewer and reshape2)–all of which are automatically installed by R.\nNote that R packages are installed in the user’s home directory by default. This is advantageous in that you do not need to have administrative privileges on the computer you are working on. But it can be a disadvantage in that if someone else logs onto the same computer where you installed a package, that person will not have access to it requiring that she install that package in her home directory thereby duplicating an instance of that same package on the same computer.\n\n2.6.1 Installing packages from GitHub\nSome packages may be in development and deemed not mature enough to reside on the CRAN repository. Such packages are often found on GitHub–a website that hosts software projects. Installing a package from GitHub requires the use of another package called devtools available on CRAN.\nFor example, to install the latest version of ggplot2 from GitHub (i.e. the developmental version and not the stable version available on CRAN) type the following:\n\ninstall.packages(\"devtools\")  # Install the devtools package if not already present\nlibrary(devtools)             # Load the devtools package in the current R session\ninstall_github(\"tidyverse/ggplot2\")\n\nThe argument tidyverse points to the name of the repository and ggplot2 to the name of the package.\n\n\n2.6.2 Using a package in a R session\nJust because a package is installed on your computer (in your home directory or in a directory accessible to you) does not necessarily mean that you have access to its functions at the beginning of an R. For example, after installing the ggplot2 library you might want to use one of its functions, ggplot, to generate a scatter plot,\n\nggplot(mtcars, aes(mpg, wt)) + geom_point()\n\nonly to see the following error message:\n\n\nError in ggplot(mtcars, aes(mpg, wt)): could not find function \"ggplot\"\n\n\nThis is because the contents of the ggplot2 package have not been loaded into the current R session. To make the functions and/or data of a package available to an existing R session, you must load its content using the library() function:\n\nlibrary(ggplot2)\n\nOnce the package is loaded in the current R session, you should have full access to its functions and datasets.\n\nggplot(mtcars, aes(mpg, wt)) + geom_point()"
  },
  {
    "objectID": "The_R_environment.html#getting-a-sessions-info",
    "href": "The_R_environment.html#getting-a-sessions-info",
    "title": "2  The R and RStudio Environments",
    "section": "2.7 Getting a session’s info",
    "text": "2.7 Getting a session’s info\nReproducibility is a fundamental idea behind an open source analysis environment such as R. So it’s only fitting that all aspects of your analysis environment be made available (along with your data and analysis results). This is because functions and programming environments may change in their behavior as versions evolve; this may be by design or the result of a bug in the code fixed in later versions. No piece of software, open-source or commercial, is immune to this. It’s therefore important that you publicize the R session used in your analysis along with any key packages critical to your work. A simple way to do this is to call the sessionInfo() function.\n\nsessionInfo() \n\nR version 4.3.2 (2023-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22621)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8  LC_CTYPE=English_United States.utf8    LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                           LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.4\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       cli_3.6.2         knitr_1.45        rlang_1.1.3       xfun_0.41         generics_0.1.3   \n [7] jsonlite_1.8.8    labeling_0.4.3    glue_1.7.0        colorspace_2.1-0  htmltools_0.5.7   scales_1.3.0     \n[13] fansi_1.0.6       rmarkdown_2.25    grid_4.3.2        evaluate_0.23     munsell_0.5.0     tibble_3.2.1     \n[19] fastmap_1.1.1     lifecycle_1.0.4   compiler_4.3.2    dplyr_1.1.4       codetools_0.2-19  htmlwidgets_1.6.4\n[25] pkgconfig_2.0.3   rstudioapi_0.15.0 farver_2.1.1      digest_0.6.34     R6_2.5.1          tidyselect_1.2.0 \n[31] utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3    withr_3.0.0       tools_4.3.2       gtable_0.3.4     \n\n\nOutput includes all loaded base packages and external packages (e.g. ggplot2 in this working example) as well as their version."
  },
  {
    "objectID": "The_R_environment.html#footnotes",
    "href": "The_R_environment.html#footnotes",
    "title": "2  The R and RStudio Environments",
    "section": "",
    "text": "R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/↩︎\nRStudio Team (2021). RStudio: Integrated Development for R. RStudio, PBC, Boston, MA URL http://www.rstudio.com/.↩︎"
  },
  {
    "objectID": "data_objects.html#core-data-types",
    "href": "data_objects.html#core-data-types",
    "title": "3  Data Object Type and Structure",
    "section": "3.1 Core data types",
    "text": "3.1 Core data types\nAn R object’s data type, or mode, defines how its values are stored in the computer. You can get an object’s mode using the typeof() function. Note that R also has a built-in mode() function that will serve the same purpose with the one exception in that it will not distinguish integers from doubles.\n\n3.1.1 Numeric\nThe numeric data type is probably the simplest. It consists of numbers such as integers (e.g. 1 ,-3 ,33 ,0) and doubles (e.g. 0.3, 12.4, -0.04, 1.0). For example, to create a numeric (double) vector we can type:\n\nx &lt;- c(1.0, -3.4, 2, 140.1)\nmode(x)\n\n[1] \"numeric\"\n\n\nTo assess if the number is stored as an integer or a double use the typeof() function.\n\ntypeof(x)\n\n[1] \"double\"\n\n\nNote that removing the fractional part of a number when creating a numeric object does not necessarily create an integer. For example, creating what seems to be an integer object returns double when queried by typeof():\n\nx &lt;- 4\ntypeof(x)\n\n[1] \"double\"\n\n\nTo force R to recognize a value as an integer add an upper case L to the number.\n\nx &lt;- 4L\ntypeof(x)\n\n[1] \"integer\"\n\n\n\n\n3.1.2 Character\nThe character data type consists of letters or words such as \"a\", \"f\", \"project\", \"house value\".\n\nx &lt;- c(\"a\", \"f\", \"project\", \"house value\")\ntypeof(x)\n\n[1] \"character\"\n\n\nCharacters can also consist of numbers represented as characters. The distinction between a character representation of a number and a numeric one is important. For example, if we have two numeric vectors x and y such as\n\nx &lt;- 3\ny &lt;- 5.3\n\nand we choose to sum the two variables, we get:\n\nx + y\n\n[1] 8.3\n\n\nIf we repeat these same steps but instead, choose to represent the numbers 3 and 5.3 as characters we get the following error message:\n\nx &lt;- \"3\"\ny &lt;- \"5.3\"\nx + y\n\nError in x + y: non-numeric argument to binary operator\n\n\nNote the use of quotes to force numbers to character mode.\n\n\n3.1.3 Logical\nLogical values can take on one of two values: TRUE or FALSE. These can also be represented as 1 or 0. For example, to create a logical vector of 4 elements, you can type\n\nx &lt;- c(TRUE, FALSE, FALSE, TRUE)\n\nor\n\nx &lt;- as.logical(c(1,0,0,1))\n\nNote that in both cases, typeof(x) returns logical. Also note that the 1’s and 0’s in the last example are converted to TRUE’s and FALSE’s internally."
  },
  {
    "objectID": "data_objects.html#naming-r-objects",
    "href": "data_objects.html#naming-r-objects",
    "title": "3  Data Object Type and Structure",
    "section": "3.2 Naming R objects",
    "text": "3.2 Naming R objects\nYou can use any combination of alphanumeric characters–including dots and underscore–to name an R object. But there are a few exceptions:\n\nNames cannot start with a number;\nNames cannot have spaces;\nNames cannot be a standalone number such as 12 or 0.34;\nNames cannot be a reserved word such as if, else, function, TRUE, FALSE and NULL just to name a few (to see the full list of reserved words, type ?Reserved).\n\nExamples of valid names include a, dat2, cpi_index, .tmp, and even a standalone dot . (though a dot can make reading code difficult under certain circumstances).\nExamples of invalid names include 1dat, dat 2 (note the space between dat and 2), df-ver2 (the dash is treated as a mathematical operator), and Inf (the latter is a reserved word listed in the ?Reserved help document).\nYou can mix cases, but use upper cases with caution since some letters look very much the same in both lower and upper cases (e.g. s and S)."
  },
  {
    "objectID": "data_objects.html#derived-data-types",
    "href": "data_objects.html#derived-data-types",
    "title": "3  Data Object Type and Structure",
    "section": "3.3 Derived data types",
    "text": "3.3 Derived data types\nWe’ve learned that data are stored as either numeric, character or logical, but they can carry additional attribute information that allow these objects to be treated in special ways by certain functions in R. These attributes define an object’s class and can be extracted from that object via the class() function.\n\n3.3.1 Factor\nFactors are normally used to group variables into a fixed number of unique categories or levels. For example, a dataset may be grouped by gender or month of the year. Such data are usually loaded into R as a numeric or character data type requiring that they be converted to a factor using the as.factor() function.\nIn the following chunk of code, we create a factor from a character object.\n\na      &lt;- c(\"M\", \"F\", \"F\", \"U\", \"F\", \"M\", \"M\", \"M\", \"F\", \"U\")\na.fact &lt;- as.factor(a)\n\nNote that a is of character data type.\n\ntypeof(a)\n\n[1] \"character\"\n\n\nHowever, the derived object a.fact is now stored as an integer!\n\ntypeof(a.fact)\n\n[1] \"integer\"\n\n\nYet, when displaying the contents of a.fact we see character values.\n\na.fact\n\n [1] M F F U F M M M F U\nLevels: F M U\n\n\nHow can this be? Well, a.fact is a more complicated object than the simple objects created thus far in that the factor is storing additional information not seen in its output. This hidden information is stored as an object attribute. To view these hidden attributes, use the attributes function.\n\nattributes(a.fact)\n\n$levels\n[1] \"F\" \"M\" \"U\"\n\n$class\n[1] \"factor\"\n\n\nThere are two attributes: levels and class. The levels attribute lists the three unique values in a.fact. The order in which these levels are listed reflect their numeric representation. So in essence, a.fact is storing each value as an integer that points to one of the three unique levels.\n\nSo why doesn’t R output the integer values when we output a.fact? To understand why, we first need to know that when we call the object name, R is passing that object name to the print function, so the following lines of code are identical.\n\na.fact\nprint(a.fact)\n\nThe print function then looks for a class attribute in the object. The class type instructs the print function on how to generate the output. Since a.fact has a factor class attribute, the print function is instructed to replace the integer values with the level “tags”.\nNaturally, this all happens behind the scenes without user intervention.\nAnother way to determine a.fact’s class type is to call the class function.\n\nclass(a.fact)\n\n[1] \"factor\"\n\n\nThe unique levels of a factor, and the order in which they are stored can be extracted using the levels function.\n\nlevels(a.fact)\n\n[1] \"F\" \"M\" \"U\"\n\n\nRemember, the order in which the levels are displayed match their integer representation.\nNote that if a class attribute is not present, the class function will return the object’s data type (though it will not distinguish between integer and double).\n\nclass(a)\n\n[1] \"character\"\n\n\nTo appreciate the benefits of a factor, we’ll first create a dataframe (dataframes are data tables whose structure will be covered later in this tutorial). One column will be assigned the a.fact factor and another will be assigned some random numeric values.\n\nx      &lt;- c(166, 47, 61, 148, 62, 123, 232, 98, 93, 110)\ndat    &lt;- data.frame(x = x, gender = a.fact)\ndat\n\n     x gender\n1  166      M\n2   47      F\n3   61      F\n4  148      U\n5   62      F\n6  123      M\n7  232      M\n8   98      M\n9   93      F\n10 110      U\n\n\nThe gender column is now a factor with three levels: F, M and U. We can use the str() function to view the dataframe’s structure as well as its columns classes.\n\nstr(dat)\n\n'data.frame':   10 obs. of  2 variables:\n $ x     : num  166 47 61 148 62 123 232 98 93 110\n $ gender: Factor w/ 3 levels \"F\",\"M\",\"U\": 2 1 1 3 1 2 2 2 1 3\n\n\nMany functions other than print will recognize factor data types and will allow you to split the output into groups defined by the factor’s unique levels. For example, to create three boxplots of the value x, one for each gender group F, M and U, type the following:\n\nboxplot(x ~ gender, dat, horizontal = TRUE)\n\n\n\n\nThe tilde ~ operator is used in the plot function to split (or condition) the data into separate plots based on the factor gender.\nFactors will prove to be quite useful in many analytical and graphical procedures as we’ll see in subsequent sections.\n\n3.3.1.1 Rearranging level order\nA factor will define a hierarchy for its levels. When we invoked the levels function in the last example, you may have noted that the levels output were ordered F, M andU–this is the level hierarchy defined for gender (i.e. F&gt;M&gt;U ). This means that regardless of the order in which the factors appear in a table, anytime a plot or operation is conditioned by the factor, the grouped elements will appear in the order defined by the levels’ hierarchy. When we created the boxplot from our dat object, the plotting function ordered the boxplot (bottom to top) following gender’s level hierarchy (i.e. F first, then M, then U).\nIf we wanted the boxplots to be plotted in a different order (i.e. U first followed by F then M) we would need to modify the gender column by recreating the factor object as follows:\n\ndat$gender &lt;- factor(dat$gender, levels=c(\"U\",\"F\",\"M\"))\nstr(dat)\n\n'data.frame':   10 obs. of  2 variables:\n $ x     : num  166 47 61 148 62 123 232 98 93 110\n $ gender: Factor w/ 3 levels \"U\",\"F\",\"M\": 3 2 2 1 2 3 3 3 2 1\n\n\nThe factor function is given the original factor values (dat$gender) but is also given the levels in the new order in which they are to appear(levels=c(\"U\",\"F\",\"M\")). Now, if we recreate the boxplot, the plot order (plotted from bottom to top) will reflect the new level hierarchy.\n\nboxplot(x ~ gender, dat, horizontal = TRUE)\n\n\n\n\n\n\n3.3.1.2 Subsetting table by level and dropping levels\nIn this example, we can subset the table by level using the subset function. For example, to subset the values associated with F and M, type:\n\ndat.f &lt;- subset(dat, gender == \"F\" | gender == \"M\")\ndat.f\n\n    x gender\n1 166      M\n2  47      F\n3  61      F\n5  62      F\n6 123      M\n7 232      M\n8  98      M\n9  93      F\n\n\nThe double equality sign == differs from the single equality sign = in that the former asses a condition: it checks if the variable to the left of == equals the variable to the right.\nHowever, if you display the levels associated with this new dataframe, you’ll still see the level U even though it no longer exists in the gender column.\n\nlevels(dat.f$gender)\n\n[1] \"U\" \"F\" \"M\"\n\n\nThis can be a nuisance when plotting the data subset.\n\nboxplot(x ~ gender, dat.f, horizontal = TRUE)\n\n\n\n\nEven though no records are available for U, the plot function allocates a slot for that level. To resolve this, we can use the droplevels function to remove all unused levels.\n\ndat.f$gender &lt;- droplevels(dat.f$gender)\nlevels(dat.f$gender)\n\n[1] \"F\" \"M\"\n\n\n\nboxplot(x ~ gender, dat.f, horizontal = TRUE)\n\n\n\n\n\n\n\n3.3.2 Date\nDate values are stored as numbers. But to be properly interpreted as a date object in R, their class attribute must be explicitly defined as a date. R provides many facilities to convert and manipulate dates and times, but a package called lubridate makes working with dates/times much easier. A separate chapter is dedicated to the creation and manipulation of date objects.\n\n\n3.3.3 NA and NULL\nYou will find that many data files contain missing or unknown values. It may be tempting to assign these missing or unknown values a 0 but doing so can lead to many undesirable results when analyzing the data. R has two placeholders for such elements: NA and NULL.\nFor example, let’s say that we made four measurements where the second measurement was not available but we wanted that missing value to be recorded in our table, we would encode that missing value as follows:\n\nx &lt;- c(23, NA, 1.2, 5)\n\nNA (Not Available) is a missing value indicator. It suggests that a value should be present but is unknown.\nThe NULL object also represents missing values but its interpretation is slightly different in that it suggests that the value does not exist or that it’s not measurable.\n\ny &lt;- c(23, NULL, 1.2, 5)\n\nThe difference between NA and NULL may seem subtle, but their interpretation in some functions can lead to different outcomes. For example, when computing the mean of x, R returns an NA value:\n\nmean(x)\n\n[1] NA\n\n\nThis serves as a check to remind the user that one of the elements is missing. This can be overcome by setting the na.rm parameter to TRUE as in mean(x, na.rm=TRUE) in which case R ignores the missing value.\nA NULL object, on the other hand, is treated differently. Since NULL implies that a value should not be present, R no longer feels the need to treat such element as questionable and allows the mean value to be computed:\n\nmean(y)\n\n[1] 9.733333\n\n\nIt’s more common to find data tables with missing elements populated with NA’s than NULL’s so unless you have a specific reason to use NULL as a missing value placeholder, use NA instead.\n\n3.3.3.1 NA data types\nNA has different data types. By default, it’s a logical variable.\n\ntypeof(NA)\n\n[1] \"logical\"\n\n\nBut it can be coerced to other types/classes such as character.\n\ntypeof( as.character(NA))\n\n[1] \"character\"\n\n\nIt can also be coerced to a derived data type such as a date.\n\nclass( as.Date(NA))\n\n[1] \"Date\"\n\n\nNote the use of class instead of typeof (recall that a date object is stored as a number but has a Date class attribute).\nAlternatively, you can make use of built-in reserved words such as NA_character_ and NA_integer_. Note that there is no reserved word for an NA date type.\n\ntypeof(NA_character_)\n\n[1] \"character\"\n\ntypeof(NA_integer_)\n\n[1] \"integer\"\n\n\nThe distinction between NA types can be important in certain settings. Examples of these will be highlighted in section 9.3."
  },
  {
    "objectID": "data_objects.html#data-structures",
    "href": "data_objects.html#data-structures",
    "title": "3  Data Object Type and Structure",
    "section": "3.4 Data structures",
    "text": "3.4 Data structures\nMost datasets we work with consist of batches of values such as a table of temperature values or a list of survey results. These batches are stored in R in one of several data structures. These include (atomic) vectors, matrices, data frames and lists.\n\n\n3.4.1 (Atomic) Vectors\nThe atomic vector (or vector for short) is the simplest data structure in R which consists of an ordered set of values of the same type and or class (e.g. numeric, character, date, etc…). A vector can be created using the combine function c() as in\n\nx &lt;- c(674 , 4186 , 5308 , 5083 , 6140 , 6381)\nx\n\n[1]  674 4186 5308 5083 6140 6381\n\n\nA vector object is an indexable collection of values which allows one to access a specific index number. For example, to access the third element of x, type:\n\nx[3]\n\n[1] 5308\n\n\nYou can also select a subset of elements by index values using the combine function c().\n\nx[c(1,3,4)]\n\n[1]  674 5308 5083\n\n\nOr, if you are interested in a range of indexed values such as index 2 through 4, use the : operator.\n\nx[2:4]\n\n[1] 4186 5308 5083\n\n\nYou can also assign new values to a specific index. For example, we can replace the second value in vector x with 0.\n\nx[2] &lt;- 0\nx\n\n[1]  674    0 5308 5083 6140 6381\n\n\nNote that a vector can store any data type such as characters.\n\nx &lt;- c(\"all\", \"b\", \"olive\")\nx\n\n[1] \"all\"   \"b\"     \"olive\"\n\n\nHowever, a vector can only be of one type. For example, you cannot mix numeric and character types as follows:\n\nx &lt;- c( 1.2, 5, \"Rt\", \"2000\")\n\n\n\n When data of different types are combined in a vector or in an operation, R will convert the element types to the highest common mode following the order:\n\n\nNULL &lt; logical &lt; integer &lt; double &lt; character.\n\n\n\n In our working example, the elements are coerced to character:\n\ntypeof(x)\n\n[1] \"character\"\n\n\n\n\n3.4.2 Matrices and arrays\nMatrices in R can be thought of as vectors indexed using two indices instead of one. For example, the following line of code creates a 3 by 3 matrix of randomly generated values. The parameters nrow and ncol define the matrix dimension and the function runif() generates the nine random numbers that populate this matrix.\n\nm &lt;- matrix(runif(9,0,10), nrow = 3, ncol = 3)\nm\n\n         [,1]      [,2]      [,3]\n[1,] 5.551420 4.5102526 0.7499725\n[2,] 1.955294 0.8549702 3.3189124\n[3,] 7.472038 7.6248636 7.8604781\n\n\nIf a higher dimension vector is desired, then use the array() function to generate the n-dimensional object. A 3x3x3 array can be created as follows:\n\nm &lt;- array(runif(27,0,10), c(3,3,3))\nm\n\n, , 1\n\n         [,1]     [,2]     [,3]\n[1,] 2.544256 7.588527 2.943160\n[2,] 5.322216 7.232504 3.844889\n[3,] 1.240625 7.446650 5.622106\n\n, , 2\n\n         [,1]     [,2]     [,3]\n[1,] 9.772901 8.789717 5.280044\n[2,] 8.648934 3.657340 9.861641\n[3,] 5.719890 7.418222 3.689159\n\n, , 3\n\n          [,1]      [,2]     [,3]\n[1,] 0.8652384 8.0297861 7.643928\n[2,] 2.5009260 0.5705866 4.630734\n[3,] 2.2500430 2.3286809 4.322838\n\n\nMatrices and arrays can store numeric or character data types, but they cannot store both. This is not to say that you can’t have a matrix of the kind\n\n\n     [,1] [,2]\n[1,] \"a\"  \"2\" \n[2,] \"b\"  \"4\" \n\n\nbut the value 2 and 4 are no longer treated as numeric values but as character values instead.\n\n\n3.4.3 Data frames\nA data frame is what comes closest to our perception of a traditional data table. Unlike a matrix, a data frame can mix data types across columns (e.g. both numeric and character columns can coexist in a data frame) but data type remains the same down each column.\n\nname   &lt;- c(\"a1\", \"a2\", \"b3\")\nvalue1 &lt;- c(23, 4, 12)\nvalue2 &lt;- c(1, 45, 5)\ndat    &lt;- data.frame(name, value1, value2)\ndat\n\n  name value1 value2\n1   a1     23      1\n2   a2      4     45\n3   b3     12      5\n\n\nTo view each column’s data type use the structure str function.\n\nstr(dat)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"a1\" \"a2\" \"b3\"\n $ value1: num  23 4 12\n $ value2: num  1 45 5\n\n\nYou’ll notice that the value1 and value2 columns are stored as numeric (i.e. as doubles) and not as integer. There is some inconsistency in R’s characterization of data type. Here, numeric represents double whereas an integer datatype would display integer. For example:\n\nvalue2 &lt;- c(1L, 45L, 5L)\ndat    &lt;- data.frame(name, value1, value2)\nstr(dat)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"a1\" \"a2\" \"b3\"\n $ value1: num  23 4 12\n $ value2: int  1 45 5\n\n\nLike a vector, elements of a data frame can be accessed by their index (aka subscripts). The first index represents the row number and the second index represents the column number. For example, to list the second row of the third column, type:\n\ndat[2, 3]\n\n[1] 45\n\n\nIf you wish to list all rows for columns one through two, leave the first index blank:\n\ndat[ , 1:2 ]\n\n  name value1\n1   a1     23\n2   a2      4\n3   b3     12\n\n\nor, if you wish to list the third row for all columns, leave the second index blank:\n\ndat[ 3 , ]\n\n  name value1 value2\n3   b3     12      5\n\n\nYou can also reference columns by their names if you append the $ character to the dataframe object name. For example, to list the values in the column named value2, type:\n\ndat$value2\n\n[1]  1 45  5\n\n\nDataframes also have attributes:\n\nattributes(dat)\n\n$names\n[1] \"name\"   \"value1\" \"value2\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n\nIt’s class is data.frame. The names attribute lists the column names and can be extracted using the names function.\n\nnames(dat)\n\n[1] \"name\"   \"value1\" \"value2\"\n\n\nThe dataframe also has a row.names attribute. Since we did not explicitly define row names, R simply assigned the row number as row names. You can extract the row names using the rownames function.\n\nrownames(dat)\n\n[1] \"1\" \"2\" \"3\"\n\n\nFinally, to get the dimensions of a dataframe (or a matrix), use the dim() function.\n\ndim(dat)\n\n[1] 3 3\n\n\nThe first value returned by the function represents the number of rows (3 rows), the second value returned by the function represents the number of columns (3 columns).\n\n\n3.4.4 Lists\nA list is an ordered set of components stored in a 1D vector. In fact, it’s another kind of vector called a recursive vector where each vector element can be of different data type and structure. This implies that each element of a list can hold complex objects such as matrices, data frames and other list objects too! Think of a list as a single column spreadsheet where each cell stores anything from a number, to a three paragraph sentence, to a five column table.\nA list is constructed using the list() function. For example, the following list consists of 3 components: a two-column data frame (tagged as component A), a two element logical vector (tagged as component B) and a three element character vector (tagged as component D).\n\nA &lt;- data.frame(\n     x = c(7.3, 29.4, 29.4, 2.9, 12.3, 7.5, 36.0, 4.8, 18.8, 4.2),\n     y = c(5.2, 26.6, 31.2, 2.2, 13.8, 7.8, 35.2, 8.6, 20.3, 1.1) )\nB &lt;- c(TRUE, FALSE)\nD &lt;- c(\"apples\", \"oranges\", \"round\")\n\nlst &lt;- list(A = A, B = B, D = D)\n\nYou can view each component’s structure using the str() function.\n\nstr(lst)\n\nList of 3\n $ A:'data.frame':  10 obs. of  2 variables:\n  ..$ x: num [1:10] 7.3 29.4 29.4 2.9 12.3 7.5 36 4.8 18.8 4.2\n  ..$ y: num [1:10] 5.2 26.6 31.2 2.2 13.8 7.8 35.2 8.6 20.3 1.1\n $ B: logi [1:2] TRUE FALSE\n $ D: chr [1:3] \"apples\" \"oranges\" \"round\"\n\n\nEach component of a list can be extracted using the $ symbol followed by that component’s name. For example, to access component A from list lst, type:\n\nlst$A\n\n      x    y\n1   7.3  5.2\n2  29.4 26.6\n3  29.4 31.2\n4   2.9  2.2\n5  12.3 13.8\n6   7.5  7.8\n7  36.0 35.2\n8   4.8  8.6\n9  18.8 20.3\n10  4.2  1.1\n\n\nYou can also access that same component using its numerical index. Since A is the first component in lst, its numerical index is 1.\n\nlst[[1]]\n\n      x    y\n1   7.3  5.2\n2  29.4 26.6\n3  29.4 31.2\n4   2.9  2.2\n5  12.3 13.8\n6   7.5  7.8\n7  36.0 35.2\n8   4.8  8.6\n9  18.8 20.3\n10  4.2  1.1\n\n\nNote that we are using double brackets to extract A. In doing so, we are extracting A in its native data format (a data frame in this example). Had we used single brackets, A would have been extracted as a single component list regardless of its native format. The following compares the different data structure outputs between single and double bracketed indices:\n\nclass(lst[[1]])\n\n[1] \"data.frame\"\n\nclass(lst[1])\n\n[1] \"list\"\n\n\nTo list the names for each component in a list use the names() function:\n\nnames(lst)\n\n[1] \"A\" \"B\" \"D\"\n\n\nNote that components do not require names. For example, we could have created a list as follows (note the omission of A=, B=, etc…):\n\nlst.notags &lt;- list(A, B, D)\n\nListing its contents displays bracketed indices instead of component names.\n\nlst.notags\n\n[[1]]\n      x    y\n1   7.3  5.2\n2  29.4 26.6\n3  29.4 31.2\n4   2.9  2.2\n5  12.3 13.8\n6   7.5  7.8\n7  36.0 35.2\n8   4.8  8.6\n9  18.8 20.3\n10  4.2  1.1\n\n[[2]]\n[1]  TRUE FALSE\n\n[[3]]\n[1] \"apples\"  \"oranges\" \"round\"  \n\n\nWhen lists do not have component names, the names() function will return NULL.\n\nnames(lst.notags)\n\nNULL\n\n\nIt’s usually good practice to assign names to components as these can provide meaningful descriptions of each component.\nYou’ll find that many functions in R return list objects such as the linear regression model function lm. For example, run a regression analysis for vector elements x and y (in data frame A) and save the output of the regression analysis to an object called M:\n\nM &lt;- lm( y ~ x, A)\n\nNow let’s verify M’s data structure. The following shows just the first few lines of the output.\n\nstr(M)\n\n\n\nList of 12\n $ coefficients : Named num [1:2] 0.0779 0.991\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"x\"\n  [list output truncated]\n - attr(*, \"class\")= chr \"lm\"\n...\n\n\nThe contents of the regression model object M consists of 12 components which include various diagnostic statistics, regression coefficients, and residuals. Let’s extract each component’s name:\n\nnames(M)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"          \"fitted.values\" \"assign\"        \"qr\"           \n [8] \"df.residual\"   \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nFortunately, the regression function assigns descriptive names to each of its components making it easier to figure out what most of these components represent. For example, it’s clear that the residuals component stores the residual values from the regression model.\n\nM$residuals\n\n         1          2          3          4          5          6          7          8          9         10 \n-2.1119291 -2.6122265  1.9877735 -0.7516888  1.5332525  0.2898782 -0.5525869  3.7654802  1.5919885 -3.1399416 \n\n\nThe M list is more complex than the simple list lst we created earlier. In addition to having more components, it stores a wider range of data types and structures. For example, element qr is itself a list!\n\nstr(M$qr)\n\nList of 5\n $ qr   : num [1:10, 1:2] -3.162 0.316 0.316 0.316 0.316 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:10] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:2] \"(Intercept)\" \"x\"\n  ..- attr(*, \"assign\")= int [1:2] 0 1\n $ qraux: num [1:2] 1.32 1.44\n $ pivot: int [1:2] 1 2\n $ tol  : num 0.0000001\n $ rank : int 2\n - attr(*, \"class\")= chr \"qr\"\n\n\nSo, if we want to access the element rank in the component qr of list M, we can type:\n\nM$qr$rank\n\n[1] 2\n\n\nIf we want to access rank using indices instead, and noting that qr is the 7th component in list M and that rank is the 5th element in list qr we type:\n\nM[[7]][[5]]\n\n[1] 2\n\n\nThis should illustrate the value in assigning names to list components; not only do the double brackets clutter the expression, but finding the element numbers can be daunting in a complex list structure."
  },
  {
    "objectID": "data_objects.html#coercing-data-from-one-type-to-another",
    "href": "data_objects.html#coercing-data-from-one-type-to-another",
    "title": "3  Data Object Type and Structure",
    "section": "3.5 Coercing data from one type to another",
    "text": "3.5 Coercing data from one type to another\nData can be coerced from one type to another. For example, to coerce the following vector object from character to numeric, use the as.numeric() function.\n\ny   &lt;- c(\"23.8\", \"6\", \"100.01\",\"6\")\ny.c &lt;- as.numeric(y)\ny.c\n\n[1]  23.80   6.00 100.01   6.00\n\n\nThe as.numeric function forces the vector to a double (you could have also used the as.double function). If you convert y to an integer, R will remove all fractional parts of the number.\n\nas.integer(y)\n\n[1]  23   6 100   6\n\n\nTo convert a number to a character use as.character().\n\nnumchar &lt;- as.character(y.c)\nnumchar\n\n[1] \"23.8\"   \"6\"      \"100.01\" \"6\"     \n\n\nYou can also coerce a number or character to a factor.\n\nnumfac &lt;- as.factor(y)\nnumfac\n\n[1] 23.8   6      100.01 6     \nLevels: 100.01 23.8 6\n\ncharfac &lt;- as.factor(y.c)\ncharfac\n\n[1] 23.8   6      100.01 6     \nLevels: 6 23.8 100.01\n\n\nThere are many other coercion functions in R, a summary of some the most common ones we’ll be using in this course follows:\n\n\n\n\n\n\n\nas.character()\nConvert to character\n\n\nas.numeric() or as.double()\nConvert to double\n\n\nas.integer()\nConvert to integer\n\n\nas.factor()\nConvert to factor\n\n\nas.logical()\nConvert to a Boolean\n\n\n\n\n3.5.1 A word of caution when converting from factors\nIf you need to coerce a factor whose levels represent characters to a character data type, use the as.character() function.\n\nchar &lt;- as.character(charfac)\nclass(char)\n\n[1] \"character\"\n\n\nNumbers stored as factors can also be coerced back to numbers, but be careful, the following will not produce the expected output:\n\nnum &lt;- as.numeric(numfac)\nnum\n\n[1] 2 3 1 3\n\n\nThe output does not look like a numeric representation of the original elements in y. Instead it lists the integers that point to the set of unique levels (see the earlier section on factors). To see the unique levels in numfac, type:\n\nlevels(numfac)\n\n[1] \"100.01\" \"23.8\"   \"6\"     \n\n\nThere are three unique values in our vector. Recall that the order in which the levels appear in the above output represents the ordered pointer values. So 1 points to level 100.01, 2 points to level 23.8, and 3 points to level 6.\nSo, to extract the actual values (and not the pointers), you must first convert the factor to character before converting to a numeric vector.\n\nnum &lt;- as.numeric(as.character(numfac))\nnum\n\n[1]  23.80   6.00 100.01   6.00"
  },
  {
    "objectID": "data_objects.html#adding-metadata-to-objects",
    "href": "data_objects.html#adding-metadata-to-objects",
    "title": "3  Data Object Type and Structure",
    "section": "3.6 Adding metadata to objects",
    "text": "3.6 Adding metadata to objects\nEarlier, you were introduced to object attributes. Attributes can be thought of as ancillary information attached to a data object’s header. In other words, it is not part of the set of values stored in the object. If one or more attributes is/are present in the data object, they can be revealed using the attributes command. We already explored the attributes associated with a factor object. For example. a.fact has two attributes: levels and class.\n\nattributes(a.fact)\n\n$levels\n[1] \"F\" \"M\" \"U\"\n\n$class\n[1] \"factor\"\n\n\nIf an attribute is not explicitly defined for a data object, then a NULL is returned.\n\nattributes(a)\n\nNULL\n\n\nYou can create your own attribute name, but it’s best to avoid attribute names commonly used in R such as class, dim, dimnames, names, row.names.\nAn object’s attribute is usually a part of a data object that a user does not need to explictly deal with, but it can be leveraged to store metadata (data documentation). R has a reserved attribute name, comment, that can be used for this purpose. You can add this attribute using the attr function.\n\nattr(a, \"comment\")  &lt;- \"a batch of character values\"\n\nNote that R has a special function for the comment attribute which can be used to extract the comment attribute value.\n\ncomment(a) \n\n[1] \"a batch of character values\"\n\n\nYou can also use the comment function to set the comment value.\n\ncomment(a) &lt;- \"comment added using the comment function\"\ncomment(a) \n\n[1] \"comment added using the comment function\"\n\n\nYou can, of course, create your own attribute. For example, to create an attribute named dim.units and assigning it the value \"meters\" to the object y.c created earlier in this tutorial, type:\n\nattr(y.c, \"dim.units\") &lt;-  \"meters\"\n\nNote that many R packages will also implement their own set of attribute names. So you should familiarize yourself with the packages used in a worklfow before defining your own attribute name."
  },
  {
    "objectID": "read_write_files.html#reading-data-files-into-r",
    "href": "read_write_files.html#reading-data-files-into-r",
    "title": "4  Reading and Writing Data Files",
    "section": "4.1 Reading data files into R",
    "text": "4.1 Reading data files into R\nData files can be loaded from the R session’s working directory, from a directory structure relative to the working directory using the single dot . or double dot .. syntax, or (for some file types) directly from a website. The following sections will expose you to a mixture of data file environments. For a refresher on directory structures, review Understanding directory structures.\n\n4.1.1 Reading from a comma delimitted (CSV) file\nA popular data file format (and one that has withstood the test of time) is the text file format where columns are separated by a tab, space or comma. In the following example, R reads a comma delimited file called ACS.csv into a data object called dat.\n\ndat &lt;- read.csv(\"ACS.csv\", header=TRUE)\n\nIf the CSV file resides on a website, you can load the file directly from that site as follows:\n\ndat &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/ACS.csv\", header=TRUE)\n\nNote that not all data file formats can be readily loaded directly from a website in a “read” function without additional lines of code. Examples are given in the next two sub-sections.\nTo read other text formats that use different delimiters, invoke the command read.table() and define the type of delimiter using the sep= parameter. For example, to read a tab delimited data file called ACS.txt, run the command read.table(\"ACS.txt\", sep=\"\\t\").\nNote that if a number or a string is identified as being a placeholder for missing values in the data file, you can use the na.strings = parameter in the read.csv function. For example, assume that the word \"missing\" was used in the csv file to denote a missing value, the function would be modified as follows:\n\ndat &lt;- read.csv(\"ACS.csv\", na.strings = \"missing\")\n\nIf more than one value is used as a placeholder for a missing value, you will need to combine the values using the c() operator. For example, if in addition to the word \"missing\" the value of -9999 was used to designate missing values, you would modify the above chunk of code as follows:\n\ndat &lt;- read.csv(\"ACS.csv\", na.strings = c(\"missing\", \"-9999\") )\n\nNote how the number is wrapped in double quotes. Also, note that the na.strings parameter is applied to all columns in the dataframe. So if the word \"missing\" or the number -9999 are valid values for some of the columns, you should not use this option. Instead, you would need to selectively replace the missing values after the dataset is loaded. You will learn how to replace values in a dataframe in subsequent chapters.\n\n\n4.1.2 Reading from an R data file\nR has its own data file format–it’s usually saved using the .rds extension. To read an R data file, invoke the readRDS() function.\n\ndat &lt;- readRDS(\"ACS.rds\")\n\nAs with a CSV file, you can load an .rds file straight from a website, however, you must first run the file through a decompressor before attempting to load it via readRDS. A built-in decompressor function called gzcon can be used for this purpose.\n\ndat &lt;- readRDS(gzcon(url(\"http://mgimond.github.io/ES218/Data/ACS.rds\")))\n\nThe .rds file format is usually smaller than its text file counterpart and will therefore take up less storage space. The .rds file will also preserve data types and classes such as factors and dates eliminating the need to redefine data types after loading the file.\n\n\n4.1.3 Reading from an Excel file\nA package that does a good job in importing Excel files is readxl. It recognizes most column formats defined by Excel including date formats. However, only one sheet can be loaded at a time. So, if multiple Excel sheets are to be worked on, each sheet will need to be loaded into separate dataframe objects.\nIf you don’t have the readxl package installed, install the package as you would any other package via RStudio’s interface or in R using the following command:\n\ninstall.packages(\"readxl\")\n\nIn this example, we will load an Excel data sheet called Discharge which tabulates daily river water discharge. The sample file, Discharge_2004_2014.xlsx, can be downloaded here.\n\nlibrary(readxl)\nxl &lt;- read_excel(\"Discharge_2004_2014.xlsx\", sheet = \"Discharge\")\n\nAn advantage to using this package for loading Excel files is its ability to preserve data types–including date formatted columns! In the above example, the Excel file has a column called Date which stores the month/day/year data as a date object. We can check that the loaded xl object recognizes the Date column as a date data type:\n\nstr(xl)\n\ntibble [3,866 × 3] (S3: tbl_df/tbl/data.frame)\n $ Date     : POSIXct[1:3866], format: \"2004-06-01\" \"2004-06-02\" \"2004-06-03\" \"2004-06-04\" ...\n $ Discharge: num [1:3866] 6170 6590 6210 7120 6990 6160 5570 4500 4940 4550 ...\n $ Code     : chr [1:3866] \"A\" \"A\" \"A\" \"A\" ...\n\n\nThe Date column is defined as a POSIXct data type; this is the computer’s way of storing dates as the number of seconds since some internal reference date. We would therefore not need to convert the date column as would be the case if the date column was loaded from a CSV file. If such was the case, then the date column would most likely be loaded as a character or factor data type. A more in-depth discussion on date objects and their manipulation in R is covered in the next chapter.\nExcel files can be loaded directly from the web using the following code chunk:\n\nweb.file &lt;- \"http://mgimond.github.io/ES218/Data/Discharge_2004_2014.xlsx\"\ntmp      &lt;- tempfile(fileext=\".xlsx\")\ndownload.file(web.file,destfile=tmp, mode=\"wb\")\nxl       &lt;-  read_excel(tmp, sheet = \"Discharge\")\n\nInstead of downloading the file into virtual memory, R needs to download the file into a temporary folder before it can open it. However, that temporary file my not be available in a later session, so you will probably need to reload the data if you launch a new R session.\n\n\n4.1.4 Importing data from proprietary data file formats\nIt’s usually recommended that a data file be stored as a CSV or tab delimited file format if compatibility across software platforms is desired. However, you might find yourself in a situation where you have no option but to import data stored in a proprietary format. This requires the use (and installation) of a package called Hmisc. The package will convert the following file formats: SAS (XPT format), SPSS (SAV format) and Stata (dta format). You can install the package on your computer as follows:\n\ninstall.packages(\"Hmisc\")\n\nIn this example, a SAS file of blood pressure from the CDC will be loaded into an object called dat (file documentation can be found here). You can donwload the file here.\n\nlibrary(Hmisc)\ndat &lt;- sasxport.get(\"BPX_J.xpt\")\n\nLikewise, to import an SPSS file, use the spss.get() function; and to import a STATA file, use the stata.get() function."
  },
  {
    "objectID": "read_write_files.html#how-to-save-r-objects-to-data-files",
    "href": "read_write_files.html#how-to-save-r-objects-to-data-files",
    "title": "4  Reading and Writing Data Files",
    "section": "4.2 How to save R objects to data files",
    "text": "4.2 How to save R objects to data files\n  \n\n4.2.1 Export to a CSV file\nTo export a data object called dat.sub as a comma delimited file, run the following:\n\nwrite.csv(dat.sub, \"ACS_sub.csv\")\n\n\n\n4.2.2 Export to a .rds file\nTo export a data object called dat.sub to an R native .rds file format, run the following:\n\nsaveRDS(dat.sub, \"ACS_sub.rds\")"
  },
  {
    "objectID": "read_write_files.html#saving-an-r-session",
    "href": "read_write_files.html#saving-an-r-session",
    "title": "4  Reading and Writing Data Files",
    "section": "4.3 Saving an R session",
    "text": "4.3 Saving an R session\nYou can save an entire R session (which includes all data objects) using the save function.\nTo save all objects, set the list= parameter to ls():\n\nsave(list = ls(), file = \"ACS_all.Rdata\")\n\nTo save only two R session objects–dat and dat.sub–to a file, pass the list of objects to the list= parameter:\n\nsave(list = c(dat, dat.sub), file = \"ACS_subset.Rdata\")"
  },
  {
    "objectID": "read_write_files.html#loading-an-r-session",
    "href": "read_write_files.html#loading-an-r-session",
    "title": "4  Reading and Writing Data Files",
    "section": "4.4 Loading an R session",
    "text": "4.4 Loading an R session\nTo load a previously saved R session type:\n\nload(\"ACS_all.Rdata\")"
  },
  {
    "objectID": "date_objects.html#creating-datetime-objects",
    "href": "date_objects.html#creating-datetime-objects",
    "title": "5  Working with Dates",
    "section": "5.1 Creating date/time objects",
    "text": "5.1 Creating date/time objects\n  \n\n5.1.1 From complete date strings\nYou can convert many representations of date and time to date objects. For example, let’s create a vector of dates represented as month/day/year character strings,\n\nx &lt;- c(\"06/23/2013\", \"06/30/2013\", \"07/12/2014\")\nclass(x)\n\n[1] \"character\"\n\n\nAt this point, R treats the vector x as characters. To force R to interpret these as dates, use lubridate’s mdy function. mdy will convert date strings where the date elements are ordered as month, day and year.\n\nlibrary(lubridate)\nx.date &lt;- mdy(x)\nclass(x.date)\n\n[1] \"Date\"\n\n\nNote that using the mode or typeof functions will not help us determine if the object is an R date object. This is because a date is stored as a numeric (double) internally. Use the class function instead as shown in the above code chunk.\nIf you need to specify the time zone, add the parameter tz=. For example, to specify Eastern Standard Time, type:\n\nx.date &lt;- mdy(x, tz=\"EST\")\nx.date\n\n[1] \"2013-06-23 EST\" \"2013-06-30 EST\" \"2014-07-12 EST\"\n\n\nThe mdy function can read in date formats that use different delimiters so that mdy(\"06/23/2013\"), mdy(\"06-23-2013\") and mdy(\"06.23.2013\") are parsed exactly the same so long as the order remains month/day/year.\nFor different month/day/year arrangements, other lubridate functions need to be used:\n\n\n\nFunctions\nDate Format\n\n\n\n\ndmy()\nday/month/year\n\n\nymd()\nyear/month/day\n\n\nydm()\nyear/day/month\n\n\n\nIf your data contains both date and time in a “month/day/year hour:minutes:seconds” format use the mdy_hms function.\n\nx &lt;- c(\"06/23/2013 03:45:23\", \"07/30/2013 14:23:00\", \"08/12/2014 18:01:59\")\nx.date &lt;- mdy_hms(x, tz=\"EST\")\nx.date\n\n[1] \"2013-06-23 03:45:23 EST\" \"2013-07-30 14:23:00 EST\" \"2014-08-12 18:01:59 EST\"\n\n\nThe characters _h, _hm or _hms can be appended to any of the four date function names described earlier to accommodate time formats. A few examples follow:\n\nmdy_h(\"6/23/2013 3\", tz=\"EST\") \n\n[1] \"2013-06-23 03:00:00 EST\"\n\ndmy_hm(\"23/6/2013 3:15\", tz=\"EST\")\n\n[1] \"2013-06-23 03:15:00 EST\"\n\nymd_hms(\"2013/06/23 3:15:7\", tz=\"EST\")\n\n[1] \"2013-06-23 03:15:07 EST\"\n\n\nNote that adding a time element to the date object will create POSIXct and POSIXt object classes instead of Date object classes.\n\nclass(x.date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nAlso, if a timezone is not explicitly defined for a time based date, the function assigns UTC ( Universal Coordinated Time).\n\ndmy_hm(\"23/6/2013 3:15\")\n\n[1] \"2013-06-23 03:15:00 UTC\"\n\n\n\n\n5.1.2 Setting and modifying timezones\nR does not maintain its own list of timezone names, instead, it relies on the operating system’s naming convention. To list the supported timezone names for your particular R environment, type:\n\nOlsonNames()\n\nFor example, to select Daylight Savings Time type tz = \"EST5EDT\".\n\nx.date &lt;- mdy_hms(x, tz=\"EST5EDT\")\nx.date\n\n[1] \"2013-06-23 03:45:23 EDT\" \"2013-07-30 14:23:00 EDT\" \"2014-08-12 18:01:59 EDT\"\n\n\n\nclass(x.date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nIf you need to convert the day/time to another timezone, use lubridate’s with_tz() function. For example, to convert x.date from it’s current EST5DST timezone to the US/Alaska time zone, type:\n\nwith_tz(x.date, tzone = \"US/Alaska\") \n\n[1] \"2013-06-22 23:45:23 AKDT\" \"2013-07-30 10:23:00 AKDT\" \"2014-08-12 14:01:59 AKDT\"\n\n\nNote that the with_tz function will change the timestamp to reflect the new time zone. If you simply want to change the time zone definition and not the timestamp, use the tz() function.\n\ntz(x.date) &lt;- \"US/Alaska\"\nx.date\n\n[1] \"2013-06-23 03:45:23 AKDT\" \"2013-07-30 14:23:00 AKDT\" \"2014-08-12 18:01:59 AKDT\"\n\n\n\n\n5.1.3 From separate date elements\nIf your data table splits the date elements into separate vector objects or columns, use the paste function to combine the elements into a single date string before passing it to one of the lubridate functions. Let’s look at an example:\n\ndat1 &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/CO2.csv\")\nhead(dat1)\n\n  Year Month Average Interpolated  Trend Daily_mean\n1 1959     1  315.62       315.62 315.70         -1\n2 1959     2  316.38       316.38 315.88         -1\n3 1959     3  316.71       316.71 315.62         -1\n4 1959     4  317.72       317.72 315.56         -1\n5 1959     5  318.29       318.29 315.50         -1\n6 1959     6  318.15       318.15 315.92         -1\n\n\nThe CO2 dataset has the date split across two columns: Year and Month (both stored as integers). You can combine the columns into a character string using the paste function. For example, if we want to create a “Year-Month” string as in 1959-10, we could type:\n\npaste(dat1$Year,dat1$Month, sep=\"-\")\n\nThe above example uses three arguments: the two objects that are pasted together (i.e. Year and Month) and the sep=\"-\" parameter which fills the gap between both objects with a dash - (by default, paste would have added spaces thus creating strings in the form of 1959 10).\nlubridate does not have a function along the lines of ym to convert just the year-month strings, this requires that we add an artificial day of the month to the string. We’ll choose to add the 15th day of the month as in\n\npaste(dat1$Year, dat1$Month, \"15\", sep=\"-\")\n\nWe are now ready to add a new column called Date to the dat object and fill that column with a real date object:\n\ndat1$Date &lt;- ymd( paste(dat1$Year, dat1$Month, \"15\", sep=\"-\") )\nhead(dat1)\n\n  Year Month Average Interpolated  Trend Daily_mean       Date\n1 1959     1  315.62       315.62 315.70         -1 1959-01-15\n2 1959     2  316.38       316.38 315.88         -1 1959-02-15\n3 1959     3  316.71       316.71 315.62         -1 1959-03-15\n4 1959     4  317.72       317.72 315.56         -1 1959-04-15\n5 1959     5  318.29       318.29 315.50         -1 1959-05-15\n6 1959     6  318.15       318.15 315.92         -1 1959-06-15\n\n\nThe sep=\"-\" option is not needed with the lubridate function since lubridate will recognize a space as a valid delimiter, so the last piece of code could have been written as:\n\ndat1$Date &lt;- ymd( paste(dat1$Year, dat1$Month, \"15\") )\n\nTo confirm that the Date column is indeed formatted as a date object type:\n\nstr(dat1)\n\n'data.frame':   721 obs. of  7 variables:\n $ Year        : int  1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 ...\n $ Month       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Average     : num  316 316 317 318 318 ...\n $ Interpolated: num  316 316 317 318 318 ...\n $ Trend       : num  316 316 316 316 316 ...\n $ Daily_mean  : int  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ Date        : Date, format: \"1959-01-15\" \"1959-02-15\" \"1959-03-15\" \"1959-04-15\" ...\n\n\nor you could type,\n\nclass(dat1$Date)\n\n[1] \"Date\"\n\n\nSince we did not add a timezone or a time component to the date object the Date column was assigned a Date class as opposed to the POSIX... class.\n\n\n5.1.4 Padding time values\nThe lubridate functions may expect the time values to consist of a specific number of characters if a delimiter such as : is not present to split the time elements. For example, the following will not generate a valide date/time object:\n\nhrmin &lt;- 712           # Time 7:12\ndate  &lt;- \"2018/03/17\"  # Date \nymd_hm(paste(date, hrmin))\n\n[1] NA\n\n\nOne solution is to pad the time element with 0’s to complete a four character vector (or a six character vector if seconds are part of the time element). We can use the str_pad function from the stringr package to pad the time object (the stringr package is covered in chapter 6).\n\nlibrary(stringr)\nymd_hm(paste(date, str_pad(hrmin, width=4, pad=\"0\")))\n\n[1] \"2018-03-17 07:12:00 UTC\""
  },
  {
    "objectID": "date_objects.html#extracting-date-information",
    "href": "date_objects.html#extracting-date-information",
    "title": "5  Working with Dates",
    "section": "5.2 Extracting date information",
    "text": "5.2 Extracting date information\nIf you want to extract the day of the week from a date vector, use the wday function.\n\nwday(x.date) \n\n[1] 1 3 3\n\n\nIf you want the day of the week displayed as its three letter designation, add the label=TRUE parameter.\n\nwday(x.date, label=TRUE) \n\n[1] Sun Tue Tue\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\nYou’ll note that the function returns a factor object with seven levels–one for each day of the week (Sun, Mon, Tue, Wed, Thu, Fri, Sat)–as well as the level hierarchy which will dictate the order in which values will be displayed if grouped by this factor. The levels are not necessarily reflected in the vector elements (only Sun, Tue are present), but the levels are there if we were ever to add additional day elements to this vector.\nThe following table lists lubridate functions that extract different elements of a date object.\n\n\n\nFunctions\nExtracted element\n\n\n\n\nhour()\nHour of the day\n\n\nminute()\nMinute of the hour\n\n\nday()\nDay of the month\n\n\nyday()\nDay of the year\n\n\ndecimal_date()\nDecimal year\n\n\nmonth()\nMonth of the year\n\n\nyear()\nYear\n\n\ntz()\nTime zone\n\n\n\nThe month() and wday() have a label option that will output the values as texts and not numbers. For example:\n\nmonth(x.date, label=TRUE)\n\n[1] Jun Jul Aug\nLevels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; Oct &lt; Nov &lt; Dec\n\n\nNote that the names are stored as factors. This may prove useful in that the names will be sorted in chronological order in the factor’s level.\nTo get the full name, set the abbreviation parameter, abbr, to FALSE.\n\nmonth(x.date, label=TRUE, abbr = FALSE)\n\n[1] June   July   August\n12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; July &lt; August &lt; September &lt; October &lt; ... &lt; December"
  },
  {
    "objectID": "date_objects.html#operating-on-dates",
    "href": "date_objects.html#operating-on-dates",
    "title": "5  Working with Dates",
    "section": "5.3 Operating on dates",
    "text": "5.3 Operating on dates\nYou can apply certain operations to dates as you would to numbers. For example, to list the number of days between the first and third elements of the vector x.date type the following:\n\n(x.date[3] - x.date[1]) / ddays()\n\n[1] 415.5949\n\n\nTo get the number of weeks between both dates:\n\n(x.date[3] - x.date[1]) / dweeks()\n\n[1] 59.37069\n\n\nLikewise, you can get the number of minutes between dates by dividing by dminutes() and the number of years by dividing by dyears().\nYou can also apply Boolean operations on dates. For example, to find which date element in x.date falls between the 11th and 24th day of any month, type:\n\n(mday(x.date) &gt; 11) & (mday(x.date) &lt; 24)\n\n[1]  TRUE FALSE  TRUE\n\n\nIf you want the command to return just the dates that satisfy this query, pass the Boolean operation as an index to the x.date vector:\n\nx.date[ (mday(x.date) &gt; 11) & (mday(x.date) &lt; 24) ]\n\n[1] \"2013-06-23 03:45:23 AKDT\" \"2014-08-12 18:01:59 AKDT\""
  },
  {
    "objectID": "date_objects.html#formatting-date-objects",
    "href": "date_objects.html#formatting-date-objects",
    "title": "5  Working with Dates",
    "section": "5.4 Formatting date objects",
    "text": "5.4 Formatting date objects\nYou can create a character vector from a date object. This is useful if you want to annotate plots with dates or include date values in reports. For example, to convert the date object x.date to a “ Year” character format, type the following:\n\nformat(x.date, \"%B %Y\")\n\n[1] \"June 2013\"   \"July 2013\"   \"August 2014\"\n\n\nThe format function accepts many different date/time format codes listed in the following table (note the cases!).\n\n\n\n\n\n\n\n\nFormat codes\nDescription\nExample\n\n\n\n\n%a\nAbbreviated weekday name\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%A\nFull weekday name\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%m\nMonth as decimal number\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%b\nAbbreviated month name\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%B\nFull month name\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%c\nDate and time, locale-specific\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%d\nDay of the month as decimal number\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%H\nHours as decimal number (00 to 23)\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%I\nHours as decimal number (01 to 12)\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%p\nAM/PM indicator in the locale\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%j\nDay of year as decimal number\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%M\nMinute as decimal number (00 to 59)\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%S\nSecond as decimal number\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%U\nWeek of the year starting on the first Sunday\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%W\nWeek of the year starting on the first Monday\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%w\nWeekday as decimal number (Sunday = 0)\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%x\nDate (locale-specific)\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%X\nTime (locale-specific)\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%Y\n4-digit year\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%y\n2-digit year\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%Z\nAbbreviated time zone\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59\n\n\n%z\nTime zone\n2013-06-23 03:45:23, 2013-07-30 14:23:00, 2014-08-12 18:01:59"
  },
  {
    "objectID": "strings.html#finding-patterns-in-a-string",
    "href": "strings.html#finding-patterns-in-a-string",
    "title": "6  Working with string objects",
    "section": "6.1 Finding patterns in a string",
    "text": "6.1 Finding patterns in a string\n  \n\n6.1.1 Checking for an exact string match\nThis is the simplest string operation one can perform. It involves assessing if a variable is equal (or not) to a complete text string.\nWe’ve already seen how the conditional statements can be used to check whether a variable is equal to, less than or greater than a number. We can use conditional statements to evaluate if a variable matches an exact string. For example, the following chunk of code returns TRUE since the strings match exactly.\n\na &lt;- \"Abc def\"\na == \"Abc def\"\n\n[1] TRUE\n\n\nHowever, note that R differentiates cases so that the following query, returns FALSE since the first character does not match in case (i.e. upper case A vs. lower case a).\n\na == \"abc def\"\n\n[1] FALSE\n\n\n\n6.1.1.1 How to ignore case sensitivity?\nIf you want R to ignore cases in any string operations, simply force all variables to a lower case and define the pattern being compared against in lower case. For example:\n\ntolower(a) == \"abc def\"\n\n[1] TRUE\n\n\n\n\n\n6.1.2 Checking for a partial match\n\n6.1.2.1 Matching anywhere in the string\nTo check if object a has the pattern \"c d\" (note the space in between the letters) anywhere in its string, use stringr’s str_detect function as follows:\n\nlibrary(stringr)\nstr_detect(a, \"c d\")\n\n[1] TRUE\n\n\nThe following example compares the string to \"cd\" (note the omission of the space):\n\nstr_detect(a, \"cd\")\n\n[1] FALSE\n\n\n\n\n6.1.2.2 Matching the begining of the string\nTo check if object a starts with the pattern \"c d\" add the carat character ^ in front of the pattern as in:\n\nstr_detect(a, \"^c d\")\n\n[1] FALSE\n\n\n\n\n6.1.2.3 Matching the end of the sring\nTo check if object a ends with the pattern \"Abc\" add the dollar character $ to the end of the pattern as in:\n\nstr_detect(a, \"Abc$\")\n\n[1] FALSE\n\n\n\n\n\n6.1.3 Matching against a list of characters\nIf you want to match against a list of characters, you may want to make use of the matching, %in%, operator. This is a built-in function and thus not part of the stringr package. For example, to check if any of the characters \"Jun\" or \"Sep\" exist in object b, type:\n\nb &lt;- c(\"Jan\", \"Feb\", \"Jun\", \"Oct\")\nb %in% c(\"Jun\", \"Sep\")\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nThe output will generate a boolean value for each element in b.\nThe %in% operator is described in greater detail in the next chapter.\n\n\n6.1.4 Locating the position of a pattern in a string\nIf you want to find where a particular pattern lies in a string, use the str_locate function. For example, to find where the pattern \"c d\" occurs in object a type:\n\nstr_locate(a, \"c d\")\n\n     start end\n[1,]     3   5\n\n\nThe function returns two values: the position in the string where the pattern starts (e.g. position 3) and the position where the pattern ends (e.g. position 5 )\nNote that if the pattern is not found, str_locate returns NA’s:\n\nstr_locate(a, \"cd\")\n\n     start end\n[1,]    NA  NA\n\n\nNote too that the str_locate function only returns the position of the first occurrence. For example, the following chunk will only return the start/end positions of the first occurrence of Ab.\n\nb &lt;- \"Abc def Abg\"\nstr_locate(b, \"Ab\")\n\n     start end\n[1,]     1   2\n\n\nTo find all occurrences, use the str_locate_all() function as in:\n\nstr_locate_all(b,\"Ab\")\n\n[[1]]\n     start end\n[1,]     1   2\n[2,]     9  10\n\n\nThe function returns a list object. To extract the position values into a dateframe, simply wrap the function in a call to as.data.frame, for example:\n\nstr.pos &lt;- as.data.frame(str_locate_all(b,\"Ab\"))\nstr.pos\n\n  start end\n1     1   2\n2     9  10\n\n\nThe reason str_locate_all returns a list and not a matrix or a data frame can be understood in the following example:\n\n# Create a 5 element string vector\nd &lt;- c(\"Abc\", \"Def \", \"Abc Def Ab\", \" bc \", \"ef \")\n\n# Search for all instances of \"Ab\"\nstr_locate_all(d,\"Ab\")\n\n[[1]]\n     start end\n[1,]     1   2\n\n[[2]]\n     start end\n\n[[3]]\n     start end\n[1,]     1   2\n[2,]     9  10\n\n[[4]]\n     start end\n\n[[5]]\n     start end\n\n\nHere, d is a five element string vector (so far we’ve worked with single element vectors). The str_locate_all function returns a result for each element of that vector, and since patterns can be found multiple times in a same vector element, the output can only be conveniently stored in a list.\n\n\n6.1.5 Finding the length of a string\nA natural extension to finding the positions of patterns in a text is to find the string’s total length. This can be accomplished with the str_length() function:\n\nstr_length(b)\n\n[1] 11\n\n\nFor a multi-element vector, the output looks like this:\n\nstr_length(d)\n\n[1]  3  4 10  4  3\n\n\n\n\n6.1.6 Finding a pattern’s frequency\nTo find out how often the pattern Ab occurs in each element of object d, use the str_count() function.\n\nstr_count(d, \"Ab\")\n\n[1] 1 0 2 0 0"
  },
  {
    "objectID": "strings.html#searching-for-non-alphanumeric-strings",
    "href": "strings.html#searching-for-non-alphanumeric-strings",
    "title": "6  Working with string objects",
    "section": "6.2 Searching for non alphanumeric strings",
    "text": "6.2 Searching for non alphanumeric strings\n  \nThe following characters need specialized syntax when sought in a regular expression: . , + , * , ? , ^ , $ , ( , ) , [ , ] , { , } , | , \\ . For example, when searching for a parenthesis ( in a string, the following code will generate an error:\n\nstr_detect(\"Some text (with parenthesis)\", \"(with\")\n\nError in stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)): Incorrectly nested parentheses in regex pattern. (U_REGEX_MISMATCHED_PAREN, context=`(with`)\n\n\nTo resolve this, you need to precede the special character with the escape characters \\\\ (two backslashes) as in:\n\nstr_detect(\"Some text (with parenthesis)\", \"\\\\(with\")\n\n[1] TRUE\n\n\nLikewise, when searching for an asterisk, *, type:\n\nstr_detect(\"x * y\", \"\\\\*\")\n\n[1] TRUE\n\n\nNote that not all special characters will generate an error. For example, if you are looking for a period . in a string, the following will not generate the desired outcome:\n\nstr_detect(\"x * y\", \".\")\n\n[1] TRUE\n\n\nThis should have outputted FALSE given that no period is present in the string. The dot has a special role in a regular expression in that it seeks out any one character in the string–hence the reason it returned TRUE given that we had at least one character in the string x * y.\nTo generate the desired outcome, add \\\\:\n\nstr_detect(\"x * y\", \"\\\\.\")\n\n[1] FALSE\n\n\nTo learn more about regular expressions, see this wikipedia entry on the topic."
  },
  {
    "objectID": "strings.html#modifying-strings",
    "href": "strings.html#modifying-strings",
    "title": "6  Working with string objects",
    "section": "6.3 Modifying strings",
    "text": "6.3 Modifying strings\n  \n\n6.3.1 Padding numbers with leading zeros\nThe str_pad() function can be used to pad numbers with leading zeros. Note that in doing so, you are creating a character object from a numeric object.\n\ne &lt;- c(12, 2, 503, 20, 0)\nstr_pad(e, width=3, side=\"left\", pad = \"0\" )\n\n[1] \"012\" \"002\" \"503\" \"020\" \"000\"\n\n\n\n\n6.3.2 Appending text to strings\nYou can append strings with custom text using the str_c() functions. For example, to add the string length at the end of each vector element in b type,\n\nstr_c(d, \" has \", str_length(d), \" characters\" )\n\n[1] \"Abc has 3 characters\"         \"Def  has 4 characters\"        \"Abc Def Ab has 10 characters\"\n[4] \" bc  has 4 characters\"        \"ef  has 3 characters\"        \n\n\n\n\n6.3.3 Removing white spaces\nYou can remove leading or ending (or both) white spaces from a string. For example, to remove leading white spaces from object d type,\n\nd.left.trim &lt;- str_trim(d, side=\"left\")\n\nNow let’s compare the original to the left-trimmed version:\n\nstr_length(d)\n\n[1]  3  4 10  4  3\n\nstr_length(d.left.trim)\n\n[1]  3  4 10  3  3\n\n\nTo remove trailing spaces set side = \"right\" and to remove both leading and trailing spaces set side = \"both\".\n\n\n6.3.4 Replacing elements of a string\nTo replace all instances of a specified set of characters in a string with another set of characters, use the str_replace_all() function. For example, to replace all spaces in object b with dashes, type:\n\nstr_replace_all(b, \" \", \"-\")\n\n[1] \"Abc-def-Abg\""
  },
  {
    "objectID": "strings.html#extracting-parts-of-a-string",
    "href": "strings.html#extracting-parts-of-a-string",
    "title": "6  Working with string objects",
    "section": "6.4 Extracting parts of a string",
    "text": "6.4 Extracting parts of a string\n \n\n6.4.1 Extracting elements of a string given start and end positions\nTo find the character elements of a vector at a given position of a given string, use the str_sub() function. For example, to find the characters between positions two and five (inclusive) type:\n\nstr_sub(b, start=2, end=5)\n\n[1] \"bc d\"\n\n\nIf you don’t specify a start position, then all characters up to and including the end position will be returned. Likewise, if the end position is not specified then all characters from the start position to the end of the string will be returned.\n\n\n6.4.2 Splitting a string by a character\nIf you want to break a string up into individual components based on a character delimiter, use the str_split() function. For example, to split the following string into separate elements by comma, type the following:\n\ng &lt;- \"Year:2000, Month:Jan, Day:23\"\nstr_split(g, \",\")\n\n[[1]]\n[1] \"Year:2000\"  \" Month:Jan\" \" Day:23\"   \n\n\nThe output is a one component list. If object g consists of more than one element, the output will be a list of as many components as there are g elements.\nDepending on your workflow, you may need to convert the str_split output to an atomic vector. For example, if you want to find an element in the above str_split output that matches the string Year:2000, the following will return FALSE and not TRUE as one would expect:\n\n\"Year:2000\" %in% str_split(g, \",\")\n\n[1] FALSE\n\n\nThe workaround is to convert the right-hand output to a single vector using the unlist function:\n\n\"Year:2000\" %in% unlist(str_split(g, \",\"))\n\n[1] TRUE\n\n\nIf you are applying the split function to a column of data from a dataframe, you will want to use the function str_split_fixed instead. This function assumes that the number of components to be extracted via the split will be the same for each vector element. For example, the following vector, T1, has two time components that need to be extracted. The separator is a dash, -.\n\nT1 &lt;- c(\"9:30am-10:45am\", \"9:00am- 9:50am\", \"1:00pm- 2:15pm\")\nT1\n\n[1] \"9:30am-10:45am\" \"9:00am- 9:50am\" \"1:00pm- 2:15pm\"\n\nstr_split_fixed(T1, \"-\", 2)\n\n     [,1]     [,2]     \n[1,] \"9:30am\" \"10:45am\"\n[2,] \"9:00am\" \" 9:50am\"\n[3,] \"1:00pm\" \" 2:15pm\"\n\n\nThe third parameter in the str_split_fixed function is the number of elements to return which also defines the output dimension (here, a three row and two column table). If you want to extract both times to separate vectors, reference the columns by index number:\n\nT1.start &lt;- str_split_fixed(T1, \"-\", 2)[ ,1]\nT1.start\n\n[1] \"9:30am\" \"9:00am\" \"1:00pm\"\n\nT1.end   &lt;- str_split_fixed(T1, \"-\", 2)[ ,2]\nT1.end\n\n[1] \"10:45am\" \" 9:50am\" \" 2:15pm\"\n\n\nYou will want to use the indexes if you are extracting strings in a data frame. For example:\n\ndat &lt;- data.frame( Time = c(\"9:30am-10:45am\", \"9:00am-9:50am\", \"1:00pm-2:15pm\"))\ndat$Start_time &lt;- str_split_fixed(dat$Time, \"-\", 2)[ , 1]\ndat$End_time   &lt;- str_split_fixed(dat$Time, \"-\", 2)[ , 2]\ndat\n\n            Time Start_time End_time\n1 9:30am-10:45am     9:30am  10:45am\n2  9:00am-9:50am     9:00am   9:50am\n3  1:00pm-2:15pm     1:00pm   2:15pm\n\n\n\n\n6.4.3 Extracting parts of a string that follow a pattern\nTo extract the three letter months from object g (defined in the last example), you can use a combination of stringr functions as in:\n\nloc &lt;- str_locate(g, \"Month:\")\nstr_sub(g, start = loc[,\"end\"] + 1, end = loc[,\"end\"]+3)\n\n[1] \"Jan\"\n\n\nThe above chunk of code first identifies the position of the Month: string and passes its output to the object loc (a matrix). It then uses the loc’s end position in the call to str_sub to extract the three characters making up the month abbreviation. The value 1 is added to the start parameter in str_sub to omit the last character of Month: (recall that the str_locate positions are inclusive).\nThis can be extend to multi-element vectors as follows:\n\n# Note the differences in spaces and string lenghts between the vector\n# elements.\ngs &lt;- c(\"Year:2000, Month:Jan, Day:23\",\n        \"Year:345, Month:Mar, Day:30\",\n        \"Year:1867 , Month:Nov, Day:5\")\n\nloc &lt;- str_locate(gs, \"Month:\")\nstr_sub(gs, start = loc[,\"end\"] + 1, end = loc[,\"end\"]+3)\n\n[1] \"Jan\" \"Mar\" \"Nov\"\n\n\nNote the non-uniformity in each element’s length and Month: position which requires that we explicitly search for the Month: string position in each element. Had all elements been of equal length and format, we could have simply assigned the position numbers in the call to str_sub function."
  },
  {
    "objectID": "boolean.html#relational-operations",
    "href": "boolean.html#relational-operations",
    "title": "7  Relational and boolean operations",
    "section": "7.1 Relational operations",
    "text": "7.1 Relational operations\nRelational operations play an important role in data manipulation. Anytime you subset a dataset based on one or more criterion, you are making use of a relational operation. The relational operators (also known as logical binary operators) include ==, !=, &lt;, &lt;=, &gt; and &gt;=. The output of a condition is a logical vector TRUE or FALSE.\n\n\n\n\n\n\n\n\n\nRelational operator\nSyntax\nExample\n\n\n\n\nExact equality\n==\n3 == 4 -&gt; FALSE\n\n\nExact inequality\n!=\n3 != 4 -&gt; TRUE\n\n\nLess than\n&lt;\n3 &lt; 4 -&gt; TRUE\n\n\nLess than or equal\n&lt;=\n4 &lt;= 4 -&gt; TRUE\n\n\nGreater than\n&gt;\n3 &gt; 4 -&gt; FALSE\n\n\nGreater than or equal\n&gt;=\n4 &gt;= 4 -&gt; TRUE"
  },
  {
    "objectID": "boolean.html#boolean-operations",
    "href": "boolean.html#boolean-operations",
    "title": "7  Relational and boolean operations",
    "section": "7.2 Boolean operations",
    "text": "7.2 Boolean operations\nBoolean operations can be used to piece together multiple evaluations.\nR has three boolean operators: The AND operator, &; The NOT operator, !; And the OR operator, |.\nThe & operator requires that the conditions on both sides of the boolean operator be satisfied. You would normally use this operator when addressing a condition along the lines of “x must be satisfied AND y must be satisfied”.\nThe | operator requires that at least one condition be met on either side of the boolean operator. You would normally use this operator when addressing a condition along the lines of “x must be satisfied OR y must be satisfied”. Note that the output will also be TRUE if both conditions are met.\nThe ! operator is a negation operator. It will reverse the outcome of a condition. So if the outcome of an expression is TRUE, preceding that expression with ! will reverse the outcome to FALSE and vice-versa.\n\n\n\n\n\n\n\n\n\n\n\nBoolean operator\nSyntax\nExample\nOutcome\n\n\n\n\nAND\n&\n4 == 3 & 1 == 1 \n4 == 4 & 1 == 1\nFALSE \nTRUE\n\n\nOR\n|\n4 == 4 | 1 == 1 \n4 == 3 | 1 == 1 \n4 == 3 | 1 == 2\nTRUE \nTRUE \nFALSE\n\n\nNOT\n!\n!(4 == 3) \n!(4 == 4)\nTRUE \nFALSE\n\n\n\n\nThe following table breaks down all possible Boolean outcomes where T = TRUE and F = FALSE:\n\n\n\nBoolean operation\nOutcome\n\n\n\n\nT & T\nTRUE\n\n\nT & F\nFALSE\n\n\nF & F\nFALSE\n\n\nT | T\nTRUE\n\n\nT | F\nTRUE\n\n\nF | F\nFALSE\n\n\n!T\nFALSE\n\n\n!F\nTRUE\n\n\n\nIf the input values to a boolean operation are numeric vectors and not logical vectors, the numeric values will be interpreted as FALSE if zero and TRUE otherwise. For example:\n\n1 & 2\n\n[1] TRUE\n\n1 & 0\n\n[1] FALSE\n\n\n\n7.2.1 Pecking order in operations\nNote that the operation a == (3 | 4) is not the same as (a == 3) | (a == 4). If, for example, a = 3, the former will return FALSE whereas the latter will return TRUE.\n\na &lt;- 3\na == (3 | 4)\n\n[1] FALSE\n\n(a == 3) | (a == 4)\n\n[1] TRUE\n\n\nThis is because R applies a pecking order to its operations. In the former case, R is first evaluating what is in between the parentheses, (3 | 4).\n\n(3 | 4)\n\n[1] TRUE\n\n\nThis returns TRUE since the numbers on either side of | are converted to TRUE (only values of 0 are converted to FALSE). It then compares a to this logical vector TRUE.\n\na == TRUE\n\n[1] FALSE\n\n\nHere, the == operator requires that both sides of the operation be of the same data type. a is numeric and TRUE is logical. Recall from Chapter 3 that R circumvents differences in data types by coercing all values to the highest common mode (see the chapter on data types). Here, numeric overrides logical type thus coercing the TRUE variable to its numeric data type representation, 1. Hence, the evaluation being performed is:\n\na == 1\n\n[1] FALSE\n\n\nWhen a vector is evaluated for more than one condition, you need to explicitly break down each condition before combining them with boolean operators.\n\n(a == 3) | (a == 4)\n\n[1] TRUE\n\n\nThe above is an example of R’s built-in operation precedence rules. For example, comparison operations such as &lt;= and &gt; are performed before boolean operations such that a == 3 | 4 will first evaluate a == 3 before evaluating ... | 4.\nEven boolean operations follow a pecking order such that ! precedes & which precedes |. For example:\n\n! TRUE & FALSE | TRUE\n\nwill first evaluate ! TRUE, then ... & FALSE, then ... | TRUE.\nTo overrride R’s built-in precedence, use parentheses. For example:\n\n! TRUE & (FALSE | TRUE)`\n\nwill first evaluate (FALSE | TRUE) and ! TRUE separately, then their output will be combined with ... & ....\nFor a full list of operation precedence, access the help page for Syntax.\n\n?Syntax\n\nThe following lists the pecking order from high to low precedence (i.e. top operation is performed before bottom operation).\n\n\n\n:: :::\naccess variables in a namespace\n\n\n$ @\ncomponent / slot extraction\n\n\n[ [[\nindexing\n\n\n^\nexponentiation (right to left)\n\n\n- +\nunary minus and plus\n\n\n:\nsequence operator\n\n\n%any% |&gt;\nspecialoperators (including %% and %/%)\n\n\n* /\nmultiply, divide\n\n\n+ -\n(binary) add, subtract\n\n\n&lt; &gt; &lt;= &gt;= == !=\nordering and comparison\n\n\n!\nnegation\n\n\n& &&\nand\n\n\n| ||\nor\n\n\n~\nas in formulae\n\n\n-&gt; -&gt;&gt;\nrightwards assignment\n\n\n&lt;- &lt;&lt;-\nassignment (right to left)\n\n\n=\nassignment (right to left)\n\n\n?\nhelp"
  },
  {
    "objectID": "boolean.html#comparing-multidimensional-objects",
    "href": "boolean.html#comparing-multidimensional-objects",
    "title": "7  Relational and boolean operations",
    "section": "7.3 Comparing multidimensional objects",
    "text": "7.3 Comparing multidimensional objects\nThe relational operators are used to compare single elements (i.e. one element at a time). If you want to compare two objects as a whole (e.g. multi-element vectors or data frames), use the identical() function. For example:\n\na &lt;- c(1, 5, 6, 10)\nb &lt;- c(1, 5, 6)\nidentical(a, a)\n\n[1] TRUE\n\nidentical(a, b)\n\n[1] FALSE\n\nidentical(mtcars, mtcars)\n\n[1] TRUE\n\n\nNotice that identical returns a single logical vector, regardless the input object’s dimensions.\nNote that the data structure must match as well as its element values. For example, if d is a list and a is an atomic vector, the output of identical will be false even if the internal values match.\n\nd &lt;- list( c(1, 5, 6, 10) )\nidentical(a, d)\n\n[1] FALSE\n\n\nIf we convert d from a list to an atomic vector using the unlist function (thus matching data structures), we get:\n\nidentical(a, unlist(d))\n\n[1] TRUE"
  },
  {
    "objectID": "boolean.html#the-match-operator-in",
    "href": "boolean.html#the-match-operator-in",
    "title": "7  Relational and boolean operations",
    "section": "7.4 The match operator %in%",
    "text": "7.4 The match operator %in%\nThe match operator %in% compares two sets of vectors and assesses if an element on the left-hand side of %in% matches any of the elements on the right-hand side of the operator. For each element in the left-hand vector, R returns TRUE if the value is present in any of the right-hand side elements or FALSE if not.\nFor example, given the following vectors:\n\nv1 &lt;- c( \"a\", \"b\", \"cd\", \"fe\")\nv2 &lt;- c( \"b\", \"e\")\n\nfind the elements in v1 that match any of the values in v2.\n\nv1 %in% v2\n\n[1] FALSE  TRUE FALSE FALSE\n\n\nThe function checks whether each element in v1 has a matching value in v2. For example, element \"a\" in v1 is compared to elements \"b\" and \"e\" in v2. No matches are found and a FALSE is returned. The next element in v1, \"b\", is compared to both elements in v2. This time, there is a match (v2 has an element \"b\") and TRUE is returned. This process is repeated for all elements in v1.\nThe logical vector output has the same length as the input vector v1 (four in this example).\nIf we swap the vector objects, we get a two element logical vector since we are now comparing each element in v2 to any matching elements in v1.\n\nv2 %in% v1\n\n[1]  TRUE FALSE"
  },
  {
    "objectID": "boolean.html#checking-if-a-value-is-na",
    "href": "boolean.html#checking-if-a-value-is-na",
    "title": "7  Relational and boolean operations",
    "section": "7.5 Checking if a value is NA",
    "text": "7.5 Checking if a value is NA\nWhen assessing if a value is equal to NA the following evaluation may behave unexpectedly.\n\na &lt;- c (3, 67, 4, NA, 10)\na == NA\n\n[1] NA NA NA NA NA\n\n\nThe output is not a logical data type we would expect from an evaluation. Instead, you must make use of the is.na() function:\n\nis.na(a)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nAs another example, if we want to keep all rows in dataframe d where z = NA, we would type:\n\nd &lt;- data.frame(x = c(1,4,2,5,2,3,NA), \n                y = c(3,2,5,3,8,1,1), \n                z = c(NA,NA,4,9,7,8,3))\n\nd[ is.na(d$z), ]\n\n  x y  z\n1 1 3 NA\n2 4 2 NA\n\n\nYou can, of course, use the ! operator to reverse the evaluation and omit all rows where z = NA,\n\nd[ !is.na(d$z), ]\n\n   x y z\n3  2 5 4\n4  5 3 9\n5  2 8 7\n6  3 1 8\n7 NA 1 3"
  },
  {
    "objectID": "subset_base.html#the-subset-function",
    "href": "subset_base.html#the-subset-function",
    "title": "8  Manipulating data tables using base functions",
    "section": "8.1 The subset function",
    "text": "8.1 The subset function\nYou can subset a dataframe object by criteria using the subset function.\n\nsubset(mtcars, mpg &gt; 25)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nYou can combine individual criterion using boolean operators when selecting by row.\n\nsubset(mtcars, (mpg &gt; 25) & (hp &gt; 65)  )\n\n               mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128      32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nFiat X1-9     27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nYou can also subset by column. To select columns, add the select = argument.\n\nsubset(mtcars, (mpg &gt; 25) & (hp &gt; 65) , select = c(mpg, cyl, hp))\n\n               mpg cyl  hp\nFiat 128      32.4   4  66\nFiat X1-9     27.3   4  66\nPorsche 914-2 26.0   4  91\nLotus Europa  30.4   4 113\n\n\nNote that the subset function can behave in unexpected ways. This is why its authors recommend against using subset in a script (see ?subset). Instead, they advocate using indices shown next."
  },
  {
    "objectID": "subset_base.html#subset-using-indices",
    "href": "subset_base.html#subset-using-indices",
    "title": "8  Manipulating data tables using base functions",
    "section": "8.2 Subset using indices",
    "text": "8.2 Subset using indices\nYou’ve already learned how to identify elements in an atomic vector or a data frame in Chapter 3. For example, to extract the second and fourth element of the following vector,\n\nx &lt;- c(\"a\", \"f\", \"a\", \"d\", \"a\")\n\ntype,\n\nx[c(2, 4)]\n\n[1] \"f\" \"d\"\n\n\nTo subset a dataframe by index, you need to define both dimension’s index (separated by a comma). For example, to extract the second row an fourth column type:\n\nmtcars[2,4 ]\n\n[1] 110\n\n\nThis returns the intersection of the row and column–a single cell.\nYou can, of course, extract table blocks by using c() operators and/or the colon operator.\n\nmtcars[5:12, c(\"hp\", \"mpg\", \"am\")]\n\n                   hp  mpg am\nHornet Sportabout 175 18.7  0\nValiant           105 18.1  0\nDuster 360        245 14.3  0\nMerc 240D          62 24.4  0\nMerc 230           95 22.8  0\nMerc 280          123 19.2  0\nMerc 280C         123 17.8  0\nMerc 450SE        180 16.4  0\n\n\nNote that here, we chose to reference the columns by their names instead of their index number."
  },
  {
    "objectID": "subset_base.html#extracting-using-logical-expression-and-indices",
    "href": "subset_base.html#extracting-using-logical-expression-and-indices",
    "title": "8  Manipulating data tables using base functions",
    "section": "8.3 Extracting using logical expression and indices",
    "text": "8.3 Extracting using logical expression and indices\nWe can also extract cells based on conditions. For example, to extract all elements in x that are equal to \"a\", type:\n\nx[ x == \"a\" ]\n\n[1] \"a\" \"a\" \"a\"\n\n\nLet’s breakdown the above expression. The output of the expression x == \"a\" is TRUE FALSE TRUE FALSE TRUE, a logical vector with the same number of elements as x. The logical elements are then passed to the indexing brackets where they act as a “mask” as shown in the following graphic.\n\nThe elements that make it through the extraction mask are then combined into a new vector element to reveal those elements satisfying the query.\nThe same idea applies to dataframes. For example, to extract all rows in mtcars where mpg &gt; 30 type:\n\nmtcars[ mtcars$mpg &gt; 30, ]\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nFiat 128       32.4   4 78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4 71.1  65 4.22 1.835 19.90  1  1    4    1\nLotus Europa   30.4   4 95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nHere, we are “masking” the rows that do not satisfy the criterion using the TRUE/FALSE logical outcomes from the conditional operation. Note that we have to add mtcars$ to the expression since the variable mpg does not exist as a standalone object.\nThe expression can be made more complex. For example, to select all rows where mpg is greater than 30 and where carb is equal to 2, and limiting the columns to hp, mpg and disp, type:\n\nmtcars[ (mtcars$mpg &gt; 30) & (mtcars$carb == 2), c(\"hp\", \"mpg\", \"disp\")]\n\n              hp  mpg disp\nHonda Civic   52 30.4 75.7\nLotus Europa 113 30.4 95.1"
  },
  {
    "objectID": "subset_base.html#replacing-values-using-logical-expressions",
    "href": "subset_base.html#replacing-values-using-logical-expressions",
    "title": "8  Manipulating data tables using base functions",
    "section": "8.4 Replacing values using logical expressions",
    "text": "8.4 Replacing values using logical expressions\nWe can adopt the same masking properties of logical variables to replace values in a vector. For example, to replace all instances of \"a\" in vector x with the character \"z\", we first expose the elements equal to \"a\", then assign the new value to the exposed elements.\n\nx[ x == \"a\" ] &lt;- \"z\"\nx\n\n[1] \"z\" \"f\" \"z\" \"d\" \"z\"\n\n\nYou can think of the logical mask as a template applied to a road surface before spraying that template with a can of \"z\" spray. Only the exposed portion of the street surface will be sprayed with the \"z\" values.\n\nYou can apply this technique to dataframes as well. For example, to replace all elements in mpg with -1 if mpg &lt; 25, type:\n\nmtcars2 &lt;- mtcars \nmtcars2[ mtcars2$mpg &lt; 25, \"mpg\"]  &lt;-  -1\nmtcars2\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           -1.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       -1.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          -1.0   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      -1.0   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   -1.0   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             -1.0   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          -1.0   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           -1.0   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            -1.0   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            -1.0   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           -1.0   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          -1.0   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          -1.0   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         -1.0   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  -1.0   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental -1.0   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   -1.0   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       -1.0   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    -1.0   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         -1.0   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          -1.0   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    -1.0   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      -1.0   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        -1.0   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       -1.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          -1.0   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nNote that we had to specify the column, mpg, into which we are replacing the values. Had we left the second index empty, we would have replaced values across all columns for records having an mpg value less than 30."
  },
  {
    "objectID": "dplyr.html#the-dplyr-basics",
    "href": "dplyr.html#the-dplyr-basics",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.1 The dplyr basics",
    "text": "9.1 The dplyr basics\nThe basic set of R tools can accomplish many data table queries, but the syntax can be overwhelming and verbose. The dplyr package offers some nifty and simple querying functions. Some of dplyr’s key data manipulation functions are summarized in the following table:\n\n\n\ndplyr function\nDescription\n\n\n\n\nfilter()\nSubset by row values\n\n\narrange()\nSort rows by column values\n\n\nselect()\nSubset columns\n\n\nmutate()\nAdd columns\n\n\nsummarise()\nSummarize columns\n\n\n\nNote that all of these functions take as first argument the data table name except when used in a piping operation (pipes are discussed later in this chapter). For example:\nWhen used alone, dataframe dat is the first argument inside the dplyr function.\n\ndat2 &lt;- select(dat, Crop)\n\nWhen used in a pipe, dataframe dat is outside of the dplyr function.\n\ndat2 &lt;- dat %&gt;% select(Crop)\n\nIf you are using R 4.1 or greater, you can make use of the native pipe.\n\ndat2 &lt;- dat |&gt; select(Crop)\n\n\n9.1.1 filter: Subset by rows\nTables can be subsetted by rows based on column values. For example, we may wish to grab all rows associated with the value Oats in the Crop column:\n\ndat.query1 &lt;- filter(dat, Crop == \"Oats\")\n\nWe can use the unique() function to identify all unique values in the Crop column. This should return a single unique value: Oats.\n\nunique(dat.query1$Crop)\n\n[1] \"Oats\"\n\n\nNote that R is case sensitive, so make sure that you respect each letter’s case (i.e. upper or lower).\nWe can expand our query by including both Oats, Buckwheat and limiting the Country column to Canada.\n\ndat.query2 &lt;- filter(dat, Crop == \"Oats\" | Crop == \"Buckwheat\", \n                          Country == \"Canada\")\n\n\nunique(dat.query1$Crop)\n\n[1] \"Oats\"\n\nunique(dat.query1$Country)\n\n[1] \"Canada\"                   \"United States of America\"\n\n\nThe character | is the Boolean operator OR. So in our example, the query can be read as “… crop equals oats OR crop equals buckwheat”. Had we used the AND operator, & as in Crop == \"Oats\" & Crop == \"Buckwheat\", the output would have returned zero rows since a Crop value cannot be both Oats AND Buckwheat.\nWe can expand this query by limiting our output to the years 2005 to 2010\n\ndat.query3 &lt;- filter(dat, Crop == \"Oats\" | Crop == \"Buckwheat\", \n                          Country == \"Canada\", \n                          Year &gt;= 2005 & Year &lt;= 2010)\n\n\nunique(dat.query3$Crop)\n\n[1] \"Oats\"      \"Buckwheat\"\n\nunique(dat.query3$Country)\n\n[1] \"Canada\"\n\nrange(dat.query3$Year)\n\n[1] 2005 2010\n\n\nNote the use of the AND Boolean operator (&) instead of the OR operator (|) for the Year query. We want the Year value to satisfy two criteria simultaneously: greater than or equal to 2005 AND less than or equal to 2010. Had we used the | operator, R would have returned all years since all year values satisfy at least one of the two critera.\n\n\n9.1.2 arrange: Sort rows by column value\nYou can sort a table based on a column’s values using the arrange function. For example, to sort dat by crop name type:\n\ndat.sort1 &lt;- arrange(dat, Crop)\nhead(dat.sort1)\n\n                   Country   Crop         Information Year      Value\n1                   Canada Barley Area harvested (Ha) 2012 2060000.00\n2                   Canada Barley       Yield (Hg/Ha) 2012   38894.66\n3 United States of America Barley Area harvested (Ha) 2012 1312810.00\n4 United States of America Barley       Yield (Hg/Ha) 2012   36533.24\n5                   Canada Barley Area harvested (Ha) 2011 2364800.00\n6                   Canada Barley       Yield (Hg/Ha) 2011   32796.43\n           Source\n1   Official data\n2 Calculated data\n3   Official data\n4 Calculated data\n5   Official data\n6 Calculated data\n\ntail(dat.sort1)\n\n     Country      Crop         Information Year    Value\n1496  Canada Triticale Area harvested (Ha) 1991  1093.00\n1497  Canada Triticale       Yield (Hg/Ha) 1991 21957.91\n1498  Canada Triticale Area harvested (Ha) 1990  1074.00\n1499  Canada Triticale       Yield (Hg/Ha) 1990 26396.65\n1500  Canada Triticale Area harvested (Ha) 1989  1093.00\n1501  Canada Triticale       Yield (Hg/Ha) 1989 21957.91\n                                       Source\n1496                            Official data\n1497                          Calculated data\n1498 FAO data based on imputation methodology\n1499                          Calculated data\n1500                             FAO estimate\n1501                          Calculated data\n\n\nBy default, arrange sorts by ascending order. To sort by descending order, wrap the column name with the function desc(). For example, to sort the table by Crop in ascending order, then by Year in descending order, type:\n\ndat.sort2 &lt;- arrange(dat, Crop, desc(Year))\nhead(dat.sort2)\n\n                   Country   Crop         Information Year      Value\n1                   Canada Barley Area harvested (Ha) 2012 2060000.00\n2                   Canada Barley       Yield (Hg/Ha) 2012   38894.66\n3 United States of America Barley Area harvested (Ha) 2012 1312810.00\n4 United States of America Barley       Yield (Hg/Ha) 2012   36533.24\n5                   Canada Barley Area harvested (Ha) 2011 2364800.00\n6                   Canada Barley       Yield (Hg/Ha) 2011   32796.43\n           Source\n1   Official data\n2 Calculated data\n3   Official data\n4 Calculated data\n5   Official data\n6 Calculated data\n\ntail(dat.sort2)\n\n     Country      Crop         Information Year    Value\n1496  Canada Triticale Area harvested (Ha) 1991  1093.00\n1497  Canada Triticale       Yield (Hg/Ha) 1991 21957.91\n1498  Canada Triticale Area harvested (Ha) 1990  1074.00\n1499  Canada Triticale       Yield (Hg/Ha) 1990 26396.65\n1500  Canada Triticale Area harvested (Ha) 1989  1093.00\n1501  Canada Triticale       Yield (Hg/Ha) 1989 21957.91\n                                       Source\n1496                            Official data\n1497                          Calculated data\n1498 FAO data based on imputation methodology\n1499                          Calculated data\n1500                             FAO estimate\n1501                          Calculated data\n\n\n\n\n9.1.3 select: Subset by column\nYou can subset a table by column(s) using the select function. To extract the columns Crop, Year and Value, type:\n\ndat.subcol &lt;- select(dat, Crop, Year, Value)\nhead(dat.subcol, 2)\n\n    Crop Year      Value\n1 Barley 2012 2060000.00\n2 Barley 2012   38894.66\n\n\nIf you want all columns other than Crop, Year and Value, add the negative - symbol before the column name:\n\ndat.subcol &lt;- select(dat, -Crop, -Year, -Value)\nhead(dat.subcol, 2)\n\n  Country         Information          Source\n1  Canada Area harvested (Ha)   Official data\n2  Canada       Yield (Hg/Ha) Calculated data\n\n\n\n\n9.1.4 mutate: Creating and/or calculating column values\nYou can add columns (and compute their values) using the mutate function. For example, to add a column Ctr_abbr and assign it the abbreviated values CAN for Canada and USA for the United States of America based on the values in column Country type:\n\ndat.extended &lt;- mutate(dat, Ctr_abbr = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))\nhead(dat.extended,3)\n\n  Country      Crop         Information Year      Value          Source\n1  Canada    Barley Area harvested (Ha) 2012 2060000.00   Official data\n2  Canada    Barley       Yield (Hg/Ha) 2012   38894.66 Calculated data\n3  Canada Buckwheat Area harvested (Ha) 2012       0.00    FAO estimate\n  Ctr_abbr\n1      CAN\n2      CAN\n3      CAN\n\ntail(dat.extended,3)\n\n                      Country    Crop         Information Year      Value\n1499 United States of America     Rye       Yield (Hg/Ha) 1961   11121.79\n1500 United States of America Sorghum Area harvested (Ha) 1961 4445000.00\n1501 United States of America Sorghum       Yield (Hg/Ha) 1961   27442.07\n              Source Ctr_abbr\n1499 Calculated data      USA\n1500   Official data      USA\n1501 Calculated data      USA\n\n\nHere, we make use of an embedded function, ifelse, which performs a conditional operation: if the Country value is Canada return CAN if not, return USA. ifelse is covered in section 9.3.\nNote that if you wish to rename a column, you can use the rename() function instead of mutate.\nYou can also use mutate to recompute column values. For example, to replace the Country column values with CAN or USA type:\n\ndat.overwrite &lt;- mutate(dat, Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))\nhead(dat.overwrite, 3)\n\n  Country      Crop         Information Year      Value          Source\n1     CAN    Barley Area harvested (Ha) 2012 2060000.00   Official data\n2     CAN    Barley       Yield (Hg/Ha) 2012   38894.66 Calculated data\n3     CAN Buckwheat Area harvested (Ha) 2012       0.00    FAO estimate\n\ntail(dat.overwrite, 3)\n\n     Country    Crop         Information Year      Value          Source\n1499     USA     Rye       Yield (Hg/Ha) 1961   11121.79 Calculated data\n1500     USA Sorghum Area harvested (Ha) 1961 4445000.00   Official data\n1501     USA Sorghum       Yield (Hg/Ha) 1961   27442.07 Calculated data\n\n\n\n\n9.1.5 mutate across muliple columns\nYou might find yourself wanting to apply a same set of mutate operations across multiple variables. For example, given the following sample dataset,\n\nmet &lt;- data.frame(Wind = c(3.4, 5.0, 99, 4.1, 1.5),\n                  Dir  = c(181, 220, 15,  15,  99 ),\n                  Prec = c(99 , 0.5,  0,  99,  99))\nmet\n\n  Wind Dir Prec\n1  3.4 181 99.0\n2  5.0 220  0.5\n3 99.0  15  0.0\n4  4.1  15 99.0\n5  1.5  99 99.0\n\n\nwhere the value 99 is a placeholder for a missing value for the variables Wind and Prec but a valid value for Dir, we want to replace all missing values with NA. We could either create two mutate operations as in,\n\nmet %&gt;%  mutate(Wind = ifelse(Wind == 99, NA, Wind),\n                Prec = ifelse(Prec == 99, NA, Prec))\n\n  Wind Dir Prec\n1  3.4 181   NA\n2  5.0 220  0.5\n3   NA  15  0.0\n4  4.1  15   NA\n5  1.5  99   NA\n\n\nor, we could reduce the separate mutate operations into a single operation by adding an across() function to its argument.\n\nmet %&gt;% mutate(across( c(Wind, Prec),\n                       ~ ifelse( . == 99, NA, .)))\n\n  Wind Dir Prec\n1  3.4 181   NA\n2  5.0 220  0.5\n3   NA  15  0.0\n4  4.1  15   NA\n5  1.5  99   NA\n\n\nacross takes two arguments: the columns the mutate operation is to operate on, and the mutate operation to perform on these columns. Here, the tilde ~ can be interpreted as “to the listed columns, apply the following function …”. The dot . in the ifelse function is a placeholder for each column listed in across’s first argument.\n\n\n9.1.6 summarise: Summarize columns\nYou can summarize (or “collapse”) one or more columns using the summarise function. For instance, to get the minimum and maximum years from the Year column, type:\n\nsummarise(dat, yr_min = min(Year), yr_max=max(Year))\n\n  yr_min yr_max\n1   1961   2012"
  },
  {
    "objectID": "dplyr.html#sec-pipe",
    "href": "dplyr.html#sec-pipe",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.2 Combining data manipulation functions using the pipe %>%",
    "text": "9.2 Combining data manipulation functions using the pipe %&gt;%\nIn most cases, you will find yourself wanting to combine several of dplyr’s data manipulation functions. For example,\n\ndat.yield  &lt;- filter(dat, Information == \"Yield (Hg/Ha)\", \n                          Crop == \"Oats\",\n                          Year == 2012)\ndat.rename &lt;- mutate(dat.yield, Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))\ndat.final  &lt;- select(dat.rename, Country, Value)\n\nhead(dat.final, 3)\n\n  Country    Value\n1     CAN 24954.79\n2     USA 21974.70\n\n\nThe downside to this approach is the creation of several intermediate objects (e.g. dat.yield and dat.rename). This can make the workflow difficult to follow and clutter your R session with needless intermediate objects.\nAnother approach to combining dplyr operations is to use the piping operator ,%&gt;%, which daisy chains dplyr operations. So our previous workflow could look like:\n\ndat.final &lt;- dat %&gt;%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Crop == \"Oats\",\n         Year == 2012)  %&gt;% \n  mutate(Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\")) %&gt;%\n  select(Country, Value)\n\nhead(dat.final, 3)\n\n  Country    Value\n1     CAN 24954.79\n2     USA 21974.70\n\n\nThe chunk of code can be read as “… with the dat table, filter by …, then mutate …., then select …” with the result from one operation being passed on to the next using the %&gt;% operator. Note that the filter, mutate and select functions do not include the data table name making the chunk of code less cluttered and easier to read. The input data table dat appears just once at the beginning of the pipe.\n\n9.2.1 R has a native pipe too\nR has recently (as of version 4.1) added its own native pipe to its base function. Its infix operator is written as |&gt;. In most code chunks covered in these tutorials, you can substitute %&gt;% with |&gt;. For example, you can write the previous code chunk as:\n\ndat.final &lt;- dat  |&gt; \n  filter(Information == \"Yield (Hg/Ha)\", \n         Crop == \"Oats\",\n         Year == 2012)   |&gt;  \n  mutate(Country = ifelse(Country == \"Canada\", \"CAN\", \"USA\"))  |&gt; \n  select(Country, Value)\n\nThere are, however, a few subtle differences between the two. A deeper dive in how they differ can be found here.\nIn this course, we’ll stick with the %&gt;% operator given that |&gt; is new and is not yet as widely adopted as %&gt;%.\n  \n\n\n9.2.2 Updating a factor’s levels in a dataframe\nSubsetting a dataframe by row might result in a factor losing all values associated with a level. You learned in section 3.3.1.2 that the droplevels function can be used to remove any levels no longer present in the factor. droplevels can also be used in a piping operation, however, the function will drop levels for all factors in the dataframe–this might not be the intended goal.\nIf you want to selectively drop the levels for a specific set of factors in a dataframe, you might want to make use of the fct_drop function from the forcats package. For example, let’s first create a subset of the built-in mtcars dataset and create factors from a couple of its variables. We’ll use the base methods to tackle this task.\n\nmt &lt;- mtcars[ , c(\"cyl\", \"gear\")]\nmt$cyl  &lt;- factor(mtcars$cyl)\nmt$gear &lt;- factor(mtcars$gear)\n\nNow, let’s filter by cyl.\n\nmt_sub &lt;- mt %&gt;% \n   filter(cyl == 8)\n\nExploring mt_sub’s summary, we see that some levels have no matching values in the factors:\n\nsummary(mt_sub)\n\n cyl    gear  \n 4: 0   3:12  \n 6: 0   4: 0  \n 8:14   5: 2  \n\n\nApplying droplevels to the dataframe will remove those levels from both factors;\n\nmt_sub &lt;- mt %&gt;% \n   filter(cyl == 8) %&gt;% \n   droplevels()\n\nsummary(mt_sub)\n\n cyl    gear  \n 8:14   3:12  \n        5: 2  \n\n\nUsing fct_drop controls which factor(s) you want to clean up the levels for. Note, however, that fct_drop must be used inside of a mutate function.\n\nlibrary(forcats) \n\nmt_sub &lt;- mt %&gt;% \n   filter(cyl == 8) %&gt;% \n   mutate( cyl = fct_drop(cyl))\n\nsummary(mt_sub)\n\n cyl    gear  \n 8:14   3:12  \n        4: 0  \n        5: 2"
  },
  {
    "objectID": "dplyr.html#conditional-statements",
    "href": "dplyr.html#conditional-statements",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.3 Conditional statements",
    "text": "9.3 Conditional statements\n  \n\n9.3.1 The base ifelse\nConditional statements are used when you want to create an output value that is conditioned on an evaluation. For example, if you want to output a value of 1 if an input value is less than 23 and a value of 0 otherwise, you can make use of the ifelse function as follows:\n\nx &lt;- c(12,102, 43, 20, 90, 0, 12, 6)\nifelse(x &lt; 23, 1, 0)\n\n[1] 1 0 0 1 0 1 1 1\n\n\nifelse takes three arguments:\n\nThe condition to evaluate (x &lt; 23 in the above example);\nThe value to output if the condition is TRUE (1 in our example);\nThe value to output if the condition is FALSE (0 in our example).\n\nThe base ifelse function works as expected when the input/output values are numeric or character, but it does not work as expected when applied to factors or dates. For example, if you wish to replace one factor level with another, the following example will not return the expected output.\n\nx &lt;- as.factor( c(\"apple\", \"banana\", \"banana\", \"pear\", \"apple\"))\nifelse(x == \"pear\", \"apple\", x)\n\n[1] \"1\"     \"2\"     \"2\"     \"apple\" \"1\"    \n\n\nThe output is a character representation of the level number (recall that factors encode level values as numbers behind the scenes, i.e. apple =1, banana=2, etc…). Likewise, if you wish to replace an erroneous date you will get:\n\nlibrary(lubridate)\ny &lt;- mdy(\"1/23/2016\", \"3/2/2016\", \"12/1/1901\", \"11/23/2016\")\nifelse( y == mdy(\"12/1/1901\"), mdy(\"12/1/2016\"), y)\n\n[1] 16823 16862 17136 17128\n\n\nHere, ifelse converts the date object to its internal numeric representation as number of days since 1970.\n\n\n9.3.2 dplyr’s if_else\nifelse does not preserve the attributes that might be present in a vector. In other words, it will strip away the vector’s class. A safer alternative is to use dplyr’s if_else function.\nReworking the above examples:\n\nif_else( y == mdy(\"12/1/1901\"), mdy(\"12/1/2016\"), y)\n\n[1] \"2016-01-23\" \"2016-03-02\" \"2016-12-01\" \"2016-11-23\"\n\n\nThe date class is preserved. Now let’s check the output of a factor.\n\nif_else(x == \"pear\", \"apple\", x)\n\n[1] \"apple\"  \"banana\" \"banana\" \"apple\"  \"apple\" \n\n\nNote that when working with factors, however, if_else will strip the factor class of an input factor. But, instead of returning the factor’s underlying integer values, it outputs the associated levels as a character data type.\nThe workaround is to convert the if_else output to a factor.\n\nfactor(if_else(x == \"pear\", \"apple\", x))\n\n[1] apple  banana banana apple  apple \nLevels: apple banana\n\n\nIf you need to explicitly define the levels, add the levels = c(\"pear\",...).\nNOTE: dplyr offers the recode function that preserves factors however, this function is being superseded according to the documentation as of version 1.1.4.\n\n\n9.3.3 Changing values based on multiple conditions: case_when\nifelse and if_else work great when a single set of conditions is to be satisfied. But, if multiple sets of conditions are to be evaluated, nested if/else statements become cumbersome and are prone to clerical error. The following code highlights an example of nested if/else statements.\n\nunit &lt;- c(\"F\",\"F\",\"C\", \"K\")\nif_else( unit == \"C\", \"Celsius\", if_else(unit == \"F\", \"Fahrenheit\", \"Kelvin\"))\n\n[1] \"Fahrenheit\" \"Fahrenheit\" \"Celsius\"    \"Kelvin\"    \n\n\nA simpler solution is to use the case_when function.\n\ncase_when(unit == \"C\" ~ \"Celsius\",\n          unit == \"F\" ~ \"Fahrenheit\",\n          unit == \"K\" ~ \"Kelvin\")\n\n[1] \"Fahrenheit\" \"Fahrenheit\" \"Celsius\"    \"Kelvin\"    \n\n\ncase_when can aso be used for more complex operations. For example, given two vectors, unit and temp, we would like to convert all temp values to Fahrenheit by applying a temperature conversion dependent on the unit value.\n\ntemp &lt;- c(45.2, 56.3, 11.0, 285)\n\n\ncase_when(unit == \"F\" ~ temp,\n          unit == \"C\" ~ (temp * 9/5) + 32,\n          TRUE ~ (temp - 273.15) * 9/5 + 32)\n\n[1] 45.20 56.30 51.80 53.33\n\n\nThe last argument, TRUE ~, applies to all conditions not satisfied by the previous two conditions (otherwise, not doing so would return NA values by default). You only need to add a TRUE ~ condition if you know that all previously listed conditions may not cover all possible outcomes. Here, we know that some observations are associated with unit == \"K\" yet that condition is not explicitly defined in the case_when arguments. We could have, of course, added the unit == \"K\" condition to the above code chunk thus alleviating the need for the TRUE ~ condition.\nNote that the order in which these conditions are listed matters since evaluation stops at the first TRUE outcome encountered. So, had the last condition been moved to the top of the stack, all temp values would be assigned the first conversion option.\n\n# What not to do ...\ncase_when(TRUE ~ (temp - 273.15) * 9/5 + 32,\n          unit == \"F\" ~ temp,\n          unit == \"C\" ~ (temp * 9/5) + 32)\n\n[1] -378.31 -358.33 -439.87   53.33\n\n\nNote that case_when can also be used inside of a mutate function. For example, to replace Canada and United States of America in variable Country with CAN and USA respectively and to create a new variable called Type which will take on the values of 1, 2 or 3 depending on the values in variable Source, type the following:\n\ndat1 &lt;- dat %&gt;% \n  mutate(Country = case_when(Country == \"Canada\" ~ \"CAN\",\n                             Country == \"United States of America\" ~ \"USA\"),\n         Type = case_when(Source == \"Calculated data\" ~ 1,\n                          Source == \"Official data\" ~ 2,\n                          TRUE ~ 3)) \nhead(dat1)   \n\n  Country         Crop         Information Year      Value          Source Type\n1     CAN       Barley Area harvested (Ha) 2012 2060000.00   Official data    2\n2     CAN       Barley       Yield (Hg/Ha) 2012   38894.66 Calculated data    1\n3     CAN    Buckwheat Area harvested (Ha) 2012       0.00    FAO estimate    3\n4     CAN  Canary seed Area harvested (Ha) 2012  101900.00   Official data    2\n5     CAN  Canary seed       Yield (Hg/Ha) 2012   12161.92 Calculated data    1\n6     CAN Grain, mixed Area harvested (Ha) 2012   57900.00   Official data    2"
  },
  {
    "objectID": "dplyr.html#replacing-values-with-na",
    "href": "dplyr.html#replacing-values-with-na",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.4 Replacing values with NA",
    "text": "9.4 Replacing values with NA\nSo far, we’ve used the ifelse or if_else functions to replace certain values with NA. dplyr offers the na_if() function to simplify the syntax. For example, to replace -999 with NA:\n\nval &lt;- c(-999, 6, -1, -999)\nna_if( val , -999 )\n\n[1] NA  6 -1 NA\n\n\nLikewise, to replace all empty character values:\n\nval &lt;- c(\"ab\", \"\", \"A b\", \"  \")\nna_if( val , \"\" )\n\n[1] \"ab\"  NA    \"A b\" \"  \" \n\n\nna_if will also preserve the object’s class. For example:\n\nx &lt;- as.factor( c(\"apple\", \"walnut\", \"banana\", \"pear\", \"apple\"))\nna_if(x , \"walnut\")\n\n[1] apple  &lt;NA&gt;   banana pear   apple \nLevels: apple banana pear walnut\n\n\nBut, note that it does not automatically drop the level being replaced with NA.\nna_if also works with dates, but don’t forget to evaluate a date object with a date value. For example, to replace dates of 12/1/1901 with NA, we need to make a date object of that value. Here, we’ll make use of the mdy() function as in mdy(\"12/1/1901\").\n\ny &lt;- mdy(\"1/23/2016\", \"3/2/2016\", \"12/1/1901\", \"11/23/2016\")\nna_if(y, mdy(\"12/1/1901\"))\n\n[1] \"2016-01-23\" \"2016-03-02\" NA           \"2016-11-23\"\n\n\nTo use na_if() in a piping operation, it needs to be embedded in a mutate() function. For example, to replace \"Calculated data\" with NA in the dat dataframe, type:\n\ndat1 &lt;- dat %&gt;% \n    mutate(Source = na_if(Source, \"Calculated data\" ))\n\n\nunique(dat1$Source)\n\n[1] \"Official data\"                           \n[2] NA                                        \n[3] \"FAO estimate\"                            \n[4] \"FAO data based on imputation methodology\""
  },
  {
    "objectID": "dplyr.html#outputting-a-vector-instead-of-a-table-using-pull",
    "href": "dplyr.html#outputting-a-vector-instead-of-a-table-using-pull",
    "title": "9  Manipulating data tables with dplyr",
    "section": "9.5 Outputting a vector instead of a table using pull",
    "text": "9.5 Outputting a vector instead of a table using pull\nPiping operations will output a table, even if a single value is returned. For example, the following summarization operation returns the total oats yield as a dataframe:\n\noats &lt;- dat %&gt;% \n  filter(Crop == \"Oats\",\n         Information == \"Yield (Hg/Ha)\") %&gt;% \n  summarise(Oats_sum = sum(Value))\noats\n\n  Oats_sum\n1  2169334\n\nclass(oats)\n\n[1] \"data.frame\"\n\n\nThere may be times when you want to output a vector element and not a dataframe. To output a vector, use the pull() function.\n\noats &lt;- dat %&gt;% \n  filter(Crop == \"Oats\",\n         Information == \"Yield (Hg/Ha)\") %&gt;% \n  summarise(Oats_sum = sum(Value)) %&gt;% \n  pull()\noats\n\n[1] 2169334\n\nclass(oats)\n\n[1] \"numeric\"\n\n\nThe pull function can also be used to explicitly define the column to extract. For example, to extract the Value column type:\n\n# This outputs a multi-element vector\nyield &lt;- dat %&gt;% \n  filter(Crop == \"Oats\",\n         Information == \"Yield (Hg/Ha)\") %&gt;% \n  pull(Value)\n\n\nhead(yield)\n\n[1] 24954.79 21974.70 29109.36 20492.37 27364.53 23056.62\n\nclass(yield)\n\n[1] \"numeric\""
  },
  {
    "objectID": "group_by.html#summarizing-data-by-group",
    "href": "group_by.html#summarizing-data-by-group",
    "title": "10  Grouping and summarizing",
    "section": "10.1 Summarizing data by group",
    "text": "10.1 Summarizing data by group\nLet’s first create a dataframe listing the average delay time in minutes, by day of the week and by quarter, for Logan airport’s 2014 outbound flights.\n\ndf &lt;- data.frame(\n  Weekday = factor(rep(c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\"), each = 4), \n                   levels = c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\")),\n  Quarter = paste0(\"Q\", rep(1:4, each = 5)), \n  Delay = c(9.9, 5.4, 8.8, 6.9, 4.9, 9.7, 7.9, 5, 8.8, 11.1, 10.2, 9.3, 12.2,\n            10.2, 9.2, 9.7, 12.2, 8.1, 7.9, 5.6))\n\nThe goal will be to summarize the table by Weekday as shown in the following graphic.\n\nThe data table has three variables: Weekday, Quarter and Delay. Delay is the value we will summarize which leaves us with one variable to collapse: Quarter. In doing so, we will compute the Delay statistics for all quarters associated with a unique Weekday value.\nThis workflow requires two operations: a grouping operation using the group_by function and a summary operation using the summarise/summarize function. Here, we’ll compute two summary statistics: minimum delay time and maximum delay time.\n\nlibrary(dplyr)\n\ndf %&gt;% \n  group_by(Weekday) %&gt;% \n  summarise(min_delay = min(Delay), max_delay = max(Delay))\n\n# A tibble: 5 × 3\n  Weekday min_delay max_delay\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Mon           5.4       9.9\n2 Tues          4.9       9.7\n3 Wed           8.8      11.1\n4 Thurs         9.2      12.2\n5 Fri           5.6      12.2\n\n\nNote that the weekday follows the chronological order as defined in the Weekday factor.\nYou’ll also note that the output is a tibble. This data class is discussed at the end of this page.\n\n10.1.1 Grouping by multiple variables\nYou can group by more than one variable. For example, let’s build another dataframe listing the average delay time in minutes, by quarter, by weekend/weekday and by inbound/outbound status for Logan airport’s 2014 outbound flights.\n\ndf2 &lt;- data.frame(\n  Quarter = paste0(\"Q\", rep(1:4, each = 4)), \n  Week = rep(c(\"Weekday\", \"Weekend\"), each=2, times=4),\n  Direction = rep(c(\"Inbound\", \"Outbound\"), times=8),\n  Delay = c(10.8, 9.7, 15.5, 10.4, 11.8, 8.9, 5.5, \n            3.3, 10.6, 8.8, 6.6, 5.2, 9.1, 7.3, 5.3, 4.4))\n\nThe goal will be to summarize the delay time by Quarter and by Week type as shown in the following graphic.\n\nThis time, the data table has four variables. We are wanting to summarize by Quater and Week which leaves one variable, Direction, that needs to be collapsed.\n\ndf2 %&gt;% \n  group_by(Quarter, Week) %&gt;% \n  summarise(min_delay = min(Delay), max_delay = max(Delay))\n\n# A tibble: 8 × 4\n# Groups:   Quarter [4]\n  Quarter Week    min_delay max_delay\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Q1      Weekday       9.7      10.8\n2 Q1      Weekend      10.4      15.5\n3 Q2      Weekday       8.9      11.8\n4 Q2      Weekend       3.3       5.5\n5 Q3      Weekday       8.8      10.6\n6 Q3      Weekend       5.2       6.6\n7 Q4      Weekday       7.3       9.1\n8 Q4      Weekend       4.4       5.3\n\n\nThe following section demonstrates other grouping/summarizing operations on a larger dataset."
  },
  {
    "objectID": "group_by.html#a-working-example",
    "href": "group_by.html#a-working-example",
    "title": "10  Grouping and summarizing",
    "section": "10.2 A working example",
    "text": "10.2 A working example\nThe data file FAO_grains_NA.csv will be used in this exercise. This dataset consists of grain yield and harvest year by North American country. The dataset was downloaded from http://faostat3.fao.org/ in June of 2014.\nRun the following line to load the FAO data file into your current R session.\n\ndat &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/FAO_grains_NA.csv\", header=TRUE)\n\nMake sure to load the dplyr package before proceeding with the following examples.\n\nlibrary(dplyr)\n\n\n10.2.1 Summarizing by crop type\nThe group_by function will split any operations applied to the dataframe into groups defined by one or more columns. For example, if we wanted to get the minimum and maximum years from the Year column for which crop data are available by crop type, we would type the following:\n\ndat %&gt;% \n  group_by(Crop) %&gt;% \n  summarise(yr_min = min(Year), yr_max=max(Year))\n\n# A tibble: 11 × 3\n   Crop         yr_min yr_max\n   &lt;chr&gt;         &lt;int&gt;  &lt;int&gt;\n 1 Barley         1961   2012\n 2 Buckwheat      1961   2012\n 3 Canary seed    1980   2012\n 4 Grain, mixed   1961   2012\n 5 Maize          1961   2012\n 6 Millet         1961   2012\n 7 Oats           1961   2012\n 8 Popcorn        1961   1982\n 9 Rye            1961   2012\n10 Sorghum        1961   2012\n11 Triticale      1989   2012\n\n\n\n\n10.2.2 Count the number of records in each group\nIn this example, we are identifying the number of records by Crop type. There are two ways this can be accomplished:\n\ndat %&gt;%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Year &gt;= 2005 & Year &lt;=2010, \n         Country==\"United States of America\") %&gt;%\n  group_by(Crop) %&gt;%\n  count()\n\nOr,\n\ndat %&gt;%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Year &gt;= 2005 & Year &lt;=2010, \n         Country==\"United States of America\") %&gt;%\n  group_by(Crop) %&gt;%\n  summarise(Count = n())\n\n# A tibble: 7 × 2\n  Crop      Count\n  &lt;chr&gt;     &lt;int&gt;\n1 Barley        6\n2 Buckwheat     6\n3 Maize         6\n4 Millet        6\n5 Oats          6\n6 Rye           6\n7 Sorghum       6\n\n\nThe former uses the count() function and the latter uses the summarise() and n() functions.\n\n\n10.2.3 Summarize by mean yield and year range\nHere’s another example where two variables are summarized in a single pipe.\n\ndat.grp &lt;- dat %&gt;%\n  filter(Information == \"Yield (Hg/Ha)\", \n         Year &gt;= 2005 & Year &lt;=2010, \n         Country==\"United States of America\") %&gt;%\n  group_by(Crop) %&gt;%\n  summarise( Yield = mean(Value), `Number of Years` = max(Year) - min(Year)) \n\ndat.grp\n\n# A tibble: 7 × 3\n  Crop       Yield `Number of Years`\n  &lt;chr&gt;      &lt;dbl&gt;             &lt;int&gt;\n1 Barley    35471.                 5\n2 Buckwheat 10418.                 5\n3 Maize     96151.                 5\n4 Millet    16548.                 5\n5 Oats      22619.                 5\n6 Rye       17132.                 5\n7 Sorghum   42258.                 5\n\n\n\n\n10.2.4 Normalizing each value in a group by the group median\nIn this example, we are subtracting each value in a group by that group’s median. This can be useful in identifying which year yields are higher than or lower than the median yield value within each crop group. We will concern ourselves with US yields only and sort the output by crop type. We’ll save the output dataframe as dat2.\n\ndat2 &lt;- dat %&gt;% \n  filter(Information == \"Yield (Hg/Ha)\",\n         Country == \"United States of America\") %&gt;%\n  select(Crop, Year, Value)                     %&gt;%\n  group_by(Crop)                                %&gt;%\n  mutate(NormYield = Value - median(Value))     %&gt;%\n  arrange(Crop)\n\nLet’s plot the normalized yields by year for Barley and add a 0 line representing the (normalized) central value.\n\nplot( NormYield ~ Year, dat2[dat2$Crop == \"Barley\",] )\nabline(h = 0, col=\"red\")\n\n\n\n\nThe relative distribution of points does not change, but the values do (they are re-scaled) allowing us to compare values based on some localized (group) context. This technique will prove very useful later on in the course when EDA topics are explored.\n\n\n10.2.5 dplyr’s output data structure\nSome of dplyr’s functions such as group_by/summarise generate a tibble data table. For example, the dat.grp object created in the last chunk of code is associated with a tb_df (a tibble).\n\nclass(dat.grp)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nA tibble table will behave a little differently than a data frame table when printing to a screen or subsetting its elements. In most cases, a tibble rendering of the table will not pose a problem in a workflow, however, this format may prove problematic with some older functions. To remedy this, you can force the dat.grp object to a standalone dataframe as follows:\n\ndat.df &lt;- as.data.frame(dat.grp)\nclass(dat.df)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "tidyr.html#introduction",
    "href": "tidyr.html#introduction",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nData tables come in different sizes and shape; they can be a very simple two column dataset or they can consist of many columns and “sub-columns”. Understanding its structure, and learning how to reshape it into a workable form is critical to an effective and error free analysis.\nFor example, a median earnings data table downloaded from the U.S. census bureau’s website might look something like this:\n\nWe are conditioned to think of a table as consisting of three components: rows, columns and data values. Implicit in this paradigm is that each column represents a unique attribute. However, this may not always be the case. For example, in the above table, each column represents two distinct variables: gender and educational attainment (two distinct sets of attributes).\n\nAnother way of describing a dataset is by defining its variable(s), values and observations. In the above example, we have four variables: gender, education, region and income. Each variable consists of either categorical values (e.g. region, gender and education) or numerical values (income).\nAn observation consists of a unique set of attribute values. For example the values West Region, Female, Graduate and $57,914 make up one observation: there is just one instance of these combined values in the data. This perspective affords us another option in presenting the dataset: we can assign each column its own variable, and each row its own observation.\n\nNote that each row of the table is part of a unique set of variable values. A dataset in this format may not be human “readable” (unlike its wide counterpart), but it’s the format of choice for many data analysis and visualization operations and one that will be used extensively in this course.\nThe next sections will demonstrate how one can convert a wide format to a long format and vice versa."
  },
  {
    "objectID": "tidyr.html#wide-and-long-table-formats",
    "href": "tidyr.html#wide-and-long-table-formats",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.2 Wide and long table formats",
    "text": "11.2 Wide and long table formats\nA 2014 Boston (Logan airport) flight data summary table will be used in this exercise. The summary displays average mean delay time (in minutes) by day of the work week and quarter.\n\ndf &lt;- data.frame( Weekday = c( \"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\" ),\n                  Q1      = c(  9.9 ,  4.9  ,  8.8 ,   12.2 ,  12.2 ),\n                  Q2      = c(  5.4 ,  9.7  , 11.1 ,   10.2 ,   8.1 ),\n                  Q3      = c(  8.8 ,  7.9  , 10.2 ,   9.2  ,   7.9 ),\n                  Q4      = c(  6.9 ,    5  ,  9.3 ,   9.7  ,   5.6 ) )\n\nReshaping a table involves modifying its layout (or “shape”). In our example, df is in a “wide” format.\n\ndf\n\n  Weekday   Q1   Q2   Q3  Q4\n1     Mon  9.9  5.4  8.8 6.9\n2    Tues  4.9  9.7  7.9 5.0\n3     Wed  8.8 11.1 10.2 9.3\n4   Thurs 12.2 10.2  9.2 9.7\n5     Fri 12.2  8.1  7.9 5.6\n\n\nThere are three unique variables: day of week, quarter of year, and mean departure delay.\n\n11.2.1 Creating a long table from a wide table\nA package that facilitates converting from wide to long (and vice versa) is tidyr. To go from wide to long we use the pivot_longer function. Note that if you are using a version of tidyr older than 1.0 you will want to use the gather() function..\nThe pivot_longer function takes three arguments:\n\ncols: list of columns that are to be collapsed. The columns can be referenced by column number or column name.\nnames_to: This is the name of the new column which will combine all column names (e.g. Q1, Q2, Q3 and Q4).\nvalues_to: This is the name of the new column which will combine all column values (e.g. average delay times) associated with each variable combination.\n\nIn our example, the line of code needed to re-express the table into a long form can be written in at least one of four ways:\n\nlibrary(tidyr)\ndf.long &lt;- pivot_longer(df, cols=2:5, names_to = \"Quarter\", values_to = \"Delay\")\n# or\ndf.long &lt;- pivot_longer(df, cols=-1, names_to = \"Quarter\", values_to = \"Delay\")\n# or\ndf.long &lt;- pivot_longer(df, cols=Q1:Q4, names_to = \"Quarter\", values_to = \"Delay\")\n# or\ndf.long &lt;- pivot_longer(df, cols=c(Q1,Q2,Q3,Q4), names_to = \"Quarter\", values_to = \"Delay\")\n\nAll four lines produce the same output, they differ only by how we are referencing the columns that are to be collapsed. Note that we assigned the names Quarter and Delay to the two new columns.\nThe first 10 lines of the output table are shown here. Note how each Delay value has its own row.\n\n\n# A tibble: 10 × 3\n   Weekday Quarter Delay\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 Mon     Q1        9.9\n 2 Mon     Q2        5.4\n 3 Mon     Q3        8.8\n 4 Mon     Q4        6.9\n 5 Tues    Q1        4.9\n 6 Tues    Q2        9.7\n 7 Tues    Q3        7.9\n 8 Tues    Q4        5  \n 9 Wed     Q1        8.8\n10 Wed     Q2       11.1\n\n\nThe following figure summarizes the wide to long conversion.\n\n\n\n11.2.2 Creating a wide table from a long table\nIf a table is to be used for a visual assessment of the values, a long format may be difficult to work with. A long table can be re-expressed into a wide form by picking the two variables that will define the new column names and values.\nContinuing with our example, we will convert df.long back to a wide format using the pivot_wider() function. This replaces the spread() function from earlier versions of tidyr (&lt;1.0). The pivot_wider() function takes at least two arguments:\n\nnames_from: Variable whose values will be converted to column names.\nvalues_from: Variable whose values will populate the table’s block of cell values.\n\n\ndf.wide &lt;- pivot_wider(df.long, names_from = Quarter, values_from = Delay) \n\nWe’ve now recreated the wide version of our table.\n\n\n# A tibble: 5 × 5\n  Weekday    Q1    Q2    Q3    Q4\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mon       9.9   5.4   8.8   6.9\n2 Tues      4.9   9.7   7.9   5  \n3 Wed       8.8  11.1  10.2   9.3\n4 Thurs    12.2  10.2   9.2   9.7\n5 Fri      12.2   8.1   7.9   5.6\n\n\nThe following figure summarizes the long to wide conversion.\n\n\nAdditional functionality in pivot_longer and pivot_wider are highlighted next."
  },
  {
    "objectID": "tidyr.html#advanced-pivot_longer-options",
    "href": "tidyr.html#advanced-pivot_longer-options",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.3 Advanced pivot_longer options",
    "text": "11.3 Advanced pivot_longer options\n\nLet’s start off with a subset of median income by sex and by work experience for 2017.\n\ndf2 &lt;- data.frame(state = c(\"Maine\", \"Massachusetts\", \n                             \"New Hampshire\", \"Vermont\"),\n                   male_fulltime   = c(50329,66066, 59962, 50530), \n                   male_other      = c(18099, 18574, 20274, 17709), \n                   female_fulltime = c(40054, 53841, 46178, 42198),\n                   female_other    = c(13781, 14981, 15121, 14422))\ndf2\n\n          state male_fulltime male_other female_fulltime female_other\n1         Maine         50329      18099           40054        13781\n2 Massachusetts         66066      18574           53841        14981\n3 New Hampshire         59962      20274           46178        15121\n4       Vermont         50530      17709           42198        14422\n\n\nAt first glance, it might seem that we have three variables, but upon closer examination, we see that we can tease out two variables from the column names: sex (male and female) and work experience (fulltime and other).\n\npivot_longer has an argument, names_sep, that is passed the character that is used to define the delimiter in the variable name. In our example, this character is _. Since the column values will be split across two variables we will also need to pass two column names to the names_to argument.\n\n\ndf2.long &lt;- pivot_longer(df2, cols = -state, \n                         names_to = c(\"sex\",\"work\"), \n                         names_sep = \"_\", \n                         values_to = \"income\")\ndf2.long\n\n# A tibble: 16 × 4\n   state         sex    work     income\n   &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n 1 Maine         male   fulltime  50329\n 2 Maine         male   other     18099\n 3 Maine         female fulltime  40054\n 4 Maine         female other     13781\n 5 Massachusetts male   fulltime  66066\n 6 Massachusetts male   other     18574\n 7 Massachusetts female fulltime  53841\n 8 Massachusetts female other     14981\n 9 New Hampshire male   fulltime  59962\n10 New Hampshire male   other     20274\n11 New Hampshire female fulltime  46178\n12 New Hampshire female other     15121\n13 Vermont       male   fulltime  50530\n14 Vermont       male   other     17709\n15 Vermont       female fulltime  42198\n16 Vermont       female other     14422"
  },
  {
    "objectID": "tidyr.html#advanced-pivot_wider-options",
    "href": "tidyr.html#advanced-pivot_wider-options",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.4 Advanced pivot_wider options",
    "text": "11.4 Advanced pivot_wider options\n \n\n11.4.1 Combining variable names when spreading\nContinuing with the df2.long dataframe, we can spread the long table back to a wide table while combining the sex and work variables. We’ll add the names_sep argument which defines the character to use to separate the two variable names. We’ll use a dot . separator in this example.\n\npivot_wider(df2.long, \n            names_from = c(sex,work), \n            values_from = income,\n            names_sep = \".\")\n\n# A tibble: 4 × 5\n  state         male.fulltime male.other female.fulltime female.other\n  &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 Maine                 50329      18099           40054        13781\n2 Massachusetts         66066      18574           53841        14981\n3 New Hampshire         59962      20274           46178        15121\n4 Vermont               50530      17709           42198        14422\n\n\n\n\n11.4.2 Spreading duplicate variable combinations\nIf your long table has more than one unique combination of variables, pivot_wider() will return a list. Note that if you have used the older version of tidyr (version &lt;1.0) its spread() function would have returned an error.\n\ndf3 &lt;- data.frame(var1 = c(\"a\", \"a\", \"b\", \"b\"),\n                  var2 = c(\"x\", \"x\", \"y\", \"y\"),\n                  val  = c(5,3,1,4))\ndf3\n\n  var1 var2 val\n1    a    x   5\n2    a    x   3\n3    b    y   1\n4    b    y   4\n\n\nHere, we have to val values for the combined values a an x–likewize for b and y, This will pose a problem when pivoting to w wide table since the intersection of a and x will result in two values.\n\nw1 &lt;- pivot_wider(df3, names_from = var2, values_from = val)\nw1\n\n# A tibble: 2 × 3\n  var1  x         y        \n  &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n1 a     &lt;dbl [2]&gt; &lt;NULL&gt;   \n2 b     &lt;NULL&gt;    &lt;dbl [2]&gt;\n\n\nSince the intersections of a:x and b:y each have two possible values, the function returns a list of values. For example, the intersection of column x and row a stores the values 5 and 3.\n\nunlist(w1[1,2])\n\nx1 x2 \n 5  3 \n\n\nAssuming that the duplicate records are not an erroneous entry, you will need to instruct the function on how to summarize the multiple values using the values_fn argument. For example, to return the maximum of the two values, type:\n\npivot_wider(df3, \n            names_from = var2, \n            values_from = val, \n            values_fn =  max)\n\n# A tibble: 2 × 3\n  var1      x     y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a         5    NA\n2 b        NA     4\n\n\nIf instead of summarizing the multiple values you wish to combine them with commas, you can type:\n\npivot_wider(df3, \n            names_from = var2, \n            values_from = val, \n            values_fn =  \\(x) paste(x, collapse=\",\"))\n\n# A tibble: 2 × 3\n  var1  x     y    \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 a     5,3   &lt;NA&gt; \n2 b     &lt;NA&gt;  1,4  \n\n\n\\(x) is a shorthand for function(x) in R version 4.1 and greater.\nYou’ll note the empty cells resulting from there not being a valid combination for a:y and b:x. You can specify the missing values using the values_fill argument. For example, to replace NA with 0 type:\n\npivot_wider(df3, names_from = var2, values_from = val, \n            values_fn   =  max,\n            values_fill =  0)\n\n# A tibble: 2 × 3\n  var1      x     y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a         5     0\n2 b         0     4"
  },
  {
    "objectID": "tidyr.html#additional-manipulation-options",
    "href": "tidyr.html#additional-manipulation-options",
    "title": "11  Tidying/reshaping tables with tidyr",
    "section": "11.5 Additional manipulation options",
    "text": "11.5 Additional manipulation options\nThe tidyr package offers other functions not directly tied to pivoting. Some of these functions are highlighted next.\n\n11.5.1 Separating elements in one column into separate columns\nTo split a column into two or more columns based on a existing column’s delimited value, use the separate_wider_delim() function.\nLet’s first create a delimited table.\n\ndf2.long &lt;- pivot_longer(df2, cols = -1, \n                         names_to = \"var1\", \n                         values_to = \"income\")\ndf2.long\n\n# A tibble: 16 × 3\n   state         var1            income\n   &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;\n 1 Maine         male_fulltime    50329\n 2 Maine         male_other       18099\n 3 Maine         female_fulltime  40054\n 4 Maine         female_other     13781\n 5 Massachusetts male_fulltime    66066\n 6 Massachusetts male_other       18574\n 7 Massachusetts female_fulltime  53841\n 8 Massachusetts female_other     14981\n 9 New Hampshire male_fulltime    59962\n10 New Hampshire male_other       20274\n11 New Hampshire female_fulltime  46178\n12 New Hampshire female_other     15121\n13 Vermont       male_fulltime    50530\n14 Vermont       male_other       17709\n15 Vermont       female_fulltime  42198\n16 Vermont       female_other     14422\n\n\nWe can now split var1 into its two components that we’ll name sex and work.\n\ndf2.sep &lt;- separate_wider_delim(df2.long, \n                    cols = var1, \n                    delim = \"_\", \n                    names = c(\"sex\", \"work\"))\ndf2.sep\n\n# A tibble: 16 × 4\n   state         sex    work     income\n   &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n 1 Maine         male   fulltime  50329\n 2 Maine         male   other     18099\n 3 Maine         female fulltime  40054\n 4 Maine         female other     13781\n 5 Massachusetts male   fulltime  66066\n 6 Massachusetts male   other     18574\n 7 Massachusetts female fulltime  53841\n 8 Massachusetts female other     14981\n 9 New Hampshire male   fulltime  59962\n10 New Hampshire male   other     20274\n11 New Hampshire female fulltime  46178\n12 New Hampshire female other     15121\n13 Vermont       male   fulltime  50530\n14 Vermont       male   other     17709\n15 Vermont       female fulltime  42198\n16 Vermont       female other     14422\n\n\n\n\n11.5.2 Splitting rows into multiple rows based on delimited values\nInstead of splitting one column into two columns, you might want to split the delimited variable into multiple rows. For example, to split df2.long into rows based on the var1 variable, use the separate_longer_delim function.\n\nseparate_longer_delim(df2.long, \n                      cols = var1, \n                      delim = \"_\")\n\n# A tibble: 32 × 3\n   state         var1     income\n   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt;\n 1 Maine         male      50329\n 2 Maine         fulltime  50329\n 3 Maine         male      18099\n 4 Maine         other     18099\n 5 Maine         female    40054\n 6 Maine         fulltime  40054\n 7 Maine         female    13781\n 8 Maine         other     13781\n 9 Massachusetts male      66066\n10 Massachusetts fulltime  66066\n# ℹ 22 more rows\n\n\nHere, instead of splitting var1 into two columns, we simply add a new line for each observation. Note that this results in duplicate income values.\n\n\n11.5.3 Replicate rows by count\nYou can expand rows based on a count column using the uncount() function. This is the opposite of a group_by(...) %&gt;% count() operation that tallies up the observations based on a grouping variable. Here, we’ll replicate rows based on the column count value.\n\ndf2b &lt;- data.frame(var1 = c(\"a\", \"b\"), \n                   var2 = c(\"x\", \"y\"), \n                   count = c(1, 3))\n\ndf2b\n\n  var1 var2 count\n1    a    x     1\n2    b    y     3\n\nuncount(df2b, count)\n\n  var1 var2\n1    a    x\n2    b    y\n3    b    y\n4    b    y\n\n\nIf you want to add an index column that identifies the replicated rows, add an .id argument.\n\nuncount(df2b, count, .id = \"id\")\n\n  var1 var2 id\n1    a    x  1\n2    b    y  1\n3    b    y  2\n4    b    y  3\n\n\n\n\n11.5.4 Combining elements from many columns into a single column\nAnother practical function in the tidyr package is unite(). It combines columns into a single column by chaining the contents of the combined columns. For example, the following table has hours, minutes and seconds in separate columns.\n\ndf &lt;- data.frame(\n      Index = c(1,   2,  3),\n      Hour  = c(2,  14, 20),\n      Min   = c(34,  2, 55),\n      Sec   = c(55, 17, 23))\n\ndf\n\n  Index Hour Min Sec\n1     1    2  34  55\n2     2   14   2  17\n3     3   20  55  23\n\n\nTo combine the three time elements into a single column, type:\n\ndf2 &lt;- unite(df,  col = Time, c(Hour, Min, Sec) , sep=\":\", remove =TRUE)\ndf2\n\n  Index     Time\n1     1  2:34:55\n2     2  14:2:17\n3     3 20:55:23\n\n\nThe col parameter defines the new column name; the parameter c(Hour, Min, Sec) defines the columns to be combined into column Time; sep=\":\" tells the function what characters are to be used to separate the elements (here, we are separating the time elements using :); remove=TRUE tells the function to remove the original columns used to create the new Time column.\n\n\n11.5.5 Creating unique combinations of variable values\nYou can use expand_grid to automatically generate a table with unique combinations of a set of variable values. For example, to fill a table with a combination of student names and homework assignments, type:\n\ndf3.long &lt;- expand_grid(\n  student    = c(\"Joe\", \"Jane\", \"Kim\"),  # Define all unique student names\n  assignment = c(paste0(\"HW\", 1:4)),     # Define all unique HW assignments\n  value      = NA)\ndf3.long\n\n# A tibble: 12 × 3\n   student assignment value\n   &lt;chr&gt;   &lt;chr&gt;      &lt;lgl&gt;\n 1 Joe     HW1        NA   \n 2 Joe     HW2        NA   \n 3 Joe     HW3        NA   \n 4 Joe     HW4        NA   \n 5 Jane    HW1        NA   \n 6 Jane    HW2        NA   \n 7 Jane    HW3        NA   \n 8 Jane    HW4        NA   \n 9 Kim     HW1        NA   \n10 Kim     HW2        NA   \n11 Kim     HW3        NA   \n12 Kim     HW4        NA   \n\n\nWe can then create a wide version of the table using pivot_wider.\n\npivot_wider(df3.long, names_from = assignment, values_from = value)\n\n# A tibble: 3 × 5\n  student HW1   HW2   HW3   HW4  \n  &lt;chr&gt;   &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n1 Joe     NA    NA    NA    NA   \n2 Jane    NA    NA    NA    NA   \n3 Kim     NA    NA    NA    NA   \n\n\n\n\n11.5.6 Expanding table with missing sets of values\nIt’s not uncommon to be handed a table with incomplete combinations of observations. For example, the following table gives us yield and data source values for each combination of year and grain type. However, several combinations of year/grain are missing.\n\ndf4.long &lt;- data.frame( Year  = c(1999,1999,2000,2000,2001,2003,2003,2005),\n                        Grain = c(\"Oats\", \"Corn\",\"Oats\", \"Corn\",\"Oats\", \"Oats\", \"Corn\",\"Oats\"),\n                        Yield = c(23,45,24,40,20,19,41,22),\n                        Src   = c(\"a\",\"a\",\"b\",\"c\",\"a\",\"a\",\"c\",\"a\"),\n                        stringsAsFactors = FALSE)\ndf4.long\n\n  Year Grain Yield Src\n1 1999  Oats    23   a\n2 1999  Corn    45   a\n3 2000  Oats    24   b\n4 2000  Corn    40   c\n5 2001  Oats    20   a\n6 2003  Oats    19   a\n7 2003  Corn    41   c\n8 2005  Oats    22   a\n\n\nWe are missing records for 2001 and Corn, 2003 and Corn, and data for both grains are missing for 2002 and 2004. To add rows for all missing pairs of year/grain values, use the complete function. Here, we’ll assign 0 to missing Yield values and NA to the Src values.\n\ndf.all &lt;- complete(df4.long, Year=1999:2005, Grain=  c(\"Oats\", \"Corn\"),\n                   fill = list(Yield = 0, Src = NA))\ndf.all\n\n# A tibble: 14 × 4\n    Year Grain Yield Src  \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n 1  1999 Corn     45 a    \n 2  1999 Oats     23 a    \n 3  2000 Corn     40 c    \n 4  2000 Oats     24 b    \n 5  2001 Corn      0 &lt;NA&gt; \n 6  2001 Oats     20 a    \n 7  2002 Corn      0 &lt;NA&gt; \n 8  2002 Oats      0 &lt;NA&gt; \n 9  2003 Corn     41 c    \n10  2003 Oats     19 a    \n11  2004 Corn      0 &lt;NA&gt; \n12  2004 Oats      0 &lt;NA&gt; \n13  2005 Corn      0 &lt;NA&gt; \n14  2005 Oats     22 a    \n\n\nNOTE 1: The dataframe should be ungrouped using ungroup() before calling complete() if a group_by() operation was previously applied to the table.\nNOTE 2: If one of the columns used to complete the set of unique values is a factor, then passing that column name as an argument will automatically create unique sets of values from that factor’s levels.\nIf you want to show just the missing rows, use dplyr::anti_join().\n\ndplyr::anti_join(df.all, df4.long)\n\n# A tibble: 6 × 4\n   Year Grain Yield Src  \n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n1  2001 Corn      0 &lt;NA&gt; \n2  2002 Corn      0 &lt;NA&gt; \n3  2002 Oats      0 &lt;NA&gt; \n4  2004 Corn      0 &lt;NA&gt; \n5  2004 Oats      0 &lt;NA&gt; \n6  2005 Corn      0 &lt;NA&gt; \n\n\n\n\n11.5.7 Filling date/time gaps in a table\n\n11.5.7.1 Example using a Date object class\nThis is another example that makes use of the complete function. Here, we seek to add missing date/time values in a time series table.\n\ndf.dt &lt;- data.frame( Date = as.Date(c(\"2000-01-01\", \"2000-01-03\", \"2000-01-05\")),\n                     Value = c(1,3,5))\ndf.dt\n\n        Date Value\n1 2000-01-01     1\n2 2000-01-03     3\n3 2000-01-05     5\n\n\nTo fill the missing dates between the minimum and maximum dates in df.dt, we first need to create the sequence of dates between these minimum and maximum values. We could do this in the complete function, but it will be clearer to the reader if these operations are performed separately.\n\ndates.all &lt;- seq(min(df.dt$Date), max(df.dt$Date), by = \"1 day\")\n\nNext, we pass the complete set of dates, dates.all to the complete function. We’ll assign NA to the missing Value values.\n\ndf.dt.all &lt;- complete(df.dt, Date = dates.all, fill = list(Value = NA))\ndf.dt.all\n\n# A tibble: 5 × 2\n  Date       Value\n  &lt;date&gt;     &lt;dbl&gt;\n1 2000-01-01     1\n2 2000-01-02    NA\n3 2000-01-03     3\n4 2000-01-04    NA\n5 2000-01-05     5\n\n\nThe seq function used in this example has a special method for Date objects. As such, it accepts a unique set of incremental parameters (via the by = parameter). In the above example, we are asking the function to increment the date object by 1 day. Other increments can be applied to a sequenced Date object as shown in the following table.\n\n\n\n\n\n\n\nIncrement units\nDescription\n\n\n\n\nday\nX number of days between dates\n\n\nweek\nX number of weeks between dates\n\n\nmonths\nX number of months between dates\n\n\nquarter\nX number of quarters between dates\n\n\nyear\nX number of years between dates\n\n\n\n\n\n11.5.7.2 Example using a Posix object class\nRecall that if a time element is to be stored in a date, the date object becomes a Posix object class. This adds additional increment units to the seq function. In the following example, we create a new data frame (with a time value), then fill the table with date/times at 6 hour increments.\n\ndf.tm &lt;- data.frame( Date = as.POSIXct(c(\"2000-01-01 18:00 EST\", \n                                         \"2000-01-02 6:00 EST\", \n                                         \"2000-01-03 12:00 EST\")),\n                     Value = c(1,3,5))\n\n\npos.all &lt;- seq(min(df.tm$Date), max(df.tm$Date), by = \"6 hour\")\n\n\ndf.tm.all &lt;- complete(df.tm, Date = pos.all, fill = list(Value = NA))\ndf.tm.all\n\n# A tibble: 8 × 2\n  Date                Value\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2000-01-01 18:00:00     1\n2 2000-01-02 00:00:00    NA\n3 2000-01-02 06:00:00     3\n4 2000-01-02 12:00:00    NA\n5 2000-01-02 18:00:00    NA\n6 2000-01-03 00:00:00    NA\n7 2000-01-03 06:00:00    NA\n8 2000-01-03 12:00:00     5\n\n\nIn this example the seq method for the Posix object adds a few more increment units to those available with the seq.Date method shown in the previous table. These include:\n\n\n\n\n\n\n\nIncrement units\nDescription\n\n\n\n\nsec\nX number of seconds between timestamps.\n\n\nmin\nX number of minutes between timestamps.\n\n\nhour\nX number of hours between timestamps.\n\n\nDSTday\nX number of days between timestamps while taking into account changes to/from daylight savings time.\n\n\n\n\n\n\n11.5.8 Identifying missing combination in dataframes\nIn an earlier example, we had the function automatically add the missing combinations using explicitly defined ranges of values. If you just want to output the missing combinations from the existing set of values in both columns, use the expand() function.\n\np.all &lt;- expand(df4.long, Year, Grain) # List all possible combinations\np.all\n\n# A tibble: 10 × 2\n    Year Grain\n   &lt;dbl&gt; &lt;chr&gt;\n 1  1999 Corn \n 2  1999 Oats \n 3  2000 Corn \n 4  2000 Oats \n 5  2001 Corn \n 6  2001 Oats \n 7  2003 Corn \n 8  2003 Oats \n 9  2005 Corn \n10  2005 Oats \n\n\nNote that this only outputs the columns of interest. If you need to see the other columns in the output, perform a join.\n\ndplyr::left_join(p.all, df4.long, by=c(\"Year\", \"Grain\"))\n\n# A tibble: 10 × 4\n    Year Grain Yield Src  \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n 1  1999 Corn     45 a    \n 2  1999 Oats     23 a    \n 3  2000 Corn     40 c    \n 4  2000 Oats     24 b    \n 5  2001 Corn     NA &lt;NA&gt; \n 6  2001 Oats     20 a    \n 7  2003 Corn     41 c    \n 8  2003 Oats     19 a    \n 9  2005 Corn     NA &lt;NA&gt; \n10  2005 Oats     22 a    \n\n\n\n\n11.5.9 Auto-fill down or up\nThe fill() function is used to replace NA values with the closest non-NA value in a column. For example, to fill down, set the .direction argument to \"down\".\n\ndf5 &lt;- data.frame(Month = 1:12, \n                 Year = c(2000, rep(NA, 4),2001, rep(NA,6)))\ndf5\n\n   Month Year\n1      1 2000\n2      2   NA\n3      3   NA\n4      4   NA\n5      5   NA\n6      6 2001\n7      7   NA\n8      8   NA\n9      9   NA\n10    10   NA\n11    11   NA\n12    12   NA\n\nfill(df5, Year, .direction=\"down\")\n\n   Month Year\n1      1 2000\n2      2 2000\n3      3 2000\n4      4 2000\n5      5 2000\n6      6 2001\n7      7 2001\n8      8 2001\n9      9 2001\n10    10 2001\n11    11 2001\n12    12 2001"
  },
  {
    "objectID": "joins.html#left-join",
    "href": "joins.html#left-join",
    "title": "12  Joining Data Tables",
    "section": "12.1 Left join",
    "text": "12.1 Left join\nIn this example, if a join element in df does not exist in dj, NA will be assigned to column z. In other words, all elements in df will exist in the output regardless if a matching element is found in dj. Note that the output is sorted in the same order as df (the left table).\n\nleft_join(df, dj, by=\"y\")\n\n   x y     z\n1  1 a apple\n2 23 b  pear\n3  4 b  pear\n4 43 b  pear\n5  2 a apple\n6 17 d  &lt;NA&gt;"
  },
  {
    "objectID": "joins.html#right-join",
    "href": "joins.html#right-join",
    "title": "12  Joining Data Tables",
    "section": "12.2 Right join",
    "text": "12.2 Right join\nIf a join element in df does not exist in dj, that element is removed from the output. A few additional important notes follow:\n\nAll elements in dj appear at least once in the output (even if they don’t have a match in df in which case an NA value is added),\nThe output table is sorted in the order in which the y elements appear in dj.\nElement y will appear as many times as there matching ys in df.\n\n\nright_join(df, dj, by=\"y\")\n\n   x y      z\n1  1 a  apple\n2 23 b   pear\n3  4 b   pear\n4 43 b   pear\n5  2 a  apple\n6 NA c orange"
  },
  {
    "objectID": "joins.html#inner-join",
    "href": "joins.html#inner-join",
    "title": "12  Joining Data Tables",
    "section": "12.3 Inner join",
    "text": "12.3 Inner join\nIn this example, only matching elements in both df and dj are saved in the output. This is basically an “intersection” of both tables.\n\ninner_join(df, dj, by=\"y\")\n\n   x y     z\n1  1 a apple\n2 23 b  pear\n3  4 b  pear\n4 43 b  pear\n5  2 a apple"
  },
  {
    "objectID": "joins.html#full-join",
    "href": "joins.html#full-join",
    "title": "12  Joining Data Tables",
    "section": "12.4 Full join",
    "text": "12.4 Full join\nIn this example, all elements in both df and dj are present in the output. For non-matching pairs, NA values are supplied. This is basically a “union” of both tables.\n\nfull_join(df, dj, by=\"y\")\n\n   x y      z\n1  1 a  apple\n2 23 b   pear\n3  4 b   pear\n4 43 b   pear\n5  2 a  apple\n6 17 d   &lt;NA&gt;\n7 NA c orange"
  },
  {
    "objectID": "joins.html#joins-in-a-piping-operation",
    "href": "joins.html#joins-in-a-piping-operation",
    "title": "12  Joining Data Tables",
    "section": "12.5 Joins in a piping operation",
    "text": "12.5 Joins in a piping operation\nThe afrementioned joining functions can be used with pipes. For example:\n\ndf %&gt;% \n  left_join(dj, by = \"y\")\n\n   x y     z\n1  1 a apple\n2 23 b  pear\n3  4 b  pear\n4 43 b  pear\n5  2 a apple\n6 17 d  &lt;NA&gt;"
  },
  {
    "objectID": "joins.html#note1",
    "href": "joins.html#note1",
    "title": "12  Joining Data Tables",
    "section": "12.6 A note about column names",
    "text": "12.6 A note about column names\nIf the common columns in both tables have different names, you will need to modify the by = argument as by = c(\"left_col\" = \"right_col\"). For example,\n\nlibrary(dplyr)\n\ndf &lt;- data.frame( x = c(1, 23, 4, 43, 2, 17),\n                  y1 = c(\"a\", \"b\", \"b\", \"b\", \"a\", \"d\"),\n                  stringsAsFactors = FALSE)\n\ndj &lt;- data.frame( z = c(\"apple\", \"pear\", \"orange\"),\n                  y2 = c(\"a\", \"b\", \"c\"),\n                  stringsAsFactors = FALSE)\n\nleft_join(df, dj, by = c(\"y1\" = \"y2\"))\n\n   x y1     z\n1  1  a apple\n2 23  b  pear\n3  4  b  pear\n4 43  b  pear\n5  2  a apple\n6 17  d  &lt;NA&gt;"
  },
  {
    "objectID": "base_plots.html#loading-the-data",
    "href": "base_plots.html#loading-the-data",
    "title": "13  Base plotting environment",
    "section": "13.1 Loading the data",
    "text": "13.1 Loading the data\nThe data files used in this tutorial can be downloaded from the course’s website as follows:\n\nload(url(\"https://github.com/mgimond/ES218/blob/gh-pages/Data/dat1_2.RData?raw=true\"))\n\nThis should load several data frame objects into your R session (note that not all are used in this exercise). The dat1l dataframe is a long table version of the crop yield dataset.\n\nhead(dat1l, 3)\n\n  Year   Crop    Yield\n1 1961 Barley 16488.52\n2 1962 Barley 18839.00\n3 1963 Barley 18808.27\n\n\nThe dat1w dataframe is a wide table version of the same dataset.\n\nhead(dat1w, 3)\n\n  Year   Barley Buckwheat    Maize     Oats      Rye\n1 1961 16488.52  10886.67 39183.63 15171.26 11121.79\n2 1962 18839.00  11737.50 40620.80 16224.60 12892.77\n3 1963 18808.27  11995.00 42595.55 16253.04 11524.11\n\n\nThe dat2 dataframe is a wide table representation of income by county and by various income and educational attainment levels. The first few lines and columns are shown here:\n\ndat2[1:3, 1:7]\n\n   County State B20004001 B20004002 B20004003 B20004004 B20004005\n1 Autauga    al     35881     17407     30169     35327     54917\n2 Baldwin    al     31439     16970     25414     31312     44940\n3 Barbour    al     25201     15643     20946     24201     42629"
  },
  {
    "objectID": "base_plots.html#base-plotting-functions",
    "href": "base_plots.html#base-plotting-functions",
    "title": "13  Base plotting environment",
    "section": "13.2 Base plotting functions",
    "text": "13.2 Base plotting functions\n  \n\n13.2.1 Point and line plots\nThe most commonly used plot function in R is plot() which is a generic function used for different plot types. For example, to plot male population median income (dat2$B20004007) vs female population median income (dat2$B20004013) for each county, type:\n\nplot(B20004007 ~ B20004013, dat = dat2)\n\n\n\n\nThe above plot command takes two arguments: B20004007 ~ B20004013 which is to be read as “plot variable B20004007 as a function of B20004013”, and dat = dat2 which tells the plot function which data frame to extract the variables from. Another way to call this command is to type:\n\nplot(dat2$B20004007 ~ dat2$B20004013)\n\nThe plot function can take on many other arguments to help tweak the default plot options. For example, we may want to change the axis labels to something more descriptive than the table column names:\n\nplot(B20004007 ~ B20004013, dat = dat2, \n      xlab = \"Female median income ($)\", \n      ylab = \"Male median income ($)\")\n\n\n\n\nThere are over 3000 observations in this dataset which makes it difficult the see what may be going on in the cloud of points. We can change the symbol type to solid fill,pch = 20, and set its color to 90% transparent (or 10% opaque) using the expression col = rgb(0, 0, 0, 0.10). The rgb() function defines the intensities for each of the display’s primary colors (on a scale of 0 to 1). The primary colors are red, green and blue. The forth value is optional and provides the fraction opaqueness with a value of 1 being completely opaque.\n\nplot(B20004007 ~ B20004013, dat = dat2, \n     xlab = \"Female median income ($)\", \n     ylab = \"Male median income ($)\", \n     pch = 20, col = rgb(0, 0, 0, 0.10) )\n\n\n\n\nThe plot could use additional tweaking, but it may be best to build the plot from scratch as will be demonstrated a few sections down.\nBy default, the plot command will plot points and not lines. To plot lines, add the type=\"l\" parameter to the plot function. For example, to plot oats crop yield as a function of year from our dat1w dataset, type:\n\nplot(Oats ~ Year, dat = dat1w, type=\"l\", \n     ylab = \"Oats yield (Hg/Ha)\" )\n\n\n\n\nTo plot both points and line, set the type parameter to \"b\" (for both). We’ll also set the point symbol to 20.\n\nplot(Oats ~ Year, dat = dat1w, type = \"b\", pch = 20, \n     ylab = \"Oats yield (Hg/Ha)\" )\n\n\n\n\nThe plot command can only graph on variable. If you want to add another variable, you will need to call the lines function. We will assign a different line type to this second variable (lty = 2):\n\nplot(Oats ~ Year, dat = dat1w, type = \"l\", \n     ylab = \"Oats yield (Hg/Ha)\" )\nlines(Barley ~ Year, dat = dat1w, lty = 2)\n\n\n\n\nNote how the plot does not automatically re-scale to accommodate the new line. The plot is a static object meaning that we need to define the axes limits before calling the original plot function. Both axes limits can be set using the xlim and ylim parameters. We don’t need to set the x-axis range since both variables cover the same year range. We will therefore only focus on the y-axis limits. We can grab both the minimum and maximum values for the variables Oats and Barley using the range function, then pass the range to the ylim parameter in the call to plot.\n\ny.rng &lt;- range( c(dat1w$Oats, dat1w$Barley) )\nplot(Oats ~ Year, dat = dat1w, type = \"l\", ylim = y.rng,\n     ylab = \"Oats yield (Hg/Ha)\")\nlines(Barley ~ Year, dat = dat1w, lty = 2)\n\n\n\n\nPoint plots from different variables can also be combined into a single plot using the points function in lieu of the lines function. In the following example, male vs. female income for population having a high school degree (blue dots) and a Bachelor’s degree (red dots) will be overlaid on the same plot. We’ll also add a legend in the top-right corner.\n\ny.rng &lt;- range( c(dat2$B20004009, dat2$B20004011) , na.rm = TRUE) \nx.rng &lt;- range( c(dat2$B20004015, dat2$B20004017) , na.rm = TRUE) \n\n# Plot income for HS degree\nplot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0, 0, 1, 0.10),\n     xlab = \"Female median income ($)\", \n     ylab = \"Male median income ($)\", \n     xlim = x.rng, ylim = y.rng)\n\n# Add points for Bachelor's degree\npoints(dat2$B20004011 ~ dat2$B20004017, dat = dat2, pch = 20, \n       col = rgb(1,0,0,0.10))\n\n# Add legend\nlegend(\"topright\", c(\"HS Degree\", \"Bachelor's\"), pch = 20, \n       col = c(rgb(0, 0, 1, 1), rgb(1, 0, 0, 1) ))\n\n\n\n\nThe na.rm = TRUE option is added as a parameter in the range function to prevent an NA value in the data from returning an NA value in the range.\nPoint symbols are defined by a numeric code. The following figure shows the list of point symbols available in R along with their numeric designation as used with the pch = argument. The symbol’s color can be defined using the col parameter. For symbols 21 through 25, which have a two-color scheme, col applies to the outline color (blue in the following figure) and bg parameter applies to the fill color (red in the following figure).\n\n\n\n\n\nYou can define the color using the rgb() function, or by a color name such as col = \"red\" or col = \"bisque\". For a full list of color names, type colors() at a command prompt.\nLine types can also be customized in the plot function using the lty = parameter. There are six different line types, each identified by a number:\n\n\n\n\n\n\n\n13.2.2 Boxplots\nA boxplot is one of many graphical tools used to summarize the distribution of a data batch. The graphic consists of a “box” that depicts the range covered by 50% of the data (aka the interquartile range, IQR), a horizontal line that displays the median, and “whiskers” that depict 1.5 times the IQR or the largest (for the top half) or smallest (for the bottom half) values.\nFor example, we can summarize the income range for all individuals as follows:\n\nboxplot(dat2$B20004001, na.rm = TRUE)\n\n\n\n\nNote that the boxplot function has no option to specify the data frame as is the case with the plot function; we must therefore pass it both the data frame name and the variable as a single argument (i.e. dat2$B20004001).\nSeveral variables can be summarized on the same plot.\n\nboxplot(dat2$B20004001, dat2$B20004007, dat2$B20004013, \n        names = c(\"All\", \"Male\", \"Female\"), main = \"Median income ($)\")\n\n\n\n\nThe names argument labels the x-axis and the main argument labels the plot title.\nThe outliers can be removed from the plot, if desired, by setting the outline parameter to FALSE:\n\nboxplot(dat2$B20004001, dat2$B20004007, dat2$B20004013, \n        names = c(\"All\", \"Male\", \"Female\"), main = \"Median income ($)\", \n        outline = FALSE)\n\n\n\n\nThe boxplot graph can also be plotted horizontally by setting the horizontal parameter to TRUE:\n\nboxplot(dat2$B20004001, dat2$B20004007, dat2$B20004013, \n        names = c(\"All\", \"Male\", \"Female\"), main = \"Median income ($)\", \n        outline = FALSE, horizontal = TRUE)\n\n\n\n\nThe last two plots highlight one downside in using a table in a wide format: the long series of column names passed to the boxplot function. It’s more practical to store such data in long form. To demonstrate this, let’s switch back to the crop data. To plot all columns in dat1w, we would need to type:\n\nboxplot(dat1w$Barley, dat1w$Buckwheat, dat1w$Maize, dat1w$Oats,dat1w$Rye,\n        names=c(\"Barley\", \"Buckwheat\", \"Maize\", \"Oats\", \"Rye\"))\n\n\n\n\nIf you use the long version of that table, the command looks like this:\n\nboxplot(Yield ~ Crop, dat1l)\n\n\n\n\nwhere ~ Crop tells the function to split the boxplots across unique Crop levels.\nOne can order the boxplots based on the median values. By default, boxplot will order the boxplots following the factor’s level order. In our example, the crop levels are ordered alphabetically. To reorder the levels following the median values of yields across each level, we can use the reorder() function:\n\ndat1l$Crop.ord &lt;- reorder(dat1l$Crop, dat1l$Yield, median)\n\nThis creates a new variable called Crop.ord whose values mirror those in variable Crop but differ in the underlying level order:\n\nlevels(dat1l$Crop.ord)\n\n[1] \"Buckwheat\" \"Rye\"       \"Oats\"      \"Barley\"    \"Maize\"    \n\n\nIf we wanted the order to be in descending order, we would prefix the value parameter with the negative operator as in reorder(dat1l$Crop, -dat1l$Yield, median).\nThe function reorder takes three arguments: the factor whose levels are to be reordered (Crop), the value whose quantity will determine the new order (Yield) and the statistic that will be used to summarize the values across each factor’s level (median).\nThe modified boxplot expression now looks like:\n\nboxplot(Yield ~ Crop.ord, dat1l)\n\n\n\n\nAlternatively, you could have embedded the reorder function directly in the plotting function.\n\nboxplot(Yield ~ reorder(Crop, Yield, median), dat1l)\n\n\n\n\n\n\n13.2.3 Histograms\nThe histogram is another form of data distribution visualization. It consists of partitioning a batch of values into intervals of equal length then tallying their count in each interval. The height of each bar represents these counts. For example, we can plot the histogram of maize yields using the hist function as follows:\n\nhist(dat1w$Maize, xlab = \"Maize\", main = NA)\n\n\n\n\nThe main = NA argument suppresses the plot title.\nTo control the number of bins add the breaks argument. The breaks argument can be passed different types of values. The simplest value is the desired number of bins. Note, however, that you might not necessarily get the number of bins defined with the breaks argument. For example assigning the value of 10 to breaks generates a 14 bin histogram.\n\nhist(dat1w$Maize, xlab = \"Maize\", main = NA, breaks = 10)\n\n\n\n\nThe documentation states that the breaks value “is a suggestion only as the breakpoints will be set to pretty values”. pretty refers to a function that rounds values to powers of 1, 2 or 5 times a power of 10.\nIf you want total control of the bin numbers, manually create the breaks as follows:\n\nn &lt;- 10  # Define the number of bin\nminx &lt;- min(dat1w$Maize, na.rm = TRUE)\nmaxx &lt;- max(dat1w$Maize, na.rm = TRUE)\nbins &lt;- seq(minx, maxx, length.out = n +1)\n\nhist(dat1w$Maize, xlab = \"Maize\", main = NA, breaks = bins)\n\n\n\n\n\n\n13.2.4 Density plot\nHistograms have their pitfalls. For example, the number of bins can drastically affect the appearance of a distribution. One alternative is the density plot which, for a series of x-values, computes the density of observations at each x-value. This generates a “smoothed” distribution of values.\nUnlike the other plotting functions, density does not generate a plot but instead, it generates a list object. The output of density can be wrapped with a plot function to generate the plot.\n\ndens &lt;- density(dat1w$Maize)\nplot(dens, main = \"Density distribution of Maize yields\")\n\n\n\n\nYou can control the bandwidth using the bw argument. For example:\n\ndens &lt;- density(dat1w$Maize, bw = 4000)\nplot(dens, main = \"Density distribution of Maize yields\")\n\n\n\n\nThe bandwidth parameter adopts the variable’s units."
  },
  {
    "objectID": "base_plots.html#customizing-plots",
    "href": "base_plots.html#customizing-plots",
    "title": "13  Base plotting environment",
    "section": "13.3 Customizing plots",
    "text": "13.3 Customizing plots\nSo far, you have learned how to customize point and line symbols, but this may not be enough. You might want to modify other graphic elements such as the axes layout and label formats for publication. Let’s see how we can further customize a plot of median income for male and female population having attained a HS degree.\nFirst, we plot the points but omit the axes and its labels with the parameters axes = FALSE, xlab = NA, ylab = NA. We will want both axes to cover the same range of values, so we will use the range function to find min and max values for both male and female incomes.\nNext, we draw the x axis using the axis function. The first parameter to this function is a number that indicates which axis is to be drawn (i.e. 1=bottom x, 2=left y, 3=top x and 4=right y). We will then use the mtext function to place the axis label under the axis line.\n\n# Plot the points without the axes\nrng &lt;- range(dat2$B20004009, dat2$B20004015, na.rm = TRUE)\nplot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0,0,0,0.15), \n     xlim = rng, ylim = rng, axes = FALSE, xlab = NA, ylab = NA )\n\n# Plot the x-axis\nlab &lt;- c(\"5,000\", \"25,000\", \"45,000\", \"$65,000\")\naxis(1, at = seq(5000, 65000, length.out = 4),  label = lab)\n\n# Plot x label\nmtext(\"Female median income (HS degree)\", side = 1, line = 2)\n\n\n\n\nNext, we will tackle the y-axis. We will rotate both the tic labels and axis label horizontally and place the axis label at the top of the axis. This will involve a different approach to that used for the x-axis. First, we need to identify each plot region’s corner coordinate values using the par function. Second, we will use the text function instead of the mtext function to place the axis label.\nFirst, let’s plot the y-axis with the custom tic labels.\n\n# Plot the y-axis\naxis(2, las = 1, at = seq(5000,65000, length.out = 4), label = lab)\n\n\n\n\nNow let’s extract the plot’s corner coordinate values.\n\nloc &lt;- par(\"usr\")\nloc\n\n[1]  3850 68650  3850 68650\n\n\nThe corner location coordinate values are in the plot’s x and y units. We want to place the label in the upper left hand corner whose coordinate values are loc[1]= 3850 and loc[2]= 68650.\n\ntext(loc[1], loc[4], \"Male median\\nincome\", pos = 3, adj = 1, xpd = TRUE)\n\n\n\n\nThe string \\n in the text \"Median\\nIncome\" is interpreted in R as being a carriage return–i.e it forces the text that follows this string to be printed on the next line. The other parameters of interest are pos and adj that position and adjust the label location (type ?axis for more information on axis parameters) and the parameter xpd=TRUE allows for the text function to display text outside of the plot region."
  },
  {
    "objectID": "base_plots.html#exporting-plots-to-image-file-formats",
    "href": "base_plots.html#exporting-plots-to-image-file-formats",
    "title": "13  Base plotting environment",
    "section": "13.4 Exporting plots to image file formats",
    "text": "13.4 Exporting plots to image file formats\nYou might need to export your plots as standalone image files for publications. R will export to many different raster image file formats such as jpg, png, gif and tiff, and several vector file formats such as PostScript, svg and PDF. You can specify the image resolution (in dpi), the image height and width, and the size of the margins.\nThe following example saves the last plot as an uncompressed tiff file with a 5”x6” dimension and a resolution of 300 dpi. This is accomplished by simply book-ending the plotting routines between the tiff() and dev.off() functions:\n\ntiff(filename = \"fig1.tif\", width = 5, height = 6, units = \"in\",\n     compression = \"none\", res = 300)\n\n  # Plot the points without the axes\n  rng &lt;- range(dat2$B20004009, dat2$B20004015, na.rm = TRUE)\n  plot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0,0,0,0.10), \n       xlim = rng, ylim = rng, axes = FALSE, xlab = NA, ylab = NA )\n  # Plot the x-axis\n  lab &lt;- c(\"5,000\", \"25,000\", \"45,000\", \"$65,000\")\n  axis(1, at = seq(5000,65000, length.out = 4),  label = lab)\n  # Plot x label\n  mtext(\"Female median income (HS degree)\", side = 1, line = 2)\n  # Plot the y-axis\n  axis(2, las = 1, at = seq(5000,65000, length.out = 4), label = lab)\n  text(loc[1], loc[4], \"Male median\\nincome\", pos = 3, adj = 1, xpd = TRUE)\n\ndev.off()\n\nTo save the same plot to a pdf file format, simply substitute tiff() with pdf() and adjust the parameters as needed:\n\npdf(file = \"fig1.pdf\", width = 5, height = 6)\n\n  # Plot the points without the axes\n  rng &lt;- range(dat2$B20004009, dat2$B20004015, na.rm = TRUE)\n  plot(B20004009 ~ B20004015, dat = dat2, pch = 20, col = rgb(0,0,0,0.15), \n       xlim = rng, ylim = rng, axes = FALSE, xlab = NA, ylab = NA )\n  # Plot the x-axis\n  lab &lt;- c(\"5,000\", \"25,000\", \"45,000\", \"$65,000\")\n  axis(1, at = seq(5000,65000, length.out=4),  label = lab)\n  # Plot x label\n  mtext(\"Female median income (HS degree)\", side = 1, line = 2)\n  # Plot the y-axis\n  axis(2, las = 1, at = seq(5000,65000, length.out = 4), label = lab)\n  text(loc[1], loc[4], \"Male median\\nincome\", pos = 3, adj = 1, xpd = TRUE)\n\ndev.off()"
  },
  {
    "objectID": "lattice_plot.html#data",
    "href": "lattice_plot.html#data",
    "title": "14  Lattice plotting environment",
    "section": "14.1 Data",
    "text": "14.1 Data\nThe data files used in this tutorial can be downloaded from the course’s website as follows:\n\nload(url(\"https://github.com/mgimond/ES218/blob/gh-pages/Data/dat1_2.RData?raw=true\"))\n\nThis should load several data frame objects into your R session (note that not all are used in this exercise). The first three lines of data frames used in the following sections are shown below:\n\nhead(dat1l, 3)\n\n  Year   Crop    Yield\n1 1961 Barley 16488.52\n2 1962 Barley 18839.00\n3 1963 Barley 18808.27\n\nhead(dat1w, 3)\n\n  Year   Barley Buckwheat    Maize     Oats      Rye\n1 1961 16488.52  10886.67 39183.63 15171.26 11121.79\n2 1962 18839.00  11737.50 40620.80 16224.60 12892.77\n3 1963 18808.27  11995.00 42595.55 16253.04 11524.11\n\nhead(dat2b, 3)\n\n  State  County Level Gender value Region\n1    al Autauga   All    All 35881  South\n2    al Baldwin   All    All 31439  South\n3    al Barbour   All    All 25201  South\n\nhead(dat2c, 3)\n\n  State                 County Level Region   All     F     M\n1    ak Aleutians East Borough   All   West 21953 20164 22940\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885\n3    ak Aleutians East Borough    HS   West 20770 19671 21192"
  },
  {
    "objectID": "lattice_plot.html#the-lattice-package",
    "href": "lattice_plot.html#the-lattice-package",
    "title": "14  Lattice plotting environment",
    "section": "14.2 The lattice package",
    "text": "14.2 The lattice package\nThe lattice package is an implementation of the Trellis display (or multi-panel display) used to visualize multivariate data or, more specifically, to visualize the dependencies and interactions between multiple variables. For example, the following lattice (or Trellis) plot displays the counts of people (grouped by gender, adult/child and economic status or crew) who perished on the Titanic.\n\n\n\n\n\nIn the following example, a scatter plot is produced for different crime types by US region.\n\n\n\n\n\nExamples of some of the most common lattice plot types follow. Note that you will need to load the lattice package to run the following chunks of code.\n\nlibrary(lattice)"
  },
  {
    "objectID": "lattice_plot.html#displaying-univariate-distributions",
    "href": "lattice_plot.html#displaying-univariate-distributions",
    "title": "14  Lattice plotting environment",
    "section": "14.3 Displaying univariate distributions",
    "text": "14.3 Displaying univariate distributions\nLet’s look at a simple example where the density distribution of yields (across all years) is conditioned on crop type.\n\ndensityplot( ~ Yield | Crop, dat1l, plot.points = \"\", layout = c(5, 1))\n\n\n\n\nThe layout=c(5,1) parameter instructs the lattice function to layout the panels along 5 columns and 1 row. If you wanted the plots to be displayed in 5 rows and 1 column you would simply type layout=c(1,5).\nThe first thing to note is that the x-axis range is the same for all five plots. At first, this may seem as an inefficient use of the range of x values–for example, the buckwheat density plot is squeezed in the left-hand side of the plot, but it is important to understand the purpose of a trellis plot: to facilitate comparison of plots. By ensuring that all scale ranges remain the same, we can easily compare the plot shape. In the above plot, it is clear that the yield of buckwheat remains consistent across the years compared to Maize which displays the greatest variability across the years.\nThe lattice package also has a histogram function:\n\nhistogram( ~ Yield | Crop, dat1l, plot.points = \"\", nint = 20, layout = c(5, 1))\n\n\n\n\nNote the importance of having a table shaped in “long form”. The density plots are conditioned on crop type which requires that crop type be treated as different values of a same variable (i.e. column Crop in dat1l). The above plot could not be generated had crop types been treated as variables (i.e. columns) as in dat1w.\nYou can also compare the distributions to a normal distribution (i.e. a Q-Q plot):\n\nqqmath( ~ Yield | Crop, dat1l, plot.points = \"\", layout = c(5, 1))\n\n\n\n\nBoxplots are another popular way to view the distribution of values. In the following example, we create boxplots of income values as a function of gender (value ~ Gender) and condition this comparison on educational levels (| Level). Since the variable Gender contains three unique values (M, F and All) we will remove all records tied to the whole population (All) by passing a condition, dat2b$Gender != \"All\" (where != is interpreted as “not equal to”), to the dat2b index.\n\nbwplot(value ~ Gender | Level, dat2b[dat2b$Gender != \"All\",] , layout = c(6,1))\n\n\n\n\nNote that bwplot automatically orders the conditional variable lexicographically unless the variable’s order is defined by its levels (the latter being the case in this example).\nWe may remove the outliers by passing the do.out = FALSE parameter to bwplot:\n\nbwplot(value ~ Gender | Level, dat2b[dat2b$Gender != \"All\",] , layout = c(6,1), do.out = FALSE)\n\n\n\n\nBut note that the y-axis range still reflects the outlier values. We will need to explicitly define the y limits using the ylim= parameter.\n\nbwplot(value ~ Gender | Level, dat2b[dat2b$Gender != \"All\",] , layout = c(6,1), do.out = FALSE, ylim=c(0,125000))"
  },
  {
    "objectID": "lattice_plot.html#visualizing-multivariate-data",
    "href": "lattice_plot.html#visualizing-multivariate-data",
    "title": "14  Lattice plotting environment",
    "section": "14.4 Visualizing multivariate data",
    "text": "14.4 Visualizing multivariate data\n  \n\n14.4.1 Basic line plots\nThe Lattice package allows us to plot several variables on a same plot. For example, we can plot yields vs year for each crop on a same plot:\n\nxyplot(Barley + Buckwheat + Maize + Oats + Rye ~ Year , dat1w, type = \"o\", pch=20,\n        auto.key = list(lines = TRUE, space = \"right\"),\n        main = \"Grain yield\", ylab = \"Yield\", xlab = \"Year\")\n\n\n\n\nNote the use of the wide version (dat1w) of the original dat1 dataset. Each crop is treated as its own variable (column) and is added to the plot using the + symbol.\nWe can also split the plots across a Trellis system by conditioning yields on crop type. But this now requires the use of the long form of the data (i.e. dat1l). The next plot also differs from the previous plot in that we substitute a point-line symbol (type=\"o\") with a line symbol only (type=\"l\").\n\nxyplot(Yield ~ Year | Crop, dat1l, type = \"l\", pch=20, layout=c(5,1),\n       main = \"Grain yield\", ylab = \"Yield\", xlab = \"Year\")\n\n\n\n\n\n\n14.4.2 Scatter plots\nOne key benefit of a Trellis system is its ability to generate bivariate scatter plots conditioned on one or more variables. For example, we may wish to compare incomes between female and male for each county and condition this relationship on educational attainment. This requires that male and female be assigned their own columns (since they are now treated as separate variables) and that educational attainment be assigned as another column whose values are each levels of educational attainment. We will therefore use dat2c.\n\ndat2c$Level &lt;- factor(dat2c$Level, levels = c(\"All\", \"NoHS\", \"HS\", \"AD\", \"BD\", \"Grad\" )) \n\nxyplot( M ~ F | Level, dat2c, type = c(\"p\",\"g\"), pch=20, cex=0.6,\n        col.symbol=rgb(0,0,0,0.1), aspect=\"iso\", abline=c(0,1), layout=c(6,1),\n        xlab=\"Female income ($)\", ylab=\"Male income($)\")\n\n\n\n\nThe xyplot function is passed several parameters. The parameter type = c(\"p\",\"g\") instructs the function to generate both points, \"p\", and a background grid ,\"g\". Parameter type can accept many other options, a few are listed in the following table:\n\n\n\nOption\nAdds…\n\n\n\n\n\"p\"\nPoints\n\n\n\"l\"\nLines\n\n\n\"b\"\nBoth points and lines\n\n\n\"o\"\nBoth points and lines\n\n\n\"r\"\nRegression line\n\n\n\"g\"\nReference grid\n\n\n\"smooth\"\nLOESS fit\n\n\n\"spline\"\nCubic spline fit\n\n\n\nThe parameters pch and cex define the symbol shape and size. The parameter col.symbol defines the point color. The parameter aspect=\"iso\" sets the aspect ratio in such a way that a unit of measure along x matches that of y, this facilitates income comparison between both sexes since the scales match exactly. The parameter abline=c(0,1) generated a 45° line. If median incomes for male and female were identical in each county, the points would line up along the 45° line. Most points are above the line indicating that male income is greater than female income when aggregated at the county level.\nNote that the point distribution seems skewed. We can warp the values in a log() function to help reveal the relationship between male and female income:\n\nxyplot( log(M) ~ log(F) | Level, dat2c, type = c(\"p\",\"g\"), pch=20, cex=0.6,\n        col.symbol=rgb(0,0,0,0.1), aspect=\"iso\", abline=c(0,1), layout=c(6,1),\n        xlab=\"Log of female income ($)\", ylab=\"Log of male income($)\")\n\n\n\n\nWe can take this a step further. We could condition the relationship in income between male and female on both educational attainment and states or regions. We won’t condition the scatterplot on states as this would generate 50 x 6 separate plots, so we will condition the plots on region.\nNote that since the output will consist of 6 (levels) x 4 (regions) we will need to modify the layout parameter to layout=c(6,4). We will also stick with the log transformation of income since it seems to do a good job in helping reveal the relationship between both income values.\n\nxyplot( M ~ F | Level + Region, dat2c, type = c(\"p\",\"g\"), pch=20, cex=0.6,\n        col.symbol=rgb(0,0,0,0.1), aspect=\"iso\", abline=c(0,1), layout=c(6,4),\n        xlab=\"Log of female income ($)\", ylab=\"Log of male income($)\")"
  },
  {
    "objectID": "lattice_plot.html#customizing-trellis-plots",
    "href": "lattice_plot.html#customizing-trellis-plots",
    "title": "14  Lattice plotting environment",
    "section": "14.5 Customizing trellis plots",
    "text": "14.5 Customizing trellis plots\nVarious elements of a trellis plot can be customized by passing the graphic parameters to the trellis.par.set() function. To get the list of graphic parameters and their values call the trellis.par.get() function.\n\ntrellis.par.get()\n\nOutput for three parameters is shown below:\n\n\n $ fontsize         :List of 2\n  ..$ text  : num 12\n  ..$ points: num 8\n  ..$ col  : chr \"gray90\"\n  ..$ lwd  : num 1\n  ..$ col  : Named chr [1:7] \"#0072B2\" \"#E69F00\" \"#009E73\" \"#D55E00\" ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"blue\" \"orange\" \"bluishgreen\" \"vermillion\" ...\n  ..$ fill : chr [1:7] \"#CCFFFF\" \"#FFCCFF\" \"#CCFFCC\" \"#FFE5CC\" ...\n  ..$ font : num [1:7] 1 1 1 1 1 1 1\n  ..$ pch  : num [1:7] 1 1 1 1 1 1 1\n $ superpose.polygon:List of 5\n\n\nEach parameter consists of a list of elements. For example, the graphic parameter fontsize is made up of two modifiable elements: text and points. The following figure shows a few of the parameters and their associated graphical element:\n\nFor example, we will modify the Titanic survival barchart by changing the strip background colors, the bar polygon colors, the text size and color and the label size and color as follows:\n\n# Define an array of colors. The first array generates unique hues while the \n# second generates different shades of grey\ncol.qual &lt;- c(\"#FBB4AE\",\"#B3CDE3\",\"#CCEBC5\",\"#DECBE4\",\"#FED9A6\",\"#FFFFCC\",\"#E5D8BD\")\ncol.grey &lt;- c(\"#DFDFDF\",\"#BFBFBF\",\"#9F9F9F\",\"#808080\",\"#606060\",\"#404040\",\"#202020\")\n\n# Modify the Trellis parameters\ntrellis.par.set(superpose.polygon = list(col = col.qual, border = \"black\"))\ntrellis.par.set(strip.background = list(col = col.grey))\ntrellis.par.set(add.text = list(cex = 0.8, col=\"grey20\"))\ntrellis.par.set(par.xlab.text = list(cex = 0.8, col=\"grey20\"))\n\n# Now generate the strip charts\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), \n         groups = Survived, stack = TRUE, layout = c(4, 1),\n         auto.key = list(title = \"Survived\", columns = 2))\n\n\n\n\nYou’ll note that some of the elements are composed of more than one value such as strip.background’s col element which takes on 7 distinct color values. We did not need to modify all seven since only the first two were used (one for age group and the other for gender). Had we conditioned the plot on a third parameter, a third strip would have been added and the third color in col.grey would have been used. For example:\n\nbarchart(Class ~ Freq | Sex + Age + Survived, data = as.data.frame(Titanic), \n         groups = Survived, stack = TRUE, layout = c(4, 2),\n         auto.key = list(title = \"Survived\", columns = 2))\n\n\n\n\nNote the third strip (Yes vs No) with the third background color (#9F9F9F) in the color array col.grey."
  },
  {
    "objectID": "ggplot2.html#sample-data",
    "href": "ggplot2.html#sample-data",
    "title": "15  ggplot2 plotting environment",
    "section": "15.1 Sample data",
    "text": "15.1 Sample data\nThe data files used in this tutorial can be downloaded from the course’s website as follows:\n\nload(url(\"https://github.com/mgimond/ES218/blob/gh-pages/Data/dat1_2.RData?raw=true\"))\n\nThis should load several data frame objects into your R session (note that not all are used in this exercise). The dat1l dataframe is a long table version of the crop yield dataset.\n\nhead(dat1l, 3)\n\n  Year   Crop    Yield\n1 1961 Barley 16488.52\n2 1962 Barley 18839.00\n3 1963 Barley 18808.27\n\n\ndat1l2 adds Country to the dat1l dataframe.\n\nhead(dat1l2, 3)\n\n  Year   Crop Country    Yield\n1 2012 Barley  Canada 38894.66\n2 2012  Maize  Canada 83611.49\n3 2012   Oats  Canada 24954.79\n\n\nThe dat1w dataframe is a wide table version of dat1l.\n\nhead(dat1w, 3)\n\n  Year   Barley Buckwheat    Maize     Oats      Rye\n1 1961 16488.52  10886.67 39183.63 15171.26 11121.79\n2 1962 18839.00  11737.50 40620.80 16224.60 12892.77\n3 1963 18808.27  11995.00 42595.55 16253.04 11524.11\n\n\nThe dat2 dataframe is a wide table representation of income by county and by various income and educational attainment levels. The first few lines and columns are shown next:\n\ndat2[1:3, 1:7]\n\n   County State B20004001 B20004002 B20004003 B20004004 B20004005\n1 Autauga    al     35881     17407     30169     35327     54917\n2 Baldwin    al     31439     16970     25414     31312     44940\n3 Barbour    al     25201     15643     20946     24201     42629\n\n\ndat2c is a long version of dat2\n\nhead(dat2c, 3)\n\n  State                 County Level Region   All     F     M\n1    ak Aleutians East Borough   All   West 21953 20164 22940\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885\n3    ak Aleutians East Borough    HS   West 20770 19671 21192"
  },
  {
    "objectID": "ggplot2.html#the-ggplot2-package",
    "href": "ggplot2.html#the-ggplot2-package",
    "title": "15  ggplot2 plotting environment",
    "section": "15.2 The ggplot2 package",
    "text": "15.2 The ggplot2 package\nThe ggplot2 package is designed around the idea that statistical graphics can be decomposed into a formal system of grammatical rules. The ggplot2 learning curve is the steepest of all graphing environments encountered thus far, but once mastered it affords the greatest control over graphical design. For an up-to-date list of ggplot2 functions, you may want to refer to ggplot2’s website.\nA plot in ggplot2 consists of different layering components, with the three primary components being:\n\nThe dataset that houses the data to be plotted;\nThe aesthetics which describe how data are to be mapped to the geometric elements (color, shape, size, etc..);\nThe geometric elements to use in the plot (i.e. points, lines, rectangles, etc…).\n\nAdditional (optional) layering components include:\n\nStatistical elements such as smoothing, binning or transforming the variable\nFacets for conditional or trellis plots\nCoordinate systems for defining the plots shape (i.e. cartesian, polar, spatial map projections, etc…)\n\nTo access ggplot2 functions, you will need to load its package:\n\nlibrary(ggplot2)\n\nFrom a grammatical perspective, a scientific graph is the conversion of data to aesthetic attributes and geometric objects. This is an important concept to grasp since it underlies the construction of all graphics in ggplot2.\nFor example, if we want to generate a point plot of crop yield as a function of year using the dat1l data frame, we type:\n\nggplot(dat1l , aes(x = Year, y = Yield)) + geom_point()\n\n\n\n\nwhere the function, ggplot(), is passed the data frame name whose contents will be plotted; the aes() function is given data-to-geometry mapping instructions (Year is mapped to the x-axis and Yield is mapped to the y-axis); and geom_line() is the geometry type.\n\nIf we wanted to include a third variable such as crop type (Crop) to the map, we would need to map its aesthetics: here we’ll map Crop to the color aesthetic..\n\nggplot(dat1l , aes(x = Year, y = Yield, color = Crop)) + geom_point()\n\n\n\n\nThe parameter color acts as a grouping parameter whereby the groups are assigned unique colors.\n\nIf we want to plot lines instead of points, simply substitute the geometry type with the geom_line() geometry.\n\nggplot(dat1l , aes(x = Year, y = Yield, color = Crop)) + geom_line()\n\n\n\n\nNote that the aesthetics are still mapped in the same way with Year mapped to the x coordinate, Yield mapped to the y coordinate and Crop mapped to the geom’s color.\nAlso, note that the parameters x= and y= can be omitted from the syntax reducing the line of code to:\n\nggplot(dat1l , aes(Year, Yield, color = Crop)) + geom_line()"
  },
  {
    "objectID": "ggplot2.html#geometries",
    "href": "ggplot2.html#geometries",
    "title": "15  ggplot2 plotting environment",
    "section": "15.3 Geometries",
    "text": "15.3 Geometries\nExamples of a few available geometric elements follow.\n\n15.3.1 geom_line\ngeom_line generates line geometries. We’ll use data from dat1w to generate a simple plot of oat yield as a function of year.\n\nggplot(dat1w, aes(x = Year, y = Oats)) + geom_line() \n\n\n\n\nParameters such as color and linetype can be passed directly to the geom_line() function:\n\nggplot(dat1w, aes(x = Year, y = Oats)) + \n  geom_line(linetype = 2, color = \"blue\", linewidth=0.4) \n\n\n\n\nNote the difference in how color= is implemented here. It’s no longer mapping a variable’s levels to a range of colors as when it’s called inside of the aes() function, instead, it’s setting the line color to blue.\n\n\n15.3.2 geom_point\nThis generates point geometries. This is often used in generating scatterplots. For example, to plot male income (variable B20004013) vs female income (variable B20004007), type:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) \n\n\n\n\nWe modify the point’s transparency by passing the alpha=0.3 parameter to the geom_point function. Other parameters that can be passed to point geoms include color, pch (point symbol type) and cex (point size as a fraction).\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(color = \"red\", pch=3 , alpha = 0.3, cex=0.6) \n\n\n\n\n\n\n15.3.3 geom_hex\nWhen a bivariate scatter plot has too many overlapping points, it may be helpful to bin the observations into regular hexagons. This provides the number of observations per bin.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_hex(binwidth = c(1000, 1000))  \n\n\n\n\nThe binwidth argument defines the width and height of each bin in the variables’ axes units.\n\n\n15.3.4 geom_boxplot\nIn the following example, a boxplot of Yield is generated for each crop type.\n\nggplot(dat1l, aes(x = Crop, y = Yield)) + geom_boxplot(fill = \"bisque\") \n\n\n\n\nIf we want to generate a single boxplot (for example for all yields irrespective of crop type) we need to pass a dummy variable to x=:\n\nggplot(dat1l, aes(x = \"\", y = Yield)) + \n  geom_boxplot(fill = \"bisque\") + xlab(\"All crops\")\n\n\n\n\n\n\n15.3.5 geom_violin\nA violin plot is a symmetrical version of a density plot which provides greater detail of a sample’s distribution than a boxplot.\n\nggplot(dat1l, aes(x = \"\", y = Yield)) + geom_violin(fill = \"bisque\") \n\n\n\n\n\n\n15.3.6 geom_histogram\nHistograms can only be plotted for single variables (unless faceting is used) as can be noted by the absence of a y= parameter in aes():\n\nggplot(dat1w, aes(x = Oats)) + geom_histogram(fill = \"grey50\") \n\n\n\n\nThe bin widths can be specified in terms of the value’s units. In our example, the unit is yield of oats (in Hg/Ha). So if we want to generate bin widths that cover 1000 Hg/Ha, we can type,\n\nggplot(dat1w, aes(x = Oats)) + \n  geom_histogram(fill = \"grey50\", binwidth = 1000) \n\n\n\n\nIf you want to control the number of bins, use the parameter bins= instead. For example, to set the number of bins to 8, modify the above code chunk as follows:\n\nggplot(dat1w, aes(x = Oats)) + \n  geom_histogram(fill = \"grey50\", bins = 8) \n\n\n\n\n\n\n15.3.7 geom_bar\nBar plots are used to summaries the counts of a categorical value. For example, to plot the number of counties in each state (noting that each record in dat2 is assigned a county):\n\nggplot(dat2, aes(State)) + geom_bar()\n\n\n\n\nTo sort the bars by length we need to rearrange the State factor level order based on the number of counties in each state (which is the number of times a state appears in the data frame). We’ll make use of forcats’s fct_infreq function to reorder the State factor levels based on frequency.\n\nlibrary(forcats)\nggplot(dat2, aes(fct_infreq(State,ordered = TRUE))) + geom_bar()\n\n\n\n\nIf we want to reverse the order (i.e. plot from smallest number of counties to greatest), wrap the fct_infreq function with fct_rev.\n\nggplot(dat2, aes(fct_rev(fct_infreq(State,ordered = TRUE)))) + geom_bar()\n\n\n\n\nThe geom_bar function can also be used with count values (i.e. variable already summarized by count). First, we’ll summaries the number of counties by state using the dplyr package. This will generate a data frame with just 51 records: one for each of the 50 states and the District of Columbia.\n\nlibrary(dplyr)\ndat2.ct &lt;- dat2 %&gt;% group_by(State) %&gt;% \n                summarize(Counties = n())\nhead(dat2.ct)\n\n# A tibble: 6 × 2\n  State Counties\n  &lt;fct&gt;    &lt;int&gt;\n1 ak          28\n2 al          67\n3 ar          74\n4 az          15\n5 ca          57\n6 co          62\n\n\nWhen using summarized data, we must pass the parameter stat=\"identity\" to the geom_bar function. We must also explicitly map the x and y axes geometries. To order the bar heights in ascending or decending order, we can make use of the generic reorder function. This function will be passed two parameters: the variable to be ordered (State), the variable whose values will determine the order (Counties). Note that this differs from the way the reorder function was used in the base plotting chapter where a third argument, median, was passed to the function due to there being more than one value per grouping variable.\n\nggplot(dat2.ct, aes(x = reorder(State, Counties), y = Counties)) + \n       geom_bar(stat = \"identity\")\n\n\n\n\nIf you want to reverse the order, simply add a minus sign, -, to the Counties variable.\n\nggplot(dat2.ct, aes(x = reorder(State, -Counties), y = Counties)) + \n       geom_bar(stat = \"identity\")\n\n\n\n\n\n\n15.3.8 dot plot\nThe dot plot is an alternative way to visualize counts as a function of a categorical variable. Instead of mapping State to the x-axis, we’ll map it to the y-axis.\n\nggplot(dat2.ct , aes(x = Counties, y = State)) + geom_point()\n\n\n\n\nDot plot graphics benefit from sorting–more so then bar plots.\n\nggplot(dat2.ct , aes(x = Counties, y = reorder(State, Counties))) + \n       geom_point()\n\n\n\n\n\n\n15.3.9 Combining geometries\nGeometries can be layered. For example, to overlay a linear regression line to the data we can add the stat_smooth layer:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(alpha = 0.3) + \n  stat_smooth(method = \"lm\")\n\n\n\n\nThe stat_smooth can be used to fit other lines such as a loess:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(alpha = 0.3) +\n  stat_smooth(method = \"loess\")\n\n\n\n\nThe confidence interval can be removed from the smooth geometry by specifying se = FALSE.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + \n  geom_point(alpha = 0.3) +\n  stat_smooth(method = \"loess\", se = FALSE)\n\n\n\n\n\n\n15.3.10 Combining datasets\nYou can plot different datasets on the same ggplot canvas. This approach usually requires that each geom be assigned its own dataset and aesthetics. In the following example, we’ll create two separate dataframes of grain data: one for barley and the other for oats. We’ll then reference these datasets in separate calls to geom_line.\n\ngrain1 &lt;- select(dat1w, Year, Barley)\ngrain2 &lt;- select(dat1w, Year, Oats)\n\nggplot() +\n  geom_line(data = grain1, aes(Year, Barley), color = \"blue\", show.legend = TRUE) +\n  geom_line(data = grain2, aes(Year, Oats), color = \"red\", show.legend = TRUE) \n\n\n\n\nYou’ll note that the legend is not automatically created since the aesthetics are not specified in the ggplot() function. To add a legend, we must first assign a character string to the color argument in aes(), then we reference that color aesthetic in the scale_color_manual function where we assign color to the aesthetic.\n\nggplot() +\n  geom_line(data = grain1, aes(Year, Barley, color = \"Barley\")) +\n  geom_line(data = grain2, aes(Year, Oats, color = \"Oats\")) +\n  scale_color_manual(name = \"Grain\", values = c(\"Barley\" = \"blue\", \"Oats\" = \"red\"))\n\n\n\n\nNote how we removed the color=\"blue\"/\"red\" arguments from each call to geom_line and added the color aesthetic in the aes() functions."
  },
  {
    "objectID": "ggplot2.html#tweaking-a-ggplot2-graph",
    "href": "ggplot2.html#tweaking-a-ggplot2-graph",
    "title": "15  ggplot2 plotting environment",
    "section": "15.4 Tweaking a ggplot2 graph",
    "text": "15.4 Tweaking a ggplot2 graph\n \n\n15.4.1 Plot title\nYou can add a plot title using the ggtitle function.\n\nggplot(dat2, aes(State)) + geom_bar() + ggtitle(\"Number of counties by state\")\n\n\n\n\n\n\n15.4.2 Axes titles\nAxes titles can be explicitly defined using the xlab() and ylab() functions.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n    xlab(\"Female income ($)\") + ylab(\"Male income ($)\")\n\n\n\n\nTo remove axis labels, simply pass NULL to the functions as in xlab(NULL) and ylab(NULL).\n\n\n15.4.3 Axes labels\nYou can customize an axis’ label elements. If you are mapping continuous values along the x and y axes, use the scale_x_continuous() and scale_y_continuous() functions. For example, to specify where to place the tics and the accompanying labels, type:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n      xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n      scale_x_continuous(breaks = c(10000, 30000, 50000),\n                         labels = c(\"$10,000\", \"$30,000\", \"$50,000\"))\n\n\n\n\nIf you want to change the label formats whereby the numbers are truncated to a thousandth of their original value, you can make use of unit_format() from the scales package:\n\nggplot(dat2, aes(x=B20004013, y=B20004007)) + geom_point(alpha=0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(labels=scales::unit_format(suffix=\"k\", \n                                                     scale=0.001, \n                                                     sep=\"\")) +\n       scale_y_continuous(labels=scales::unit_format(suffix=\"k\", \n                                                     scale=0.001, \n                                                     sep=\"\"))\n\n\n\n\nThe scales package also has a comma_format() function that will add commas to large numbers:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n        xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n        scale_x_continuous(labels = scales::comma_format()) +\n        scale_y_continuous(labels = scales::comma_format())\n\n\n\n\nYou can rotate axes labels using the theme function.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nThe hjust argument justifies the values horizontally. Its value ranges from 0 to 1 where 0 is completely left-justified and 1 is completely right-justified. Note that the justification is relative to the text’s orientation and not to the axis. So it may be best to first rotate the label values and then to adjust justification based on the plot’s look as needed.\nIf you want the label values rotated 90° you might also need to justify vertically (relative to the text’s orientation) using the vjust argument where 0 is completely top-justified and 1 is completely bottom-justified.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0)) \n\n\n\n\n\n\n15.4.4 Axes limits\nThe axis range can be set using xlim() and ylim().\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       xlim(10000, 75000) + ylim(10000, 75000)\n\n\n\n\nHowever, if you are calling the scale_x_continuous() and scale_y_continuous() functions, you do not want to use xlim and ylim instead, you should add the limit= argument to the aforementioned functions. For example,\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format()) +\n       scale_y_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format())\n\n\n\n\n\n\n15.4.5 Axes breaks\nYou can explicitly define the breaks with the breaks argument. Continuing with the last example, we get:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format(),\n                          breaks = c(10000, 30000, 50000, 70000)) +\n       scale_y_continuous(limit  = c(10000, 75000),\n                          labels = scales::comma_format(),\n                          breaks = c(10000, 30000, 50000, 70000))\n\n\n\n\nNote that the breaks argument can be used in conjunction with other arguments (as shown in this example), or by itself.\n\n\n15.4.6 Axes and data transformations\nIf you wish to apply a non-linear transformation to either axes (while preserving the untransformed axis values) add the coord_trans() function as follows:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_trans(x = \"log\")\n\n\n\n\nYou can also transform the y-axis by specifying the parameter y=. The log transformation defaults to the natural log. For a log base 10, use \"log10\" instead. For a square root transformation, use \"sqrt\". For the inverse use \"reciprocal\".\nAdvanced transformations can be called via the scales package. For example, to implement the box-cox transformation (with a power of -0.3), type:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_trans(x = scales::boxcox_trans(-0.3))\n\n\n\n\nNote that any statistical geom (such as the regression line) will be applied to the un-transformed data. So a linear model may end up looking non-linear after an axis transformation:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       stat_smooth(method = \"lm\", se = FALSE) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_trans(x = \"log\")\n\n\n\n\nIf a linear fit is to be applied to the transformed data, a better alternative is to transform the values instead of the axes. The transformation can be done on the original data or it can be implemented in ggplot using the scale_x_continuous and scale_y_continuous functions.\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       stat_smooth(method = \"lm\", se = FALSE) +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       scale_x_continuous(trans = \"log\", breaks = seq(10000,60000,10000))\n\n\n\n\nThe scale_x_continuous and scale_y_continuous functions will accept scales transformation parameters–e.g. scale_x_continuous(trans = scales::boxcox_trans(-0.3)). Note that the parameter breaks is not required but is used here to highlight the transformed nature of the axis.\n\n\n15.4.7 Aspect ratio\nYou can impose an aspect ratio to your plot using the coord_equal() function. For example, to set the axes units equal (in length) to one another set ratio=1:\n\nggplot(dat2, aes(x = B20004013, y = B20004007)) + geom_point(alpha = 0.3) +\n       stat_smooth(method = \"lm\") +\n       xlab(\"Female income ($)\") + ylab(\"Male income ($)\") +\n       coord_equal(ratio = 1)\n\n\n\n\n\n\n15.4.8 Colors\nYou can customize geom colors using one of two sets of color schemes: one for continuous values, the other for categorical (discrete) values.\n\n\n\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nscale_color_gradient scale_color_gradient2 scale_color_distiller scale_fill_gradient2 scale_fill_gradient scale_fill_distiller\nscale_color_hue scale_color_grey scale_color_manual scale_color_brewer\n\n\n\nA few examples follow.\n\n15.4.8.1 Continuous color schemes\nThe following chunk of code summarizes dat2 by tallying the number of counties in each state and by computing the median county income values.\n\ndat2.ct2 &lt;- dat2 %&gt;% group_by(State) %&gt;% \n  summarize(Counties = n(), Income = median(B20004001))\nhead(dat2.ct2)\n\n# A tibble: 6 × 3\n  State Counties Income\n  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 ak          28 33980.\n2 al          67 28946 \n3 ar          74 26320.\n4 az          15 28799 \n5 ca          57 33438 \n6 co          62 30892.\n\n\nThe following chunk applies a green to red color gradient fill to each bar based on the median county incomes. Note that we are using the summarized count table (and not the original dat2 table). Recall that when plotting bars from counts that are already tabulated we must specify stat=\"identity\" in the geom_bar function.\n\nggplot(dat2.ct2, aes(x = fct_reorder(State, Counties), y = Counties, fill = Income)) + \n       geom_bar(stat = \"identity\") + \n       scale_fill_gradient(low = \"green\", high = \"red\")\n\n\n\n\nThe following chunk applies a divergent color scheme while allowing one to specify the central value of this scheme. Note that the colors are symmetrical about the midpoint which may result in only a partial range of the full possible gradient of colors.\n\nggplot(dat2.ct2, aes(x = fct_reorder(State, Counties), y = Counties, fill = Income)) + \n       geom_bar(stat = \"identity\") + \n       scale_fill_gradient2(low = \"darkred\", mid = \"white\",  high = \"darkgreen\", \n                            midpoint = 30892)\n\n\n\n\nIn the last two code chunks, we filled the bars with colors (note the use of functions with the string _fill_). When assigning color to point or line symbols, use the function with the _color_ string. For example:\n\nggplot(dat2.ct2, aes(y = fct_reorder(State, Counties), x = Counties, col = Income)) +\n       geom_point() +\n       scale_color_gradient2(low = \"darkred\", mid = \"white\",  high = \"darkgreen\", \n                              midpoint = 30892)\n\n\n\n\n\n\n15.4.8.2 Discrete color schemes\nIn the following chunk, we assign colors manually to each level in the variable Yield. The order of the color names mirror the order of the variable levels.\n\nggplot(dat1l, aes(Year, Yield, col = Crop)) + \n       geom_line() +\n       scale_color_manual(values = c(\"red\", \"orange\", \"green\", \"blue\", \"yellow\"))\n\n\n\n\nThe following chunk applies a predefined discrete color scheme using one of Brewer’s preset qualitative colors, Dark2, to each level.\n\nggplot(dat1l, aes(Year, Yield, col = Crop)) + \n              geom_line() +\n              scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nYou can also apply sequential or divergent Brewer color schemes to variables having an implied order.\nLet’s assume that there is an implied order to the crop types. For example, we’ll reorder the crop types based on their median yield (this creates an ordered factor from the Crop variable). We can then use one of Brewer’s sequential color schemes such as Reds.\n\nggplot(dat1l, aes(Year, Yield, col = reorder(Crop, Yield, median))) + \n              geom_line() +\n              guides(color = guide_legend(title = \"Crops\")) +  \n              scale_color_brewer(palette = \"Reds\") \n\n\n\n\nNote that we’ve added a guides() function to rename the legend title. This is not needed to generate the sequential colors.\nTo reverse the color scheme, set the direction argument to -1.\n\nggplot(dat1l, aes(Year, Yield, col = reorder(Crop, Yield, median))) + \n              geom_line() +\n              guides(color = guide_legend(title = \"Crops\")) +  \n              scale_color_brewer(palette = \"Reds\", direction = -1) \n\n\n\n\nYou can view a list of predefined Brewer color schemes by typing the following:\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\n15.4.9 Adding mathematical symbols to a plot\nYou can embed math symbols using plotmath’s mathematical expressions by wrapping these expressions in an expression() function. For example,\n\nggplot(dat2, aes(x = B20004013^0.333, y = sqrt(B20004007))) + geom_point(alpha = 0.3) +\n       xlab( expression( (\"Female income\") ^ frac(1,3) ) ) + \n       ylab( expression( sqrt(\"Male income\") ) )\n\n\n\n\nTo view the full list of mathematical expressions, type ?plotmath at a command prompt."
  },
  {
    "objectID": "ggplot2.html#faceting",
    "href": "ggplot2.html#faceting",
    "title": "15  ggplot2 plotting environment",
    "section": "15.5 Faceting",
    "text": "15.5 Faceting\n  \n\n15.5.1 Faceting by categorical variable\n  \n\n\n15.5.2 facet_wrap\nFaceting (or conditioning on a variable) can be implemented in ggplot2 using the facet_wrap() function.\n\nggplot(dat1l2, aes(x = Year, y = Yield, color = Crop)) + geom_line() + \n       facet_wrap( ~ Country, nrow = 1)\n\n\n\n\nThe parameter ~ Country tells ggplot to condition the plots on country. If we wanted the plots to be stacked, we would set nrow to 2.\nWe can also condition the plots on two variables such as crop and Country. (Note that we will also rotate the x-axis labels to prevent overlaps).\n\nggplot(dat1l2, aes(x = Year, y=Yield)) + geom_line() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    facet_wrap(Crop ~ Country, nrow = 1)\n\n\n\n\n\n15.5.2.1 Wrapping facet headers\nIf the header names get truncated in the plot header, you can opt to wrap the facet headers using the label_wrap_gen() function as an argument value to labeler. For example, to wrap the United States of America value, we’ll specify the maximum number of characters per line using the width argument:\n\nggplot(dat1l2, aes(x = Year, y=Yield)) + geom_line() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n  facet_wrap(Crop ~ Country, nrow = 1, labeller = label_wrap_gen(width = 12))\n\n\n\n\n\n\n15.5.2.2 facet_grid\nThe above facet_wrap example generated unique combinations of the variables Crop and Country. But such plots are usually best represented in a grid structure where one variable is spread along one axis and the other variable is spread along another axis of the plot layout. This can be easily accomplished using the facet_grid function:\n\nggplot(dat1l2, aes(x = Year, y = Yield)) + geom_line() + \n       facet_grid( Crop ~ Country)\n\n\n\n\n\n\n\n15.5.3 Faceting by continuous variable\nIn the above examples, we are faceting the plots based on a categorical variable: Country and/or crop. But what if we want to facet the plots based on a continuous variable? For example, we might be interested in comparing male and female incomes across different female income ranges. This requires that a new categorical field (a factor) be created assigning to each case (row) an income group. We can use the cut() function to accomplish this task (we’ll also omit all values greater than 100,000):\n\ndat2c$incrng &lt;- cut(dat2c$F , breaks = c(0, 25000, 50000, 75000, 100000) )\nhead(dat2c)\n\n  State                 County Level Region   All     F     M          incrng\n1    ak Aleutians East Borough   All   West 21953 20164 22940     (0,2.5e+04]\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885     (0,2.5e+04]\n3    ak Aleutians East Borough    HS   West 20770 19671 21192     (0,2.5e+04]\n4    ak Aleutians East Borough    AD   West 26383 26750 26352 (2.5e+04,5e+04]\n5    ak Aleutians East Borough    BD   West 22431 19592 27875     (0,2.5e+04]\n6    ak Aleutians East Borough  Grad   West 74000 74000 71250 (5e+04,7.5e+04]\n\n\nIn the above code chunk, we create a new variable, incrng, which is assigned an income category group depending on which range dat2c$F (female income) falls into. The income interval breaks are defined in breaks=. In the output, you will note that the factor incrng defines a range of incomes (e.g. (0 , 2.5e+04]) where the parenthesis ( indicates that the left-most value is exclusive and the bracket ] indicates that the right-most value is inclusive.\nHowever, because we did not create categories that covered all income values in dat2c$F we ended up with a few NA’s in the incrng column:\n\nsummary(dat2c$incrng)\n\n    (0,2.5e+04] (2.5e+04,5e+04] (5e+04,7.5e+04] (7.5e+04,1e+05]            NA's \n           9419            7520            1425              33               5 \n\n\nWe will remove all rows associated with missing incrng values:\n\ndat2c &lt;- na.omit(dat2c)\nsummary(dat2c$incrng)\n\n    (0,2.5e+04] (2.5e+04,5e+04] (5e+04,7.5e+04] (7.5e+04,1e+05] \n           9419            7520            1425              33 \n\n\nWe can list all unique levels in our newly created factor using the levels() function.\n\nlevels(dat2c$incrng) \n\n[1] \"(0,2.5e+04]\"     \"(2.5e+04,5e+04]\" \"(5e+04,7.5e+04]\" \"(7.5e+04,1e+05]\"\n\n\nThe intervals are not meaningful displayed as is (particularly when scientific notation is adopted). So, we will assign more meaningful names to the factor levels as follows:\n\nlevels(dat2c$incrng) &lt;-  c(\"Under 25k\", \"25k-50k\", \"50k-75k\", \"75k-100k\")\nhead(dat2c)\n\n  State                 County Level Region   All     F     M    incrng\n1    ak Aleutians East Borough   All   West 21953 20164 22940 Under 25k\n2    ak Aleutians East Borough  NoHS   West 21953 19250 22885 Under 25k\n3    ak Aleutians East Borough    HS   West 20770 19671 21192 Under 25k\n4    ak Aleutians East Borough    AD   West 26383 26750 26352   25k-50k\n5    ak Aleutians East Borough    BD   West 22431 19592 27875 Under 25k\n6    ak Aleutians East Borough  Grad   West 74000 74000 71250   50k-75k\n\n\nNote that the order in which the names are passed must match that of the original breaks.\nNow we can facet male vs. female scatter plots by income ranges. We will also throw in a best fit line to the plots.\n\nggplot(dat2c, aes(x = F, y = M)) + geom_point(alpha=0.2, pch=20) +\n       stat_smooth(method = \"lm\", col = \"red\") +\n       facet_grid( . ~ incrng) \n\n\n\n\nOne reason we would want to explore our data across different ranges of value is to assess the consistency in relationship between variables. In our example, this plot helps assess whether the relationship between male and female income is consistent across income groups."
  },
  {
    "objectID": "ggplot2.html#adding-45-slope-using-geom_abline",
    "href": "ggplot2.html#adding-45-slope-using-geom_abline",
    "title": "15  ggplot2 plotting environment",
    "section": "15.6 Adding 45° slope using geom_abline",
    "text": "15.6 Adding 45° slope using geom_abline\nIn this next example, we will add a 45° line using geom_abline where the intercept will be set to 0 and the slope to 1. This will help visualize the discrepancy between the batches of values. So if a point lies above the 45° line, then the male’s income is greater, if the point lies below the line, then the female’s income is greater.\nTo help highlight differences in income, we will make a few changes to the faceted plots. First, we will reduce the y-axis range to $0-$150k (this will remove a few points from the data); we will force the x-axis and y-axis units to match so that a unit of $50k on the x-axis has the same length as that on the y-axis. We will also reduce the number of x tics and assign shorthand notation to income values (such as “50k” instead of “50000”). All this can be accomplished by adding the scale_x_continuous() function to the stack of ggplot elements.\n\nggplot(dat2c,  aes(x = F, y = M)) + geom_point(alpha = 0.2, pch = 20, cex = 0.8) + \n      ylim(0, 150000) +\n      stat_smooth(method = \"lm\", col = \"red\") + \n      facet_grid( . ~ incrng) + \n      coord_equal(ratio = 1) + \n      geom_abline(intercept = 0, slope = 1, col = \"grey50\") +\n      scale_x_continuous(breaks = c(50000, 100000), labels = c(\"50k\", \"100k\"))\n\n\n\n\nNote the change in regression slope for the last facet. Note that the stat_smooth operation is only applied to the data limited to the axis range defined by ylim.\nNow let’s look at the same data but this time conditioned on educational attainment.\n\n# Plot M vs F by educational attainment except for Level == All\nggplot(dat2c, aes(x = F, y = M)) + geom_point(alpha = 0.2, pch = 20, cex = 0.8) +\n       ylim(0, 150000) + \n       stat_smooth(method = \"lm\", col = \"red\") +\n       facet_grid( . ~ Level) +  \n       coord_equal(ratio = 1) + \n       geom_abline(intercept = 0, slope = 1, col = \"grey50\") +\n       scale_x_continuous(breaks = c(50000, 100000), labels  =c(\"50k\", \"100k\"))\n\n\n\n\nWe can also condition the plots on two variables. For example: educational attainment and region.\n\nggplot(dat2c, aes(x = F, y = M)) + geom_point(alpha = 0.2, pch = 20, cex = 0.8) +\n       ylim(0, 150000) + \n       stat_smooth(method = \"lm\", col = \"red\") + \n       facet_grid( Region ~ Level) +  \n       coord_equal(ratio = 1) +\n       geom_abline(intercept = 0, slope = 1, col = \"grey50\") +\n       scale_x_continuous(breaks = c(50000, 100000), labels = c(\"50k\", \"100k\"))"
  },
  {
    "objectID": "ggplot2.html#tile-plots-heat-maps",
    "href": "ggplot2.html#tile-plots-heat-maps",
    "title": "15  ggplot2 plotting environment",
    "section": "15.7 Tile plots (heat maps)",
    "text": "15.7 Tile plots (heat maps)\nYou can create so-called heat maps by tiling the data. This typically requires the use of three variables–two of which are either categorical or have equally spaced continuous values that define a rectangular grid layout, and the third that defines the grid cells’ color. For example, a tile plot can be created showing the median income (for all sexes) as a function of education level and region.\nWe will first summarize our data to generate a three variable dataset.\n\ndat2c.med &lt;- dat2c %&gt;% \n  filter(Level != \"All\") %&gt;% \n  group_by(Level, Region) %&gt;% \n  summarise(Income = median(All))\n\nhead(dat2c.med)\n\n# A tibble: 6 × 3\n# Groups:   Level [2]\n  Level Region        Income\n  &lt;fct&gt; &lt;fct&gt;          &lt;dbl&gt;\n1 NoHS  Northeast     21431 \n2 NoHS  South         18594 \n3 NoHS  North Central 19639 \n4 NoHS  West          18978.\n5 HS    Northeast     28637 \n6 HS    South         25825 \n\n\nNext, we will assign the Region variable to the x-axis, the Level variable to the y-axis and the Income values will be used to define the fill colors.\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_gradient(low = \"yellow\", high = \"red\")\n\n\n\n\nThe above example adopts a continuous color scheme. If you want to bin the color swatches using user defined breaks, swap the scale_fill_gradient function with the scale_fill_binned function.\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_binned(low = \"yellow\", high = \"red\",\n                    breaks = c(21000, 28000, 32000, 42000, 53000))\n\n\n\n\nAs of ggplot2 version 3.3, you can use the guide_colorsteps function to control the look of your legend. In the last figure, the breaks are not even, yet the legend splits the color swatches into equal length units. Setting the even.steps argument to FALSE scales the color swatches to match the true interval lengths.\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_binned(low = \"yellow\", high = \"red\",\n                    breaks = c(21000, 28000, 32000, 42000, 53000),\n                    guide = guide_colorsteps(even.steps = FALSE))\n\n\n\n\nThe scale_fill_binned function offers additional control over the legend such as its height (barheight), width (barwidth) and the display of the minimum and maximum values (show.limits).\n\nggplot(dat2c.med, aes(x = Region, y = Level, fill = Income)) + geom_tile() +\n  scale_fill_binned(low = \"yellow\", high = \"red\",\n                    breaks = c(21000, 28000, 32000, 42000, 53000),\n                    guide = guide_colorsteps(even.steps = FALSE,\n                                              barheight = unit(2.3, \"in\"),\n                                              barwidth = unit(0.1, \"in\"),\n                                              show.limits = TRUE))"
  },
  {
    "objectID": "ggplot2.html#exporting-to-an-image",
    "href": "ggplot2.html#exporting-to-an-image",
    "title": "15  ggplot2 plotting environment",
    "section": "15.8 Exporting to an image",
    "text": "15.8 Exporting to an image\nYou can export a ggplot figure to an image using the ggsave function. For example,\n\np1 &lt;- ggplot(dat1l2, aes(x = Year, y = Yield, color = Crop)) + geom_line() + \n  facet_wrap( ~ Country, nrow = 1) +\n  scale_y_continuous(labels = scales::comma_format())\n\nggsave(\"fig0.png\", plot = p1, width = 6, height = 2, units = \"in\", device = \"png\")\n\n\nThe width and height arguments are defined in units of inches, in. You can also specify these parameters in units of centimeters by setting units = \"cm\". The device argument controls the image file type. Other file types include \"jpeg\", \"tiff\", \"bmp\" and \"svg\" just to name a few.\nFor greater control of the font sizes, you need to make use of the theme function when buiding the plot.\n\np1 &lt;- ggplot(dat1l2, aes(x = Year, y = Yield, color = Crop)) + geom_line() + \n  facet_wrap( ~ Country, nrow = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme(axis.text    = element_text(size = 8, family = \"mono\"),\n        axis.title   = element_text(size = 11, face = \"bold\"),\n        strip.text   = element_text(size = 11, face=\"italic\", family = \"serif\"),\n        legend.title = element_text(size = 10, family = \"sans\"),\n        legend.text  = element_text(size = 8,  color = \"grey40\"))\n\nggsave(\"fig1.png\", plot = p1, width = 6, height = 2, units = \"in\")\n\n\nThe family argument controls the font type. It does not automatically access all the fonts in your operating system. The three R fonts accessible by default are \"serif\", \"sans\" and \"mono\". These are usually mapped to your system’s fonts.\nTo access other fonts on your operating system, you will need to make use of the showtext package. The package is not covered in this tutorial, instead, refer to the package’s website for instructions on using the package."
  },
  {
    "objectID": "univariate_plots.html#introduction",
    "href": "univariate_plots.html#introduction",
    "title": "16  Visualizing univariate distributions",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nUnivariate data consist of a single measurement type that is often broken up into categories.\nLet’s take two batches of values that measure a same variable. We’ll label these batches a and b.\nHow do the two batches differ?\n\n\n\nIf the difference wasn’t obvious from the table view we can create a jittered point plot from the data. (The jitter plot spreads the values randomly about their group label to prevent the points from overlapping.)\n\n\n\n\n\nIt’s clear that both batches differ by their uniform values: batch a is made up of identical values, 12, and batch b is made up of a different set of identical values, 15.\nNow let’s compare a heterogeneous batch of values.\n\n\n\nComparing the values using a jitter plot gives us:\n\n\n\n\n\nSo how do these batches differ? This time, the difference is not so obvious. However, they seem to differ by their central value. For example, each batch’s mean is:\n\n\n# A tibble: 2 × 2\n  Group  mean\n  &lt;chr&gt; &lt;dbl&gt;\n1 a        10\n2 b        17\n\n\nThe center value (aka location), is one summary statistic we can use to compare batches. Another property of a batch that we might also want to compare is its distribution (aka spread). For example, does the spread between the two batches differ as well? It’s difficult to tell from the above plot given that the batches are offset, so we’ll level the batches by subtracting the means from their respective batches.\n\n\n\n\n\nRemoving the location (or mean in our example) from each value facilitates our comparison of both spreads. From our working example we can, at best, say that the batches differ in range (i.e. their minimum and maximum values). However, a spread can be characterized in many more ways than by its range. Next, we’ll focus on four exploratory tools that will help us explore and quantify a dataset’s spread. These are the histogram, the boxplot, the density plot and the quantile plot."
  },
  {
    "objectID": "univariate_plots.html#histograms",
    "href": "univariate_plots.html#histograms",
    "title": "16  Visualizing univariate distributions",
    "section": "16.2 Histograms",
    "text": "16.2 Histograms\nA histogram bins the values (usually in equal sized bins) and plots the frequency in which each bin is filled. For example, to create a histogram of batch b where each bin size covers one unit, we type:\n\n# Generate some random numbers\nset.seed(23)\na   &lt;- round(runif(60, 5, 15))\nb   &lt;- round(runif(100, 10, 20))\ndf  &lt;- data.frame(Value = c(a,b), Group = c(rep(\"a\",6), rep(\"b\",10))) # Long form\n\n# Generate the histogram for batch a\nlibrary(ggplot2)\nggplot(as.data.frame(b), aes(x = b)) + geom_histogram(breaks = seq(9.5,20.5, by = 1), color = \"white\")\n\n\n\n\n(Note that because ggplot requires a dataframe as input, we have to create a dataframe from vector b.)\nHere, we are explicitly defining the bin width as 1 unit, and the range as 6.5 to 16.5 via the parameter breaks = seq(6.5,16.5,by=1). The color parameter specifies the outline color. To change the fill color use the fill parameter instead. In our example, we have one value that falls in the first bin (bin ranging from 6.5 to 7.5), another value that falls in the second bin (bin value ranging from 7.5 to 8.5) and so on up to the second to last bin which has 3 values falling in it (bin covering the range 14.5 to 15.5). No values fall in the 15.5 to 16.5 bin range.\nWe can modify the width of each bin. For example, to have each bin cover two units instead of one, type:\n\nggplot(as.data.frame(b), aes(x = b)) + geom_histogram(breaks = seq(9.5,20.5,by = 2), \n                                                      colour = \"white\") \n\n\n\n\nYou’ll note that changing bin widths can alter the look of the histogram, this is particularly true when plotting large batches of values.\nYou can also opt to have the function determine the bin width by simply specifying the number of bins using the bins = parameter:\n\nggplot(as.data.frame(b), aes(x = b)) + geom_histogram(bins = 12, colour = \"white\")\n\n\n\n\nYou’ll note that the function will map the total number of observations falling in each bin to the y-axis. This may be easy to interpret, but it may make it difficult to compare batches of values having different number of observations. To demonstrate this, let’s plot both batches side-by-side.\n\nggplot(df, aes(x = Value)) + geom_histogram(bins = 10) + facet_wrap(~ Group)\n\n\n\n\nThe differences in batch sizes results in differing bar heights. To resolve this issue, we can convert counts to density whereby the areas covered by each bin sum to 1. Each bin’s density value is computed from:\n\\[\ndensity = \\frac{count}{bin\\ width * total\\ observations}\n\\] To see an example of this, let’s revert back to the earlier dataset to facilitate a manual calculation of the density values. Here, we’ll map the after_stat(density) aesthetic to the y-axis to have ggplot compute density values.\n\nggplot(df, aes(x = Value, y=after_stat(density))) + geom_histogram(bins = 10) + \n  facet_wrap(~ Group)\n\n\n\n\nThis makes for a more appropriate comparison."
  },
  {
    "objectID": "univariate_plots.html#density-plots",
    "href": "univariate_plots.html#density-plots",
    "title": "16  Visualizing univariate distributions",
    "section": "16.3 Density plots",
    "text": "16.3 Density plots\nThe histogram is not only sensitive to bin sizes, but it also suffers from discontinuities in its bins. Let’s revert back to the orignal set of values for a.\n\nset.seed(23)\na  &lt;- round(runif(10, 5, 15))\ndf   &lt;- data.frame(a)\n\nIn the following example, two histograms are generated using the same bin sizes and counts but with different starting x values. The orange marks along the x-axis show the location of the a values. The second histogram suggests a slightly bimodal (two peak) distribution while the one on the left suggests a unimodal distribution.\n\n\n\n\n\nOne workaround to the histogram’s limitations is to compute density values on overlapping bins. For example, let’s take the first bin and have it count the number of values between 5 and 9 (exclusive), then divide that number by the total number of values times the bin width–this gives us two observations falling in the bin thus a density value of 2 / (10 * 4) = 0.05. The following plot shows the bin. An orange dot is also added to represent the bin’s midpoint.\n\n\n\n\n\nNext, we shift the bin over by one unit, then we calculate the density of observations in the same way it was computed for the first bin. The density value is plotted as an orange dot. Note how the bin overlaps partially with the first bin.\n\n\n\n\n\nThe same process is repeated for the third bin.\n\n\n\n\n\nThe process is repeated for each bin until the last bin is reached. (Note that some of the a values are duplicates such as 13 and 15–hence the high density values for the upper range).\n\n\n\n\n\nIf we remove the bins and connect the dots, we end up with a density trace.\n\n\n\n\n\nA property associated with the density trace is that the area under the curve sums to one since each density value represents the local density at x.\nSo far, when we computed the density values, we assigned an equal weight to each point within their assigned bin–regardless how far the values were to the bin’s midpoint. This resulted in some raggedness to the plot. To smooth out the plot, we can apply different weights to each point such that points closest to the bin’s midpoint are assigned greater weight than the ones furthest from the midpoint. A Gaussian function can be used to generate the weights. The following figure depicts the difference in weights assigned to any point falling within the first bin whose range covers the interval 4 to 8 centered on 6.\n\n\n\n\n\nWith the rectangular weight, all points within a bin width are assigned equal weight. With the Gaussian weight, points closest to the bin center are assigned greater weight than those furthest from the center. ggplot’s density function defaults to the Gaussian weighting strategy. The weight type is referred to as a kernel.\nYou can generate a density plot of the data using the geom_density function.\n\nggplot(df, aes(x = a)) + geom_density(fill = \"grey60\")\n\n\n\n\nThis plot appears much smoother than the one created in the above demonstration. This is because the density function generates 512 points for its density trace as opposed to the seven points used in the above demonstration.\nThe function adopts the gaussian weight function and will automatically define the bandwidth (analogous in concept to the bin width).\nTo adopt a rectangular weight, set the kernel parameter to \"rectangular\".\n\nggplot(df, aes(x = a)) + geom_density(fill = \"grey60\", kernel = \"rectangular\")\n\n\n\n\nNote the raggedness in the plot.\nYou can modify the smoothness of the density plot by adjusting its bandwidth argument bw. Here, the bandwidth defines the standard deviation of the Gaussian function.\n\nggplot(df, aes(x = a)) + geom_density(fill = \"grey60\", bw = 1)"
  },
  {
    "objectID": "univariate_plots.html#boxplots",
    "href": "univariate_plots.html#boxplots",
    "title": "16  Visualizing univariate distributions",
    "section": "16.4 Boxplots",
    "text": "16.4 Boxplots\nA boxplot is another popular plot used to explore distributions. In ggplot2 we use the geom_boxplot() function as in,\n\nggplot(df, aes(x = a)) + geom_boxplot() + \n          xlab(NULL) + theme(axis.text.y = element_blank(), \n                             axis.ticks.y = element_blank()) \n\n\n\n\nThe geom_boxplot function can map the values to the x-axis or to the y-axis. Traditionally, it’s mapped to the y-axis. Here, we choose to map it to the x-axis. You’ll note the extra functions xlab(NULL) + theme(axis.text.y=element_blank(), ... ); these are added to suppress labels and values/tics along the y-axis given that their default values do not serve a purpose here.\nThe following figure describes the anatomy of a boxplot.\n\nThe boxplot provides us with many meaningful pieces of information. For example, it gives us a center value: the median. It also tells us where the middle 50% of the values lie (in our example, approximately 50% of the values lie between 9.5 and 14.5). This range is referred to as the interquartile range (or IQR for short). Note that this is only an approximation given that some datasets may not lend themselves well to defining exactly 50% of their central values. For example, our batch only has four data points falling within the interquartile range (instead of five) because of tied values in the upper end of the distribution.\nThe long narrow lines extending beyond the interquartile range are referred to as the adjacent values–you might also see them referred to as whiskers. They represent either 1.5 times the width between the median and the nearest interquartile value or the most extreme value, whichever is closest to the batch center.\nSometimes, you will encounter values that fall outside of the lower and/or upper adjacent values; such values are often referred to as outliers.\n\n16.4.1 Not all boxplots are created equal!\nNot all boxplots are created equal. There are many different ways in which quantiles can be defined. For example, some will compute a quantile as \\(( i - 0.5) / n\\) where \\(i\\) is the nth element of the batch of data and \\(n\\) is the total number of elements in that batch. This is the method implemented by Bill Cleveland and we will refer to this method as Cleveland’s quantile method. This also happens to be the method implemented by the base’s boxplot function; this explains its different boxplot output compared to geom_boxplot in our working example:\n\nboxplot(a, horizontal = TRUE)\n\n\n\n\nThe upper and lower quartiles differ from those of ggplot since the three upper values values, 15, end up falling inside the interquartile range following the aforementioned quantile definition. This eliminates any upper whiskers. In most cases, however, the difference will not matter as long as you adopt the same boxplot procedure when comparing batches. Also, the difference between the plots become insignificant with larger batch size.\n\n\n16.4.2 Implementing different quantile types in geom_boxplot\nIf you wish to implement different quantile methods in ggplot, you will need to create a custom function. For example, if you wish to adopt Cleveland’s quantile method (type = 5 in the quantile function) type the following:\n\n# Function to extract quantiles given an f-value type (type = 5 in this example)\nqtl.bxp &lt;- function(x, type = 5) {\n  qtl &lt;- quantile(x, type = type)\n   df &lt;- data.frame(ymin  = qtl[1], ymax = qtl[5], \n                    upper = qtl[4], lower = qtl[2], middle = qtl[3])\n}\n\n# Plot the boxplot\nggplot(df, aes(x = \"\", y = a)) + \n  stat_summary(fun.data = qtl.bxp, fun.args = list(type = 5),\n               geom = 'boxplot') +\n  xlab(NULL) + theme(axis.text.y = element_blank()) +\n  coord_flip()\n\n\n\n\nNote the use of stat_summary instead of geom_boxplot."
  },
  {
    "objectID": "univariate_plots.html#quantile-plots",
    "href": "univariate_plots.html#quantile-plots",
    "title": "16  Visualizing univariate distributions",
    "section": "16.5 Quantile plots",
    "text": "16.5 Quantile plots\nA quantile plot generates a point plot that joins the quantile to each value in a batch. The boxplot covered in the last section is a special case of the \\(f\\)-quantile function in that it only returns the 1st, 2nd (median) and 3rd quartiles. The \\(f\\)-quantile returns the full range of quantile values. The quantile is directly related to the concept of a percentile: it identifies the fraction of the batch of numbers that is less than a value of interest. The following figure describes the anatomy of a quantile plot.\n\nThe x-axis shows the \\(f\\)-values: the full range of fractions. The y-axis is the \\(f\\)-quantile, \\(q(f)\\), which shows the sorted batch values (from smallest to largest). The points in the plot link the values on the y-axis to the \\(f\\)-values on the x-axis. For example, the \\(f\\)-value of 0.25 (~the 25th percentile) is associated with the \\(q(f)\\) value of 9 meaning that 25% of the values in the dataset have values of 9 or less. Likewise, the \\(f\\)-value of 0.5 (the median) is associated with a \\(q(f)\\) value of 12.5 implying that half of the dataset’s values are 12.5 or less. The boxplot is shown alongside the quantile plot to highlight the analogy.\n\n16.5.1 Computing the \\(f\\)-quantile\nComputing \\(f\\) requires that the batch of numbers be ordered from smallest to largest.\n\na.o &lt;- sort(a)\na.o\n\n [1]  7  8  9 11 12 13 13 15 15 15\n\n\nWith the numbers sorted, we can proceed with the computation of \\(f\\) following Cleveland’s method:\n\\[\nf_i = \\frac{i - 0.5}{n}\n\\]\nwhere \\(i\\) is the nth element of the batch of data and \\(n\\) is the total number of elements in that batch. As noted in the Boxplots section of this chapter, there are many ways one can compute a quantile, however, the differences may not matter much when working with large batches of values.\nFor each value in a, the \\(f\\) value is thus:\n\ni     &lt;- 1 : length(a)          # Create indices\nf.val &lt;- (i - 0.5) / length(a)  # Compute the f-value\na.fi  &lt;- data.frame(a.o, f.val) # Create dataframe of sorted values\n\nNote that in the last line of code, we are appending the ordered representation of a to f.val given that f.val assumes an ordered dataset. The data frame a.fi should look like this:\n\n\n\n\n\n\n\n\na.o\nf.val\n\n\n\n\n7\n0.05\n\n\n8\n0.15\n\n\n9\n0.25\n\n\n11\n0.35\n\n\n12\n0.45\n\n\n13\n0.55\n\n\n13\n0.65\n\n\n15\n0.75\n\n\n15\n0.85\n\n\n15\n0.95\n\n\n\n\nIt may be desirable at times to find a value associated with a quantile that might not necessarily match an exact value in our batch. For example, there is no value in a associated with a quantile of \\(0.5\\); this is because we have an even number of values in our dataset. The solution is to interpolate a value based on a desired quantile. The quantile() function does just that. For example, to find the value associated with a quantile of \\(0.5\\), type:\n\nquantile(a, 0.5)\n\n 50% \n12.5 \n\n\nIf we want to get quantile values for multiple fractions, simply wrap the fractions with the c() function:\n\nquantile(a, c(0.25, 0.5, 0.75))\n\n 25%  50%  75% \n 9.5 12.5 14.5 \n\n\nThe quantile function is designed to accept different quantile methods. To see the list of algorithm options, type ?quantile at a command prompt. By default, R adopts algorithm type = 7. To adopt Cleveland’s algorithm, set type = 5. E.g.:\n\nquantile(a, c(0.25, 0.5, 0.75), type = 5)\n\n 25%  50%  75% \n 9.0 12.5 15.0 \n\n\nNote the difference in the upper quartile value compared to the default type = 7.\n\n\n16.5.2 Creating a quantile plot\nA batch’s quantile is best viewed as a plot where we plot the values as a function of the \\(f\\)-values:\n\nggplot(a.fi, aes(x = f.val, y = a.o)) + geom_point() + xlab(\"f-value\")\n\n\n\n\n\n16.5.2.1 Using ggplot’s qq geom\nIf you did not want to go through the trouble of computing the \\(f\\)-values and the dataframe a.fi, you could simply call the function stat_qq() as in:\n\nggplot(df, aes(sample = a)) + stat_qq(distribution = qunif) + xlab(\"f-value\")\n\n\n\n\nHowever, ggplot’s stat_qq function does not adopt Cleveland’s \\(f\\)-value calculation. Hence, you’ll notice a slight offset in position along the x-axis. For example, the third-to-last point has an \\(f\\)-value of 0.744 instead of an \\(f\\)-value of 0.75 as calculated using Cleveland’s method.\nAlso note the change in mapping parameter: sample = a. We are no longer specifying the axis to map to. Instead, we are passing the sample to the stat_qq function which then chooses to map the sorted a values to the y-axis."
  },
  {
    "objectID": "univariate_plots.html#how-quantile-plots-behave-in-the-face-of-skewed-data",
    "href": "univariate_plots.html#how-quantile-plots-behave-in-the-face-of-skewed-data",
    "title": "16  Visualizing univariate distributions",
    "section": "16.6 How quantile plots behave in the face of skewed data",
    "text": "16.6 How quantile plots behave in the face of skewed data\nIt can be helpful to simulate distributions of difference skewness to see how a quantile plot may behave. In the following figure, the top row shows the different density distribution plots and the bottom row shows the quantile plots for each distribution (note that the x-axis maps the f-values)."
  },
  {
    "objectID": "compare_batches.html#boxplots",
    "href": "compare_batches.html#boxplots",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.1 Boxplots",
    "text": "17.1 Boxplots\nA boxplot gives us additional handles by which to compare batches of values.\n\nggplot(df2) + aes(x = voice.part, y = height) + geom_boxplot()\n\n\n\n\nIn addition to providing us with the median, it also allows us to compare the lower and upper quartiles. Here, the bassists are taller than the tenors by all three measures. But note that the difference in height values is not consistent.\n\ndf2 %&gt;% group_by(voice.part) %&gt;% \n  summarise(upper = quantile(height,0.75), \n            mid = quantile(height,0.5),\n            lower = quantile(height,0.25))\n\n# A tibble: 2 × 4\n  voice.part upper   mid lower\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bass 2        74    72    70\n2 Tenor 1       71    68    66\n\n\nWe see that the singer heights differ by 3 inches when comparing the upper quartiles, and by 4 inches when comparing the medians and lower quartiles.\nWhen comparing the minimum values, (these values can be extracted from the lower ends of the whiskers), we see that the difference is just 2 inches. And, interestingly, the tallest bassist is taller than the tallest tenor! So the offset between singer heights when paired up by their ordered height values is not consistent in size and in direction.\nAs you can see from this example, there is more than one way to compare batches of values and yet, we’ve limited our comparisons to just 5 values. So, how can we compare all values in both datasets? Answer: By comparing their quantile functions."
  },
  {
    "objectID": "compare_batches.html#the-empirical-quantile-quantile-q-q-plot",
    "href": "compare_batches.html#the-empirical-quantile-quantile-q-q-plot",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.2 The empirical quantile-quantile (q-q) plot",
    "text": "17.2 The empirical quantile-quantile (q-q) plot\nAn empirical quantile-quantile plot (or q-q plot for short) combines two separate quantile functions from different batches of values by pairing their quantile values with their common \\(f\\)-value. The word empirical implies that we are comparing observational values. This serves to differentiate it from the theoretical q-q plot that will be covered in the next chapter.\n\nIn the above figure, batches a and b have the same number of observations. As such, they have the same f-values. If the number of values in both batches are equal, then the plotting process is straightforward: sort both batches (from smallest value to largest value), then pair up the sorted values and plot one batch vs. the other batch.\nIf, on the other hand, the two batches differ in size (as is the case with our example where we have 21 tenors and 26 bassists), we won’t be able to match their sorted values.\n\nFor example, in the above graph, you’ll note that the Tenor 1 height value of 68 is associated with an \\(f\\)-value of 0.5, however, there is no singer height value associated with an \\(f\\)-value of 0.5 in the Bass 2 batch.\nTo overcome the problem of batch size mismatch, we limit the number of points in the q-q plot to the number of values associated with the smallest sized batch (21 in our working example). This requires that we find matching singer height values to the set of \\(f\\)-values associated with a batch of 21 values. There are a few ways this can be accomplished in R. The easiest way is to make use of the built-in qqplot function.\nqqplot is a base package that will generate a q-q plot using the base plotting environment. It requires that the two batches be loaded as separate vector objects. It also offers the option to output the q-q plot interpolated values as a list by setting the parameter plot.it = FALSE. We then convert this list object to a dataframe using as.data.frame.\n\nTenor &lt;- filter(df, voice.part == \"Tenor 1\") %&gt;%  pull(height)\nBass  &lt;- filter(df, voice.part == \"Bass 2\")  %&gt;%  pull(height)\n\nqq.out &lt;- qqplot(x=Tenor, y=Bass, plot.it=FALSE)\nqq.out &lt;- as.data.frame(qq.out)\n\nnames(qq.out) &lt;- c(\"Tenor\", \"Bass\")  # Add batch names to output\n\nSince the qqplot function will default the column names to x and y, you’ll note that we had to reassign the voice part names to the dataframe in the same order that they were passed to qqplot.\nWe can now generate the q-q plot using ggplot.\n\n# Set the x and y limits\nxylim &lt;- range( c(qq.out$Tenor, qq.out$Bass) )\n\n# Generate the QQ plot\nggplot(qq.out, aes( x= Tenor, y = Bass)) + \n               geom_point() + \n               geom_abline( intercept=0, slope=1) +\n               coord_fixed(ratio = 1, xlim=xylim, ylim = xylim) \n\n\n\n\nBut note that the qqplot function can also generate a base plot with fewer lines of code.\n\nqqplot(x=Tenor, y=Bass, asp=1)\nabline( c(0,1))"
  },
  {
    "objectID": "compare_batches.html#q-q-plots-vs.-traditional-scatter-plots",
    "href": "compare_batches.html#q-q-plots-vs.-traditional-scatter-plots",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.3 q-q plots vs. traditional scatter plots",
    "text": "17.3 q-q plots vs. traditional scatter plots\nIt’s important to note the difference between a quantile-quantile plot and a traditional scatter plot. With the latter, the pairing of values between variables is explicitly defined (for example average male and female income values may be paired by county). The singer data does not assume any pairing of observations (e.g. the height measurement for a singer in Bass is independent for that of a singer in Tenor 2). This is made more evident with our working example which has unequal number of singers in both groups being compared. The pairing of values in a q-q plot is constructed from the ordering of values in each batch and nothing more."
  },
  {
    "objectID": "compare_batches.html#what-can-we-learn-from-a-q-q-plot",
    "href": "compare_batches.html#what-can-we-learn-from-a-q-q-plot",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.4 What can we learn from a q-q plot?",
    "text": "17.4 What can we learn from a q-q plot?\n\n17.4.1 Additive offset\nA valuable by-product of an empirical q-q plot is the mathematical relationship between the batches of values. If the distributions are identical (i.e. all the points of a q-q plot fall on the 45° line) then we could characterize the relationship as batch1 = batch2.\nIf the points follow a pattern mimicking a line parallel to the 45° line as in the following plot, then we say that there is an additive shift between batch1 and batch2.\n\n\n\n\n\nAn additive shift between batches results in an offset in density distributions with matching shape as shown in the the figure on the right.\nThe shift can usually be eyeballed from the plot. In this example, the shift is around 2 units or batch2 =  batch1 + 2.\n\n\n\n\n\nAdding 2 to batch1 lines the points up along the 45° line. This translates to overlapping the density plots as shown on the right.\n\n\n17.4.2 Multiplicative offset\nWhen the points follow a line at an angle to the 45° line as in the following plot, then we say that there is a multiplicative shift between the batches.\n\n\n\n\n\nA multiplicative shift between the batches results in a change in density shapes with one batch’s density being shorter and wider (batch2 in this example) than that of the other batch (batch1 in this example).\nThe multiplier can be a bit difficult to glean graphically so trial and error may be the best approach whereby we multiply one of the batches by a multiplier. For example, after some experimenting, we arrive at a multiplier of 2 which seems to do a good job in aligning the q-q points along the 45° line. We thus define a relationship of batch2 = batch1 * 2.\n\n\n\n\n\nNotice how applying a multiplier not only re-aligns the shape of both densities, but it also corrects the offset observed between both density distributions in the previous figure."
  },
  {
    "objectID": "compare_batches.html#additive-and-multiplicative-offsets",
    "href": "compare_batches.html#additive-and-multiplicative-offsets",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.5 Additive and multiplicative offsets",
    "text": "17.5 Additive and multiplicative offsets\nSometimes, you might encounter a relationship that is both additive and multiplicative in which case you should first resolve the multiplicative part of the pattern until the points are close to being parallel with the 45° line. Once the multiplicative component is taken care of, you then resolve the additive offset.\n\n\n\n\n\nThe above dataset can be decomposed into its multiplicative component (multiply batch1 by ~ 1.5),\n\n\n\n\n\nOnce you align the points parallel to the 45° line, you tackle its additive component (here, we’ll try and additive offset of 4):\n\n\n\n\n\nThe relationship between both batches can thus be defined by batch2 = batch1 * 1.5 + 4.\n\n17.5.1 Is the relationship between tenor and bass additive or multiplicative?\nTurning back to our voice part dataset, an additive shift is apparent, but a multiplicative shift not as much. To check, we’ll add 2.5 to the Bass value.\n\nggplot(qq.out, aes( x= Tenor + 2.5, y = Bass)) + \n             geom_point() + \n             geom_abline( intercept=0, slope=1) +\n             coord_fixed(ratio = 1, xlim=c(65,80), ylim = c(65,80)) + xlab(\"Tenor + 2.5\")\n\n\n\n\nThe bulk of the data appears to follow an additive shift except for one or two outliers at the upper end of the distribution. These outliers should not mislead us into assuming that a multiplicative offset is warranted here."
  },
  {
    "objectID": "compare_batches.html#the-tukey-mean-difference-plot",
    "href": "compare_batches.html#the-tukey-mean-difference-plot",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.6 The Tukey mean-difference plot",
    "text": "17.6 The Tukey mean-difference plot\nOur eyes are better suited at judging deviations from a horizontal line than from a 45° line. It can therefore be helpful to visually “rotate” the plot 45° so as to render what was a 45° line a horizontal line. To tackle this requires that we subtract the y-value (Bass2) from the x-value (Tenor 2), and compare this difference to the mean of the two values:\n\\[\nY = Bass- Tenor\n\\] \\[\nX = \\frac{Bass + Tenor}{2}\n\\]\nWhile the axes values change, the layout of the points relative to the line does not.\n\n17.6.1 Generating a Tukey m-d plot using ggplot\nWe will continue with the qq.out dataset generated from the qqplot function.\n\nmd.y  &lt;- (qq.out$Bass - qq.out$Tenor) \nmd.x  &lt;- (qq.out$Bass + qq.out$Tenor) * 0.5\ndf.md &lt;- data.frame(md.x, md.y)\n\nggplot(df.md,  aes(x = md.x, y = md.y)) + geom_point() + geom_abline( slope=0 ) +\n               xlab(\"Mean height (in)\") + ylab(\"Difference in height (in)\")\n\n\n\n\nThe units on both axes are in inches. It’s clear from this plot that differences in heights are pretty much consistent across the quantiles with an additive shift of about 2.5 inches, except near the higher quantiles for a few lone points.\nWe can leverage this plot to help fine tune the additive offset by computing the median value from the height difference values.\n\nmd.offset &lt;- median(md.y)\nmd.offset\n\n[1] 2.5\n\n\nThe value of 2.5 is in agreement with what we eyeballed earlier."
  },
  {
    "objectID": "compare_batches.html#the-eda_qq-function",
    "href": "compare_batches.html#the-eda_qq-function",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.7 The eda_qq function",
    "text": "17.7 The eda_qq function\nThe tukeyedar package offers an improved version of the q-q plot function that you will be encouraged to use in this course. Some of its features that you may find helpful are highlighted next.\nThe function can take two separate vector elements as input:\n\nlibrary(tukeyedar)\neda_qq(Tenor, Bass) \n\n\n\n\nIt can also take a dataframe in long form as input. Note that the batch assigned to the y-axis will be the first one listed in the group’s levels. To view the levels order associated with the voice.part variable, type:\n\nlevels(df2$voice.part)\n\n[1] \"Bass 2\"  \"Tenor 1\"\n\n\nHere, Bass 2 will be mapped to the y-axis since it appears first in the levels elements.\n\neda_qq(df2, height, voice.part) \n\n\n\n\nThe plot includes the following features:\n\nShaded boxes representing each batch’s interquartile range (mid 50% of values).\nSolid dashed lines inside the shaded boxes representing each batch’s median values.\nLightly shaded dashed dots representing each batch’s 12.5th and 87.5th quantiles (i.e. they show the ends of the mid 80% of values).\nThe power transformation applied to both batches shown in the upper right-hand corner of the plot. Power transformations will be covered later in this course.\n\nTo add the offset to the x-axis, pass \"x + 2.5\" to the fx argument.\n\neda_qq(Tenor, Bass, fx = \"x + 2.5\")\n\n\n\n\nIf you want to apply an offset to the y-axis, use the fy argument. Recall from basic alegbra that \\(y = ax + b \\Leftrightarrow x = (y - b) / a\\). Don’t forget to reference the y variable instead of the x variable in the formula.\n\neda_qq(Tenor, Bass, fy = \"y - 2.5\")\n\n\n\n\nYou’ll note that the function returns a “suggested” offset in the console. For example, the above code chunks returned the following suggested offset:\n\n\n[1] \"Suggested offsets:y = x * 0.8571 + (12.4286)\"\n\n\nWhen applied to the x-axis variable, we get:\n\neda_qq(Tenor, Bass, fx = \" x * 0.8571 + 12.4286 \")\n\n\n\n\nThis seems to improve little over the simpler x + 2 offset expression. Always seek a parsimonious relationship between batches when possible.\nThis function will also generate the Tukey mean-difference plot by setting md to TRUE.\n\neda_qq(Tenor, Bass, md = TRUE)\n\n\n\n\nNote that the offset arguments fx and fy can also be applied to the mean-difference plot.\n\neda_qq(Tenor, Bass, md = TRUE, fx = \"x + 2.5\")\n\n\n\n\nTo learn more about the eda_qq function, see here."
  },
  {
    "objectID": "compare_batches.html#so-how-to-the-batches-of-singer-heights-compare",
    "href": "compare_batches.html#so-how-to-the-batches-of-singer-heights-compare",
    "title": "17  Comparing distributions: The q-q plot",
    "section": "17.8 So how to the batches of singer heights compare?",
    "text": "17.8 So how to the batches of singer heights compare?\nWe’ve learned that the Tenor 1 and Bass 2 batches differ only by location (i.e. central value) and not by spread (i.e. shape of their distribution). How do the other singer groups compare? This requires making \\(8 * (8-1) /2 = 28\\) comparisons, something that is best done by generating a plot matrix.\n\n# Find smallest batch size\nmin_size &lt;- min(tapply(df$height, df$voice.part, length)) \n\n# Split singers into groups based on their voice parts\nsinger_split &lt;- split(df$height, df$voice.part) # Split singers into groups\n\n# Get the range of height values for all groups\nrng &lt;- range(df$height) \n\n# Compute quantiles for each batch using the smallest range of f-values defined\n# in min_size\nqq_df &lt;- as.data.frame(lapply(singer_split, \n                              function(x) quantile(x, type = 5,\n                                                   p = (1:min_size -0.5)/min_size) ))\n\n# Generate plot matrix\nplotfun = function(x,y, ...){\npoints(x,y,pch=18)\n  abline(c(0,1),col=\"blue\")\n}\n\npairs(qq_df, upper.panel=NULL, panel = plotfun, xlim = rng, ylim=rng)  \n\n\n\n\nWith a few exceptions, the singer height distributions differ mostly by an additive offset given that most points are parallel to the 45° line. There is one notable exception: the Tenor 1 and Tenor 2 q-q plot which seems to show a prominent multiplicative offset. We can explore this pair in greater detail. We’ll make use of our custom q-q plot function:\n\nTenor1 &lt;- filter(df, voice.part == \"Tenor 1\") %&gt;%  pull(height)\nTenor2 &lt;- filter(df, voice.part == \"Tenor 2\") %&gt;%  pull(height)\neda_qq(Tenor2, Tenor1)\n\n\n\n\nIt may help to understand how these batches differ by comparing their density distributions. Here, we’ll make use of another custom function, eda_dens.\n\neda_dens(Tenor2, Tenor1)\n\n\n\n\nThe differences in shape between both distributions explains the multiplicative offset observed in the q-q plot. The distribution of height values is wider for the Tenor 1 group than it is for the Tenor 2 group. This suggests greater variability in height values for Tenor 1."
  },
  {
    "objectID": "theoretical_qq.html#introduction",
    "href": "theoretical_qq.html#introduction",
    "title": "18  The theoretical q-q plot",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\nThus far, we have used the quantile-quantile plots to compare the distributions between two empirical (i.e. observational) datasets–hence the name empirical q-q plot. We can also use the q-q plot to compare an empirical distribution to a theoretical distribution (i.e. one defined mathematically). Such a plot is usually referred to as a theoretical Q-Q plot. Examples of popular theoretical distribution are the normal distribution (aka the Gaussian distribution), the chi-square distribution, and the exponential distribution just to name a few.\n\n\n\n\n\nThere are many reasons we might want to compare empirical data to theoretical distributions:\n\nA theoretical distribution is easy to parameterize. For example, if the shape of the distribution of a batch of numbers can be approximated by a normal distribution we can reduce the complexity of our data to just two values: the mean and the standard deviation.\nIf data can be approximated by certain theoretical distributions, then many mainstream statistical procedures can be applied to the data.\nIn inferential statistics, knowing that a sample was derived from a population whose distribution follows a theoretical distribution allows us to derive certain properties of the population from the sample. For example, if we know that a sample comes from a normally distributed population, we can define confidence intervals for the sample mean using a t-distribution.\nModeling the distribution of the observed data can provide insight into the underlying process that generated the data.\n\nBut very few empirical datasets follow any theoretical distributions exactly. So the questions usually ends up being “how well does theoretical distribution X fit my data?”\nThe theoretical quantile-quantile plot is a tool to explore how a batch of numbers deviates from a theoretical distribution and to visually assess whether the difference is significant for the purpose of the analysis. In the following examples, we will compare empirical data to the normal distribution using the normal quantile-quantile plot or normal q-q plot for short."
  },
  {
    "objectID": "theoretical_qq.html#the-normal-q-q-plot",
    "href": "theoretical_qq.html#the-normal-q-q-plot",
    "title": "18  The theoretical q-q plot",
    "section": "18.2 The normal q-q plot",
    "text": "18.2 The normal q-q plot\nThe normal q-q plot is just a special case of the empirical q-q plot we’ve covered in the previous chapter, the difference being that we assign the empirical quantiles to the y-axis and the matching normal quantiles to the x-axis.\n\n\n\n\n\nWhen comparing a batch of numbers to a theoretical distribution on a q-q plot, we are looking for significant deviation from a straight line. To make it easier to judge straightness, we can fit a line to the points. Note that we are not creating a 45° (x=y) slope as was done with the empirical q-q plot–the range of values between both sets of numbers do not match. Here, we are only seeking the straightness of the point pattern.\nWe’ll first learn how to generate this plot using the built-in R function, then we’ll do the same with ggplot2.\n\n18.2.1 Using R’s built-in functions\nIn the following example, we’ll compare the Alto 1 group to a normal distribution.\n\nlibrary(dplyr)\n\ndf &lt;- lattice::singer\n\nalto  &lt;- df %&gt;% \n  filter(voice.part == \"Alto 1\") %&gt;% \n  pull(height)\n\nNote that alto is a single vector element. We’ll use two built-in functions to generate a normal q-q plot: qqnorm and qqline.\n\nqqnorm(alto)\nqqline(alto, qtype = 5)\n\n\n\n\nThere are many ways one can fit a line to the data, Cleveland opts to fit a line to the first and third quartile (IQR) of the q-q plot (this is the method we adopted by setting qtype = 5 in the call to qqline).\n\n\n18.2.2 Using the ggplot2 plotting environment\nTo generate the theoretical q-q plot, we first use the stat_qq function to generate the point plot, then we call the stat_qq_line function to generate the IQR fit.\n\nlibrary(ggplot2)\n\nggplot() + aes(sample = alto) + stat_qq(distribution = qnorm) +\n  stat_qq_line(col = \"blue\") +\n  xlab(\"Unit normal quantile\") + ylab(\"Height\")\n\n\n\n\nNote the slight difference in syntax used with ggplot when passing a vector instead of a dataframe to the function. Here, we take the aes() function outside of the ggplot() function. This is done to render a cleaner syntax. The alternative, ggplot(,aes(sample = alto)), would make it difficult to notice the comma thus increasing the chance for a typo.\nThe stat_qq_line function uses the built-in quantile function and as such will adopt the default quantile type 7 (i.e. it computes the f-value as \\((i - 1)/(n - 1))\\). This differs from Cleveland’s approach to computing the f-value. This explain the slight difference in the fitted line between the ggplot method and the base method where we set qtype to 5. This setting cannot be changed in stat_qq_line.\nNote that geom_qq and geom_qq_line functions are identical to stat_qqand stat_qq_line."
  },
  {
    "objectID": "theoretical_qq.html#how-normal-is-my-dataset",
    "href": "theoretical_qq.html#how-normal-is-my-dataset",
    "title": "18  The theoretical q-q plot",
    "section": "18.3 How normal is my dataset?",
    "text": "18.3 How normal is my dataset?\nThe alto batch of values seem to do a good job in following a normal distribution given how well they follow a straight line. The stair-step pattern in the points is simply a byproduct of the rounding of height values to the nearest inch. A few observations at the tail ends of the distribution deviate from normal, but this is to be expected given that tail ends of distributions tend to be noisy.\nSo how do the other singer groups compare to a normal distribution? We’ll make use of ggplot’s faceting function to generate all eight normal q-q plots.\n\nggplot(df, aes(sample=height)) + stat_qq(distribution=qnorm) +\n  stat_qq_line( col = \"blue\") +\n  xlab(\"Unit normal quantile\") + ylab(\"Height\") +\n  facet_wrap(~voice.part, nrow = 1)\n\n\n\n\nFor the most part, all eight batches appear to follow a normal distribution."
  },
  {
    "objectID": "theoretical_qq.html#what-would-a-dataset-pulled-from-a-normal-distribution-look-like",
    "href": "theoretical_qq.html#what-would-a-dataset-pulled-from-a-normal-distribution-look-like",
    "title": "18  The theoretical q-q plot",
    "section": "18.4 What would a dataset pulled from a normal distribution look like?",
    "text": "18.4 What would a dataset pulled from a normal distribution look like?\nSimulations are a great way to develop an intuitive feel for what a dataset pulled from a normal distribution might look like in a normal q-q plot. You will seldom come across perfectly normal data in the real world. Noise is an inherent part of any underlying process. As such, random noise can influence the shape of a q-q plot despite the data coming from a normal distribution. This is especially true with small datasets as demonstrated in the following example where we simulate five small batches of values pulled from a normal distribution. The rnorm function is used in this example to randomly pick a number from a normal distribution whose mean is set to the mean of the alto values and whose standard deviation is set to the standard deviation of the alto values. We also round the values to mimic the rounding of height values observed in the singer dataset.\n\nset.seed(321)  # Sets random generator seed for consistent output\n\n# Simulate values from a normal distribution\nsim &lt;- data.frame(sample = paste0(\"Sample\",1:5),\n                  value  = round(rnorm(length(alto)*5, \n                                 mean = mean(alto), sd = sd(alto))))\n\n# Generate q-q plots of the simulated values\nggplot(sim, aes(sample = value)) + stat_qq(distribution = qnorm) +\n  stat_qq_line(line.p = c(0.25, 0.75), col = \"blue\") +\n  xlab(\"Unit normal quantile\") + ylab(\"Simulated normals\") +\n  facet_wrap(~ sample, nrow = 1) \n\n\n\n\nOf the five simulated batches, Sample3 generates a textbook normal q-q plot that one would expect from a normally distributed batch of values. Sample2 could lead one to question whether the data were pulled from a normal distribution, even though we know that they were!\nThe singer height normal q-q plots do not look different from some of these simulated plots. In fact, they probably look more Normal then the simulated set of values! This lends confidence in our earlier verdict that the singer height distributions can be characterized by a normal distribution."
  },
  {
    "objectID": "theoretical_qq.html#how-normal-q-q-plots-behave-in-the-face-of-skewed-data",
    "href": "theoretical_qq.html#how-normal-q-q-plots-behave-in-the-face-of-skewed-data",
    "title": "18  The theoretical q-q plot",
    "section": "18.5 How normal q-q plots behave in the face of skewed data",
    "text": "18.5 How normal q-q plots behave in the face of skewed data\nIt can be helpful to simulate distributions of difference skewness to see how a normal quantile plot may behave. In the following figure, the top row shows different density distribution plots; the bottom row shows the normal q-q plots for each distribution."
  },
  {
    "objectID": "theoretical_qq.html#the-eda_qq-function",
    "href": "theoretical_qq.html#the-eda_qq-function",
    "title": "18  The theoretical q-q plot",
    "section": "18.6 The eda_qq function",
    "text": "18.6 The eda_qq function\nYou were introduced to the eda_qq custom function in the previous chapter. This function can also be used to generate normal q-q plots by setting norm to TRUE. In such a case, the function takes as input a single vector of values.\n\nlibrary(tukeyedar)\neda_qq(alto, norm=TRUE) \n\n\n\n\nThe function defaults to the quantile type 5. To adopt the default quantile type used in ggplot2, set q.type = 7.\nNote that when the eda_qq function is used to generate a normal q-q plot, the light dashed lines highlight the standard deviation for both sets of values. This differs from the mid 80% representation of values adopted by the function when generating an empirical q-q plot."
  },
  {
    "objectID": "rf.html#fitting-the-data",
    "href": "rf.html#fitting-the-data",
    "title": "19  Fits and residuals",
    "section": "19.1 Fitting the data",
    "text": "19.1 Fitting the data\nUnivariate data can be characterized by their location and by their spread. The different groups of singers differ by their central values, we will therefore fit the group means to each group batch and compare the residuals between groups.\nFirst, we’ll load the libraries that will be used in this chapter, then we’ll load the singer data into the df object.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lattice)\n\ndf &lt;- singer\n\nNext, we’ll plot the singer values using jittered points. We’ll also add an orange point to each batch which will represent each group’s mean.\n\nggplot(df, aes(y = height, x = voice.part)) + \n  geom_jitter(width = 0.1, height = 0, alpha = 0.1) +\n  stat_summary(fun = \"mean\", geom = \"point\", cex = 3, pch = 21, col = \"red\", bg = \"orange\") \n\n\n\n\nWe’ve fitted each group with the mean–a mathematical description of the batches. Note that we could have used other measures of location such as the median, but since the data seem to follow a symmetrical distribution, the mean remains an adequate choice."
  },
  {
    "objectID": "rf.html#computing-the-residuals",
    "href": "rf.html#computing-the-residuals",
    "title": "19  Fits and residuals",
    "section": "19.2 Computing the residuals",
    "text": "19.2 Computing the residuals\nNow, we’ll subtract the group means from their respective group values: this will give us the residuals for each batch.\n\n# Add residual values to the data\ndf2 &lt;- df %&gt;% \n  group_by(voice.part) %&gt;%\n  mutate(Height.res = height - mean(height))\n\nNext, we will generate a plot of the (jittered) residuals.\n\n# Now plot the data after fitting height with group mean\nggplot(df2) + aes(y=Height.res, x=voice.part)             + \n  geom_jitter(width = 0.1, height=0, alpha=0.1) +\n  stat_summary(fun = \"mean\", geom = \"point\", cex = 3, pch = 21, col = \"red\", bg=\"orange\") \n\n\n\n\nWe’ve normalized the batches to a common location. Note that the values along the y-axis have changed: all values are now spread around 0. Next, we’ll check that the batches of residuals have similar spread."
  },
  {
    "objectID": "rf.html#comparing-the-residuals",
    "href": "rf.html#comparing-the-residuals",
    "title": "19  Fits and residuals",
    "section": "19.3 Comparing the residuals",
    "text": "19.3 Comparing the residuals\nThe feature that interests us in the residuals is the spread. We’ve learned that a good way to compare spreads is to plot the quantiles of each batch against one another.\n\n19.3.1 Residual q-q plots\nIf we want to compare all batches of residuals, we can create a matrix of pairwise residual q-q plots. We’ll adopt the same code chunk used to generate the pairwise empirical q-q plots in chapter 17.\n\nmin_size     &lt;- min(tapply(df2$Height.res, df$voice.part, length)) \nsinger_split &lt;- split(df2$Height.res, df$voice.part)\nrng          &lt;- range(df2$Height.res) \n \nqq_df &lt;- as.data.frame(lapply(singer_split, \n                              function(x) quantile(x, type = 5,\n                                                   p = (1:min_size -0.5)/min_size) ))\n\nplotfun = function(x,y, ...){\npoints(x,y,pch=18)\n  abline(c(0,1),col=\"blue\")\n}\n\npairs(qq_df, upper.panel=NULL, panel = plotfun, xlim = rng, ylim=rng) \n\n\n\n\nSince we removed the means from each batch of values, each pair of values should no longer display any significant offsets. This facilitates our comparison of the spreads and allows us to focus just on the multiplicative offsets.\nThe residual q-q plots suggest that the spreads are very similar across singer heights given that the points fall almost perfectly along the one-to-one line.\n\n\n19.3.2 Comparing batches to pooled residuals using a q-q plot\nSince the spreads are homogeneous across the batches, we may choose to combine (pool) the residuals and compare the residuals of each batch to the pooled residuals. The advantage with this approach is that we are increasing the size of the reference residual distribution thus reducing noise that results from a relatively small sample size. It also reduces the number of q-q plots to analyze–even gone from 28 plots to just eight!\n\ndf3 &lt;- df2 %&gt;%\n  group_by(voice.part)  %&gt;%\n  arrange(Height.res)  %&gt;% \n  mutate(f.val    = (row_number() - 0.5) / n())  %&gt;%\n  ungroup()  %&gt;%\n  mutate(Pooled.res = quantile(Height.res, probs = f.val))  %&gt;%\n  select(voice.part, Height.res, Pooled.res)\n\nggplot(df3, aes(y = Height.res, x = Pooled.res)) + geom_point(alpha = 0.5) + \n              geom_abline(intercept = 0, slope = 1) +\n              facet_wrap(~ voice.part, nrow = 1) \n\n\n\n\nAll eight batches seem to have similar spreads. This makes it possible to compare batch means using a residual-fit spread plot (covered later in this chapter).\n\n19.3.2.1 What to expect if one or more of the batches have different spreads\nThe residual vs. pooled residual plots can be effective at identifying batches with different spreads. In the following example, we combine four simulated batches generated from an identical distribution (V1, V2, V3 and V4) with two simulated batches generated from a different distribution (V5 and V6). Their boxplots are shown next.\n\n\n\n\n\nNow let’s take a look at the residual vs. pooled residual plots.\n\n\n\n\n\nBatches V5 and V6 clearly stand out as having different distributions from the rest of the batches. But it’s also important to note that V5 and V6 contaminate the pooled residuals. This has the effect of nudging the other four batches away from the one-to-one line. Note what happens when batches V5 and V6 are removed from the pooled residuals.\n\n\n\n\n\nThe tightness of points around the one-to-one line suggests nearly identical distributions between V1, V2, V3 and V4 as would be expected given that they were generated from the same underlying distribution.\nPerforming simulations like this can help understand how a pooled residual q-q plot may behave under different sets of distributions."
  },
  {
    "objectID": "rf.html#residual-fit-spread-plot",
    "href": "rf.html#residual-fit-spread-plot",
    "title": "19  Fits and residuals",
    "section": "19.4 Residual-fit spread plot",
    "text": "19.4 Residual-fit spread plot\nSo far, we’ve learned that the spreads of singer heights are the same across all batches. This makes it feasible to assess whether the differences in mean heights between voice parts are comparable in magnitude to the spread of the pooled residuals.\n\n19.4.1 A simple example\nFirst, let’s compare the following two plots. Both plots show two batches side-by-side. The difference in location is nearly the same in both plots (group a and b have a mean of 10 and 11 respectively), but the difference in spreads are not.\n\n\n\n\n\nPlot 2 does not allow us to say, with confidence, that the two batches differ significantly despite both means being different. Plot 1 on the other hand, shows a significant difference in batch locations. One cannot make inferences about differences in central values without knowing the batches’ distributions.\nFor example, in Plot 1, the spread (or difference) in mean values is relatively large compared to the spread of the residuals for each group (note that the spreads are nearly identical between both batches a and b). The difference in means spans one unit while the spread of each sets of residuals spans about the same amount. So the difference in location is significant and is very likely not due to chance alone. The same cannot be said for Plot 2.\nIf we split each batch in Plot 1 into a location component plot (normalized to the overall mean) and a pooled residual component plot, and then compare those values against a quantile, we get a residual-fit spread plot, or r-f spread plot for short.\n\n\n\n\n\nIt’s clear from this r-f spread plot that the spread of the mean distribution (between batches a and b) is important compared to that of its residuals. This suggests that the groups a and b explain much of the variability in the data.\nFor Plot 2, the difference in mean values is also one unit, but the spread of residuals spans almost 5 units. An r-f spread plot makes this difference quite clear.\n\n\n\n\n\nThe spread between each batch’s fitted mean is small compared to that of the combined residuals suggesting that much of the variability in the data is not explained by the differences between groups a and b for Plot 2.\n\n\n19.4.2 Are the fitted voice part values significantly different?\nTo generate the r-f plot, we first need to normalize the data to the global mean. We then split the normalized singer height data into two parts: the modeled means and the residuals. For example, the smallest value in the Bass 2 group is 66. When normalized to the global mean, that value is -1.29. The normalized value is then split between the group (normalized) mean of 4.1 and its residual of -5.39 (i.e. the difference between its value and the Bass 2 group mean). These two values are then each added to two separate plots: the fitted values plot and the residuals plot. This process is repeated for each observation in the dataset to generate the final r-f spread plot.\n\nTo generate the R-F plot using ggplot2, we must first split the data into its fitted and residual components. We’ll make use of piping operations to complete this task.\n\ndf4 &lt;- singer %&gt;%\n  mutate(norm = height - mean(height)) %&gt;%   # Normalize values to global mean\n  group_by(voice.part) %&gt;% \n  mutate( Residuals  = norm - mean(norm),    # Extract group residuals\n          `Fitted values` = mean(norm))%&gt;%   # Extract group means\n  ungroup() %&gt;% \n  select(Residuals, `Fitted values`) %&gt;% \n  pivot_longer(names_to = \"type\",  values_to = \"value\", cols=everything()) %&gt;% \n  group_by(type) %&gt;% \n  arrange(value) %&gt;% \n  mutate(fval = (row_number() - 0.5) / n()) \n\nNext, we’ll plot the data.\n\nggplot(df4, aes(x = fval, y = value)) + \n  geom_point(alpha = 0.3, cex = 1.5) +\n  facet_wrap(~ type) +\n  xlab(\"f-value\") +\n  ylab(\"Height (inches)\") \n\n\n\n\nAn alternative to the side-by-side r-f plot is one where both fits and residuals are overlapping.\n\nggplot(df4, aes(x = fval, y = value, col = type)) + \n  geom_point(alpha = 0.3, cex = 1.5) +\n  xlab(\"f-value\") +\n  ylab(\"Height (inches)\") \n\n\n\n\n\n\nThe spread of the fitted heights (across each voice part) is not insignificant compared to the spread of the combined residuals. The spread in the fitted values (aka the means) encompasses about 90% of the spread in the residuals (you can eyeball the percentage by matching the upper and lower mean values with the residuals’ f-values). So height differences between singer groups cannot be explained by random chance alone or, put another way, the voice-parts can explain a good part of the variation in the data!"
  },
  {
    "objectID": "rf.html#comparing-pooled-residuals-to-the-normal-distribution",
    "href": "rf.html#comparing-pooled-residuals-to-the-normal-distribution",
    "title": "19  Fits and residuals",
    "section": "19.5 Comparing pooled residuals to the normal distribution",
    "text": "19.5 Comparing pooled residuals to the normal distribution\nOur exploration of the singer height batches have been visual thus far. But there may be times when the analysis may need to culminate in a statistical test. Some of these tests reduce the data to mathematically tractable models such as the mean and the standard deviation.\nWe’ll take advantage of the pooled residuals to give us a larger sample size for comparison with the theoretical normal distribution.\n\n# Find the equation for the line\nggplot(df3, aes(sample = Pooled.res)) + stat_qq(distribution = qnorm) + \n  geom_qq_line(distribution = qnorm)\n\n\n\n\nThis dataset has behaved quite well. Its batches differed only by location, yet its spread remained homogeneous (enough) across the batches to pool them and enable us to confirm, with greater confidence, that the spread follows a normal distribution.\nThis well behaved dataset allows us to model its spread using the sample standard deviation. It’s important to note that had the data not followed a normal distribution, then characterizing its spread using the standard deviation would have been inappropriate. Unfortunately, many ancillary data analysts seldom check the distribution requirements of their data before choosing to characterize its distribution using the standard deviation. In such a situation, you would have to revert to a far less succinct characterization of spread: the quantile.\nYou can compute the standard deviation as:\n\nsd(df2$Height.res)\n\n[1] 2.465049\n\n\nWe can now model singer height by both voice.part means, and the group standard deviation of 2.47."
  },
  {
    "objectID": "sl_plot.html#introduction",
    "href": "sl_plot.html#introduction",
    "title": "20  Spread-level plots",
    "section": "20.1 Introduction",
    "text": "20.1 Introduction\nSome batches of data may show a systematic change in spread vs. location. In other words, the variability in each batch may be dependent on that batches median value. Such dependency is often undesirable (e.g. in an ANOVA for instance) and preferably removed in an analysis. A plot well suited for visualizing this dependency is the spread-level plot, s-l (or spread-location plot as Cleveland calls it)."
  },
  {
    "objectID": "sl_plot.html#constructing-the-s-l-plot",
    "href": "sl_plot.html#constructing-the-s-l-plot",
    "title": "20  Spread-level plots",
    "section": "20.2 Constructing the s-l plot",
    "text": "20.2 Constructing the s-l plot\nThe s-l plot compares a measure of the spread’s residual to the location (usually the median) for each batch of data. The spread is usually distilled down to its residual (what remains after subtracting each batch value by the batch median) then it’s transformed by taking the square root of its absolute value. The following block walks you through the steps needed to create an s-l plot.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsinger &lt;- lattice::singer\nres.sq &lt;-  singer %&gt;% group_by(voice.part) %&gt;% \n                      mutate(Median   = median(height),\n                             Residual = sqrt(abs(height - Median)))\n\nggplot(res.sq, aes(x=Median, y=Residual)) + \n  geom_jitter(alpha=0.4,width=0.2) +\n  stat_summary(fun = median, geom = \"line\", col = \"red\") +\n  ylab(expression(sqrt(abs(\" Residuals \")))) +\n  geom_text(aes(x = Median, y = 3.3, label = voice.part))\n\n\n\n\nThe red line in the plot helps identify the type of relationship between spread and location. If the line increases monotonically upward, there is an increasing spread as a function of increasing location; if the line decreases monotonically downward, there is a decreasing spread as a function of increasing location; and if line is neither increasing nor decreasing monotonically, there is no change in spread as a function of location.\n\nNote that if you are to rescale the y-axis when using the stat_summary() function, you should use the coord_cartesian(ylim = c( .. , .. )) function instead of the ylim() function. The latter will mask the values above its maximum range from the stat_summary() function, the former will not.\n\nThe singer dataset does not seem to exhibit any dependence between a voice part’s spread and its median value.\nNext, we’ll look at an example of a dataset that does exhibit a dependence between spread and fitted values."
  },
  {
    "objectID": "sl_plot.html#rs-built-in-iris-dataset",
    "href": "sl_plot.html#rs-built-in-iris-dataset",
    "title": "20  Spread-level plots",
    "section": "20.3 R’s built in iris dataset",
    "text": "20.3 R’s built in iris dataset\nR has a built-in dataset called iris that provide measurements of sepal and petal dimensions for three different species of the iris family. In this next example, we will plot the spreads of the Petal.Length residuals (after removing their group median values) to their group medians.\n\n# Create two new columns: group median and group residuals\n df1 &lt;- iris %&gt;%\n   group_by(Species)  %&gt;%\n   mutate( Median = median(Petal.Length),\n           Residuals = sqrt(abs( Petal.Length - Median)))  \n\n# Generate the s-l plot \n ggplot(df1, aes(x = Median, y = Residuals)) + \n   geom_jitter(alpha = 0.4, width = 0.05, height = 0) +\n   stat_summary(fun = median, geom = \"line\", col = \"red\") +\n   ylab(expression(sqrt( abs(\" Residuals \")))) +\n   geom_text(aes(x = Median, y = 1.3, label = Species))\n\n\n\n\nA monotonic spread is apparent in this dataset too, i.e. as the median length of the Petal increases, so does the spread."
  },
  {
    "objectID": "sl_plot.html#how-can-we-stabilize-spreads-in-a-dataset",
    "href": "sl_plot.html#how-can-we-stabilize-spreads-in-a-dataset",
    "title": "20  Spread-level plots",
    "section": "20.4 How can we stabilize spreads in a dataset?",
    "text": "20.4 How can we stabilize spreads in a dataset?\nA technique used to help reduce or eliminate monotonic variations in the spreads as a function of fitted values is to re-express the original values. Re-expression, which involves transforming values via a pwoer transformation, will be covered in the next chapter. However, in the next section, we learn of a variation of the s-l plot that can identify a power transformation that can help stabilize spread.\n\n20.4.1 Variations of the S-L plot\nAnother version of the S-L plot (and one that seems to be more mainstream) pits the log of the inter-quartile spread vs the log of the median. This approach only works for positive values (this may require that values be adjusted so that the minimum value be no less than or equal to 0).\nThis approach is appealing in that the slope of the best fit line can be used to come up with a power transformation (a topic covered in next week’s lecture) via power = 1 - slope.\nThis variant of the s-l plot can be computed in R as follows (we will use the food web data as an example).\n\nsl &lt;- iris %&gt;%\n  group_by(Species)  %&gt;%\n  summarise (level  = log(median(Petal.Length)),\n                IQR = IQR(Petal.Length),  # Computes the interquartile range\n             spread = log(IQR))\n\nggplot(sl, aes(x = level, y = spread)) + geom_point() + \n  stat_smooth(method = MASS::rlm, se = FALSE) +\n  xlab(\"Median (log)\") + ylab(\"Spread (log)\") +\n  geom_text(aes(x = level, y = spread, label = Species), cex=2.5)\n\n\n\n\nNote how this plot differs from our earlier s-l plot in that we are only displaying each batch’s median spread value and we are fitting a straight line to the medians instead of connecting them.\nThe slope suggests a monotonic increase in spread vs location. We can extract the slope value from a regression model. Here, we’ll adopt a robust bivariate model (bivariate analysis is covered later in this course).\n\ncoefficients(MASS::rlm(spread ~ level, sl))\n\n(Intercept)       level \n  -2.204242    1.143365 \n\n\nThe slope is the second coefficient in the above output. The computed slope value is 1.14. This suggests a power transformation of 1 - 1.14 (or about -0.14). You will learn in the next chapter that as a power transformation value approaches 0, when can opt to use the log transformation instead. We’ll try this next.\n\n# Create two new columns: group median and group residuals\n df1 &lt;- iris %&gt;%\n   group_by(Species)  %&gt;%\n   mutate( log.length = log(Petal.Length),\n           Median = median(log.length),\n           Residuals = sqrt(abs( log.length - Median)))  \n\n# Generate the s-l plot \n ggplot(df1, aes(x = Median, y = Residuals)) + \n   geom_jitter(alpha = 0.4, width = 0.05, height = 0) +\n   stat_summary(fun = median, geom = \"line\", col = \"red\") +\n   ylab(expression(sqrt( abs(\" Residuals \")))) +\n   geom_text(aes(x = Median, y = 1.3, label = Species))\n\n\n\n\nApplying a log transformation to the petal length values seems to have helped stabilize the spread given that the connected lines are close to flat. However, there is a very slight upward slope–though probably insignificant. Would applying a power transformation of -0.14 have helped? We will revisit this dataset in the next chapter."
  },
  {
    "objectID": "re_express.html#introduction",
    "href": "re_express.html#introduction",
    "title": "21  Re-expressing values",
    "section": "21.1 Introduction",
    "text": "21.1 Introduction\nDatasets do not always follow a nice symmetrical distribution nor do their spreads behave systematically across different levels (e.g. medians). Such distributions do not lend themselves well to visual exploration since they can mask simple patterns. They can also be a problem when testing hypotheses using traditional statistical procedures. A solution to this problem is non-linear re-expression (aka transformation) of the values. In univariate analysis, we often seek to symmetrize the distribution and/or equalize the spread. In multivariate analysis, the objective is to usually linearize the relationship between variables and/or to normalize the residual in a regression model.\nRe-expressing values consist of changing the scale of measurement from what is usually a linear scale to a non-linear scale. One popular form of re-expression is the log (natural or base 10).\n\n\nThe log is not the only transformation that can be applied to a dataset. There is a whole family of power transformations (of which the log is a special case) that can be implemented using either the Tukey transformation or the Box-Cox transformation."
  },
  {
    "objectID": "re_express.html#the-log-transformation",
    "href": "re_express.html#the-log-transformation",
    "title": "21  Re-expressing values",
    "section": "21.2 The log transformation",
    "text": "21.2 The log transformation\nOne of the most popular transformations used in data analysis is the logarithm. The log, \\(y\\), of a value \\(x\\) is the power to which the base must be raised to produce \\(x\\). This requires that the log function be defined by a base, \\(b\\), such as 10, 2 or exp(1) (the latter defining the natural log).\n\\[\ny = log_b(x) \\Leftrightarrow  x=b^y\n\\]\nIn R, the base is defined by passing the parameter base= to the log() function as in log(x , base=10).\nRe-expressing with the log is particularly useful when the change in one value as a function of another is multiplicative and not additive. An example of such a dataset is the compounding interest. Let’s assume that we start off with $1000 in an investment account that yields 10% interest each year. We can calculate the size of our investment for the next 50 years as follows:\n\nrate &lt;- 0.1                 # Rate is stored as a fraction\ny    &lt;- vector(length = 50) # Create an empty vector that can hold 50 values\ny[1] &lt;- 1000                # Start 1st year with $1000\n\n# Next, compute the investment amount for years 2, 3, ..., 50.\n# Each iteration of the loop computes the new amount for year i based \n# on the previous year's amount (i-1).\nfor(i in 2:length(y)){\n  y[i] &lt;- y[i-1] + (y[i-1] * rate)  # Or y[i-1] * (1 + rate)\n}\n\nThe vector y gives us the amount of our investment for each year over the course of 50 years.\n\n\n [1]   1000.000   1100.000   1210.000   1331.000   1464.100   1610.510   1771.561   1948.717\n [9]   2143.589   2357.948   2593.742   2853.117   3138.428   3452.271   3797.498   4177.248\n[17]   4594.973   5054.470   5559.917   6115.909   6727.500   7400.250   8140.275   8954.302\n[25]   9849.733  10834.706  11918.177  13109.994  14420.994  15863.093  17449.402  19194.342\n[33]  21113.777  23225.154  25547.670  28102.437  30912.681  34003.949  37404.343  41144.778\n[41]  45259.256  49785.181  54763.699  60240.069  66264.076  72890.484  80179.532  88197.485\n[49]  97017.234 106718.957\n\n\nWe can plot the values as follows:\n\nplot(y, pch = 20)\n\n\n\n\nThe change in difference between values from year to year is not additive, in other words, the difference between years 48 and 49 is different than that for years 3 and 4.\n\n\n\n\n\n\n\nYears\nDifference\n\n\n\n\ny[49] - y[48]\n8819.75\n\n\ny[4] - y[3]\n121\n\n\n\nHowever, the ratios between the pairs of years are identical:\n\n\n\n\n\n\n\nYears\nRatio\n\n\n\n\ny[49] / y[48]\n1.1\n\n\ny[4] / y[3]\n1.1\n\n\n\nWe say that the change in value is multiplicative across the years. In other words, the value amount 6 years out is \\(value(6) = (yearly\\_increase)^{6} \\times 1000\\) or 1.1^6 * 1000 = 1771.561 which matches value y[7].\nWhen we expect a variable to change multiplicatively as a function of another variable, it is usually best to transform the variable using the logarithm. To see why, plot the log of y.\n\nplot(log(y), pch=20)\n\n\n\n\nNote the change from a curved line to a perfectly straight line. The logarithm will produce a straight line if the rate of change for y is constant over the range of x. This is a nifty property since it makes it so much easier to see if and where the rate of change differs. For example, let’s look at the population growth rate of the US from 1850 to 2013.\n\ndat &lt;- read.csv(\"https://mgimond.github.io/ES218/Data/Population.csv\", header=TRUE)\nplot(US ~ Year, dat, type=\"l\") \n\n\n\n\nThe population count for the US follows a slightly curved (convex) pattern. It’s difficult to see from this plot if the rate of growth is consistent across the years (though there is an obvious jump in population count around the 1950’s). Let’s log the population count.\n\nplot(log(US) ~ Year, dat, type=\"l\")  \n\n\n\n\nIt’s clear from the log plot that the rate of growth for the US has not been consistent over the years (had it been consistent, the line would have been straight). In fact, there seems to be a gradual decrease in growth rate over the 150 year period (though a more thorough analysis would be needed to see where and when the growth rates changed).\nA logarithm is defined by a base. Some of the most common bases are 10, 2 and exp(1) with the latter being the natural log. The bases can be defined in the call to log() by adding a second parameter to that function. For example, to apply the log base 2 to the 5th value of the vector y, type log( y[5], 2). To apply the natural log to that same value, simply type log( y[5], exp(1)). If you don’t specify a base, R will default to the natural log.\nThe choice of a log base will not impact the shape of the logged values in the plot, it only changes its absolute values. So unless interpretation of the logged value is of concern, any base will do. Generally, you want to avoid difficult to interpret logged values. For example, if you apply log base 10 to the investment dataset, you will end up with a smaller range of values–thus more decimal places to work with–whereas a base 2 logarithm will generate a wider range of values and thus fewer decimal places to work with.\n\n\n\n\n\nA rule of thumb is to use log base 10 when the range of values to be logged covers 3 or more powers of ten, \\(\\geq 10^3\\) (for example, a range of 5 to 50,000); if the range of values covers 2 or fewer powers of ten, \\(\\leq 10^2\\)(for example, a range of 5 to 500) then a natural log or a log base 2 log is best."
  },
  {
    "objectID": "re_express.html#the-tukey-transformation",
    "href": "re_express.html#the-tukey-transformation",
    "title": "21  Re-expressing values",
    "section": "21.3 The Tukey transformation",
    "text": "21.3 The Tukey transformation\nThe Tukey family of transformations offers a broader range of re-expression options (which includes the log). The values are re-expressed using the algorithm:\n\\[\n\\begin{equation} T_{Tukey} =\n\\begin{cases} x^p , & p \\neq  0 \\\\\n              log(x), & p = 0  \n\\end{cases}\n\\end{equation}\n\\] The objective is to find a value for \\(p\\) from a “ladder” of powers (e.g. -2, -1, -1/2, 0, 1/2, 1, 2) that does a good job in re-expressing the batch of values. Technically, \\(p\\) can take on any value. But in practice, we normally pick a value for \\(p\\) that may be “interpretable” in the context of our analysis. For example, a log transformation (p=0) may make sense if the process we are studying has a steady growth rate. A cube root transformation (p = 1/3) may make sense if the entity being measured is a volume (e.g. rain fall measurements). But sometimes, the choice of \\(p\\) may not be directly interpretable or may not be of concern to the analyst.\nA nifty solution to finding an appropriate \\(p\\) is to create a function whose input is the vector (that we want to re-express) and a \\(p\\) parameter we want to explore.\n\nRE &lt;- function(x, p = 0) {\n  if(p != 0) {\n    z &lt;- x^p\n  } else{\n    z &lt;- log(x)\n  }\n  return(z)\n}\n\nTo use the custom function RE simply pass two vectors: the batch of numbers being re-expressed and the \\(p\\) parameter.\n\n# Create a skewed distribution of 50 random values\nset.seed(9)\na &lt;- rgamma(50, shape = 1)\n\n# Let's look at the skewed distribution\nboxplot(a, horizontal = TRUE)\n\n\n\n\nThe batch is strongly skewed to the right. Let’s first try a square-root transformation (p=1/2)\n\na.re &lt;- RE(a, p = 1/2)   \nboxplot(a.re, horizontal = TRUE)\n\n\n\n\nThat certainly helps minimize the skew, but the distribution still lacks symmetry. Let’s try a log transformation (p=0):\n\na.re &lt;- RE(a, p = 0)   \nboxplot(a.re, horizontal = TRUE)\n\n\n\n\nThat’s a little too much over-correction; we don’t want to substitute a right skew for a left skew. Let’s try a power in between (i.e. p=1/4):\n\na.re &lt;- RE(a, p = 1/4)   \nboxplot(a.re, horizontal = TRUE)\n\n\n\n\nThat’s much better. The distribution is now nicely balanced about its median."
  },
  {
    "objectID": "re_express.html#the-box-cox-transformation",
    "href": "re_express.html#the-box-cox-transformation",
    "title": "21  Re-expressing values",
    "section": "21.4 The Box-Cox transformation",
    "text": "21.4 The Box-Cox transformation\nAnother family of transformations is the Box-Cox transformation. The values are re-expressed using a modified version of the Tukey transformation:\n\\[\n\\begin{equation} T_{Box-Cox} =\n\\begin{cases} \\frac{x^p - 1}{p}, & p \\neq  0 \\\\\n              log(x), & p = 0\n\\end{cases}\n\\end{equation}\n\\] Just as we can create a custom Tukey transformation function, we can create a Box-Cox transformation function too:\n\nBC &lt;- function(x, p = 0) {\n  if(p == 0) {\n    z &lt;- log(x)\n  } else {\n    z &lt;- (x^p - 1)/p\n  }\n  return(z)\n}\n\nWhile both the Box-Cox and Tukey transformations method will generate similar distributions when the power p is 0 or greater, they will differ in distributions when the power is negative. For example, when re-expressing mtcars$mpg using an inverse power (p = -1), Tukey’s re-expression will change the data order but the Box-Cox transformation will not as shown in the following plots:.\n\nplot(mpg ~ disp, mtcars, main = \"Original data\")\nplot(RE(mtcars$mpg, p = -1) ~ mtcars$disp, main = \"Tukey\")\nplot(BC(mtcars$mpg, p = -1) ~ mtcars$disp, main = \"Box-Cox\")\n\n\n\n\nThe original data shows a negative relationship between mpg and disp; the Tukey re-expression takes the inverse of mpg which changes the nature of the relationship between the y and x variables where we have a positive relationship between the re-expressed mpg variable and disp variable (note that by simply changing the sign of the re-expressed value, -x^(-1) maintains the nature of the original relationship); the Box-Cox transformation, on the other hand, maintains this negative relationship.\nThe choice of re-expression will depend on the analysis context. For example, if you want an easily interpretable transformation then opt for the Tukey re-expression. If you want to compare the shape of transformed variables, the Box-Cox approach will be better suited."
  },
  {
    "objectID": "re_express.html#re-expressing-to-stabilize-spread",
    "href": "re_express.html#re-expressing-to-stabilize-spread",
    "title": "21  Re-expressing values",
    "section": "21.5 Re-expressing to stabilize spread",
    "text": "21.5 Re-expressing to stabilize spread\nWe learned in the last chapter that one version of the spread-level plot can suggest the power transformation to use. We’ll recreate the plots (however, this time we’ll make use of the base plotting environment).\n\nlibrary(dplyr)\n\n# Create s-l table \ndf.sl &lt;- iris %&gt;%\n  group_by(Species)  %&gt;%\n  summarise (level  = log(median(Petal.Length)),\n                IQR = IQR(Petal.Length),  # Computes the interquartile range\n             spread = log(IQR))\n\n# Plot spread vs median\nplot(spread ~ level, df.sl, pch = 16)\n\n\n\n\nThe plot suggests a monotonic relationship between spread and median. Next, we’ll fit a line to this scatter plot and compute its slope. We’ll use the robust MASS::rlm() function, but note that any other line fitting strategies could be used as well.\n\nM &lt;- MASS::rlm(spread ~ level, df.sl)\n\n\n\n\nThe slope can be used to come up with the best power transformation to minimize the systematic increase in spread: \\(p = 1 - slope\\).\nThe slope is extracted from the model M using the coef function:\n\ncoef(M)\n\n(Intercept)       level \n  -2.204242    1.143365 \n\n\nThe second value in the output is the slope. So the power to use is 1 - 1.14 or -0.14. In the previous chapter, we suggested rounding the power to 0 (i.e. a log transformation). Here, we will apply the suggested power using BC function to re-express the Petal.Length values. We choose the Box-Cox method over the Tukey method because the power is negative. We’ll add the re-expressed values as a new column to df:\n\niris$re.Petal.Length &lt;- BC(iris$Petal.Length, -0.14)\n\nLet’s compare boxplots between the original values with the re-expressed values.\n\nboxplot(Petal.Length ~ reorder(Species, Petal.Length, median), iris, \n        main = \"Original data\")\nboxplot(re.Petal.Length ~ reorder(Species, Petal.Length, median), iris, \n        main = \"Re-expressed data\")\n\n\n\n\nRecall that our goal here is to minimize any systematic relationship between spread and median. The re-expression seems to have eliminated the monotonic increase in spread, but an s-l plot will be more effective in helping confirm this.\n\nlibrary(ggplot2)\ndf1 &lt;- iris %&gt;%\n  group_by(Species)  %&gt;%\n  mutate( Median = median(re.Petal.Length),\n          Residuals = sqrt( abs( re.Petal.Length - Median)))   \n\n# Generate the s-l plot\nggplot(df1, aes(x = Median, y = Residuals)) + \n  geom_jitter(alpha = 0.3, width = 0.003, height = 0) +\n  stat_summary(fun.y = median, geom = \"line\", col = \"red\") +\n  ylab(expression(sqrt( abs( \" Residuals \")))) +\n  geom_text(aes(x = Median, y = 0.6, label = Species) )\n\n\n\n\nThe transformation does a good job in stabilizing the spread. But, given that the log transformation did just as good a job in stabilizing the spread, it may prove easier, for sake of interpretation, to adopt the ubiquitous log transformation for this dataset."
  },
  {
    "objectID": "re_express.html#the-eda_re-function",
    "href": "re_express.html#the-eda_re-function",
    "title": "21  Re-expressing values",
    "section": "21.6 The eda_re function",
    "text": "21.6 The eda_re function\nThe tukeyedar package offers a re-expression function that implements both the Tukey and Box-Cox transformations. To apply the Tukey transformation set tukey = TRUE (the default setting). For the Box-Cox transformation, set tukey = FALSE. The power parameter is defined by the p argument.\nFor a Tukey transformation:\n\nlibrary(tukeyedar)\nboxplot( eda_re(mtcars$mpg, p=-0.25), horizontal = TRUE)\n\n\n\n\nFor a Box-Cox transformation:\n\nboxplot( eda_re(mtcars$mpg, p=-0.25, tukey = FALSE), horizontal = TRUE)"
  },
  {
    "objectID": "re_express.html#the-eda_unipow-function",
    "href": "re_express.html#the-eda_unipow-function",
    "title": "21  Re-expressing values",
    "section": "21.7 The eda_unipow function",
    "text": "21.7 The eda_unipow function\nThe eda_unipow function will generate a matrix of distribution plots for a given vector of values using different power of transformations. By default, these powers are c(2, 1, 1/2, 1/3, 0, -1/3, -1/2, -1, -2)–this generates a 3-by-3 plot matrix.\n\neda_unipow(a)\n\n\n\n\nWhat we are seeking is a symmetrical distribution. The power of 1/3 comes close. We can tweak the input power parameters to cove a smaller range of values.\n\neda_unipow(a, p = c(1/3, 1/4, 1/5, 1/6))\n\n\n\n\nThe above plot suggests that we could go with a power transformation of 0.25 or 0.2. The choice may come down to convenience and “interpretability”.\nThe next chapter looks at an advanced technique in tweaking a power transformation: Letter value summaries."
  },
  {
    "objectID": "letter_values.html#introduction",
    "href": "letter_values.html#introduction",
    "title": "22  Letter value summaries",
    "section": "22.1 Introduction",
    "text": "22.1 Introduction\nThe boxplot is a five number summary of a batch of values that gives us a handle on the symmetry (or lack thereof) of the data. The five numbers consist of the median, the inter-quartile values and the upper and lower adjacent values (aka whiskers). The letter value summary was introduced by John Tukey and extends the boxplot’s 5 number summary by exploring the symmetry of the batch for depth levels other than the half (median) or the fourth (quartiles)."
  },
  {
    "objectID": "letter_values.html#constructing-the-letter-value-summaries",
    "href": "letter_values.html#constructing-the-letter-value-summaries",
    "title": "22  Letter value summaries",
    "section": "22.2 Constructing the letter value summaries",
    "text": "22.2 Constructing the letter value summaries\nLet’s start with a simple batch of numbers: 24, 3, 5, 10, 13, 6, 16, 22, 4, 19, 17.\n\n22.2.0.1 Order the values\nFirst, we order the numbers from smallest to largest.\n\n\n\n22.2.0.2 Find the median (M)\nNext, we find the median. It’s the location in the enumerated values that splits the batch into two equal sets of values. If the number of values is odd, then the median is the value furthest from the ends. If the number of values in the batch is even, then the median is the average of the two middle values, each furthest from its end. A simple formula to identify the element number (or depth) of the batch associated with the median is:\n\\[\ndepth\\ of\\ median = \\frac{n + 1}{2}\n\\]\nwhere \\(n\\) is the number of values in the batch. In our example, we have 11 values, so the median is (11 + 1)/2 or 6; it’s the 6th element from the left (or the right) of the sorted values.\n\nIf our batch consisted of an even number of values such as 10, the median would be the 5.5th value which does not coincide with an existing value. This would require that we find the 5th and 6th elements in the batch, then compute their average to find the median.\nThe median is the value furthest from the extreme values; it’s said to have the greatest depth (e.g. a depth of 6 in our example). The minimum and maximum values have the lowest depth with a depth value of 1, each.\n\n\n22.2.0.3 Find the hinges (H)\nNext, we take both halves of our batch of ordered numbers and find the middle of each. These mid points are referred to as hinges. They can be easily computed by modifying the formula used to find the median: we simply substitute the value \\(n\\) with the depth associated with the median, \\(d(M)\\) (i.e. the median becomes an extreme value in this operation).\n\\[\ndepth\\ of\\ hinge = \\frac{d(M) + 1}{2}\n\\]\nIn our working example, the depth of the median is 6, therefore the depth of the hinge is (6+1)/2 = 3.5. So the hinge is the 3.5th element from the left (or right) of the first half of the batch and the 3.5th element from the left (or right) of the second half of the batch. Since the depth does not fall on an existing value, we need to compute it using the two closest values (depth 3 and depth 4). This gives us (8+11)/2=9.5 for the left hinge and (22+24)/2=23 for the right hinge.\n\nIf our batch consisted of even number of values, we would need to drop the ½ fraction from depth of the median before computing the depth of the hinge. For example, if we had 10 values the depth of the median would be 5.5 and the depth of the hinge would be calculated as (5+1)/2.\nNote that the hinges are similar to the quartiles but because they are computed differently, their values may be slightly different from what you might get from a boxplot, for example.\n\n\n22.2.0.4 Find the other letter summaries (E, D, C, B, A, etc…)\nSo far, we’ve found the median (M) and the hinges (H). We keep computing the depths for each outer group of values delimited by the outer extreme values and the previous depth. For example, the mid-point of the outer quarters gives us our eights (E):\n\\[\ndepth\\ of\\ eights = \\frac{d(H) + 1}{2}\n\\]\nor, after dropping the ½ fraction from the depth of the hinge, (3+1)/2=2.\nThis continues until we’ve exhausted all depths (i.e. until we reach a depth of 1 associated with the minimum and maximum values). Once past the eight, we label each subsequent depths using letters in reverse lexicographic order starting with D (for sixteenth) then C, B, A, Z, Y, etc…\nIn our working example, we stop at a depth of D (though some will stop at a depth of two and only report the extreme values thereafter).\n\n\n22.2.0.5 The mids and spreads\nOnce we’ve identified the values associated with each depth, we compute the middle value for each depth pair. For example, the middle value for the paired hinges is 16.25; the middle value for the paired eights is 14; and so on. We can also compute the spread for each depth by computing the difference between each paired value.\n\nThe letter value summary is usually reported in tabular form:\n\n\n\n\n\nletter\ndepth\nlower\nmid\nupper\nspread\n\n\n\n\nM\n6.0\n13.0\n13.00\n13\n0.0\n\n\nH\n3.5\n5.5\n11.75\n18\n12.5\n\n\nE\n2.0\n4.0\n13.00\n22\n18.0\n\n\nD\n1.5\n3.5\n13.25\n23\n19.5\n\n\nC\n1.0\n3.0\n13.50\n24\n21.0"
  },
  {
    "objectID": "letter_values.html#the-eda_lsum-function",
    "href": "letter_values.html#the-eda_lsum-function",
    "title": "22  Letter value summaries",
    "section": "22.3 The eda_lsum function",
    "text": "22.3 The eda_lsum function\nA custom function, eda_lsum, is available in the tukeyedar package that will compute the letter value summaries.\nFor example, to generate the letter summary function for a batch of values x, type:\n\nlibrary(tukeyedar)\nx &lt;- c(22, 8, 11, 3, 26, 1, 14, 18, 20, 25, 24)\neda_lsum(x)\n\n  letter depth lower   mid upper spread\n1      M   6.0  18.0 18.00  18.0    0.0\n2      H   3.5   9.5 16.25  23.0   13.5\n3      E   2.0   3.0 14.00  25.0   22.0\n4      D   1.5   2.0 13.75  25.5   23.5\n5      C   1.0   1.0 13.50  26.0   25.0\n\n\nYou can specify the number of levels with the l= argument. For example, to limit the output to just 3 depths, type:\n\neda_lsum(x, l = 3)\n\n  letter depth lower   mid upper spread\n1      M   6.0  18.0 18.00    18    0.0\n2      H   3.5   9.5 16.25    23   13.5\n3      E   2.0   3.0 14.00    25   22.0"
  },
  {
    "objectID": "letter_values.html#interpreting-the-letter-value-summaries",
    "href": "letter_values.html#interpreting-the-letter-value-summaries",
    "title": "22  Letter value summaries",
    "section": "22.4 Interpreting the letter value summaries",
    "text": "22.4 Interpreting the letter value summaries\nLet’ explore the letter summary values for five simulated distributions. We’ll start with a strong right-skewed distribution then progress to a strong left-skewed distribution with a Gaussian (normal) distribution in between.\n\n\n\n\n\nNote the shape of the letter summaries vis-a-vis the direction of the skew. Note too that the letter value summary plot is extremely sensitive to deviations from perfect symmetry. This is apparent in the middle plot which is for a perfectly symmetrical (Gaussian) distribution. The reason has to do with machine precision: the range of values along the y-axis is extremely small, \\(10^{-16}\\), which is the lower limit of the computer’s precision.\nThis sensitivity has its rewards. Note the second plot from the left and the right. The asymmetry is barely noticeable in both distributions, yet the letter value summaries do a great job in identifying the slight asymmetry. Even the boxplots cannot convey this asymmetry as effectively.\n\n\n\n\n\nThis is not to say that just because asymmetry is present in the letter summary values we necessarily have a problem; but it may warrant further exploration before proceeding with the analysis–especially if statistical procedures warrant it."
  },
  {
    "objectID": "robustness.html#introduction",
    "href": "robustness.html#introduction",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.1 Introduction",
    "text": "23.1 Introduction\nThe following data represent 1,2,3,4-tetrachlorobenzene (TCB) concentrations (in units of ppb) for two site locations: a reference site free of external contaminants and a contaminated site that went through remediation (dataset from Millard et al., p. 416-417).\n\n# Create the two data objects (TCB concentrations for reference and contaminated sites)\nRef &lt;-  c(0.22,0.23,0.26,0.27,0.28,0.28,0.29,0.33,0.34,0.35,0.38,0.39,\n          0.39,0.42,0.42,0.43,0.45,0.46,0.48,0.5,0.5,0.51,0.52,0.54,\n          0.56,0.56,0.57,0.57,0.6,0.62,0.63,0.67,0.69,0.72,0.74,0.76,\n          0.79,0.81,0.82,0.84,0.89,1.11,1.13,1.14,1.14,1.2,1.33)\nCont &lt;- c(0.09,0.09,0.09,0.12,0.12,0.14,0.16,0.17,0.17,0.17,0.18,0.19,\n          0.2,0.2,0.21,0.21,0.22,0.22,0.22,0.23,0.24,0.25,0.25,0.25,\n          0.25,0.26,0.28,0.28,0.29,0.31,0.33,0.33,0.33,0.34,0.37,0.38,\n          0.39,0.4,0.43,0.43,0.47,0.48,0.48,0.49,0.51,0.51,0.54,0.6,\n          0.61,0.62,0.75,0.82,0.85,0.92,0.94,1.05,1.1,1.1,1.19,1.22,\n          1.33,1.39,1.39,1.52,1.53,1.73,2.35,2.46,2.59,2.61,3.06,3.29,\n          5.56,6.61,18.4,51.97,168.64)\n\n# We'll create a long-form version of the data for use with some of the functions\n# in this exercise\ndf &lt;- data.frame( Site = c(rep(\"Cont\",length(Cont) ), rep(\"Ref\",length(Ref) ) ),\n                  TCB  = c(Cont, Ref ) )\n\nOur goal is to assess if, overall, the concentrations of TCB at the contaminated site are different from those of the reference site with the alternative being that the contaminated site has concentrations greater than those at the reference site."
  },
  {
    "objectID": "robustness.html#a-typical-statistical-approach-the-two-sample-t-test",
    "href": "robustness.html#a-typical-statistical-approach-the-two-sample-t-test",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.2 A typical statistical approach: the two sample t-Test",
    "text": "23.2 A typical statistical approach: the two sample t-Test\nWe are interested in answering the question: “Did the cleanup at the contaminated site reduce the concentration of TCB down to background (reference) levels?”. If the question being addressed is part of a decision making process such as “Should we continue with the remediation?” we might want to assess if the difference in TCBs between both sites is “significant” enough to conclude that the TCBs are higher than would be expected if chance alone was the process at play.\nA popular statistical procedure used to help address this question is the two sample t-Test. The test is used to assess whether or not the mean concentration between both batches of values are significantly different from one another. The test can be framed in one of three ways: We can see if the batches are similar, if one batch is greater than the other batch, or if one batch is smaller than the other batch. In our case, we will assess if the Cont batch is greater than the Ref batch (this is the alternative hypothesis). We’ll make use of the t.test function and set the parameter alt to \"greater\".\n\nt.test(Cont, Ref, alt=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  Cont and Ref\nt = 1.4538, df = 76.05, p-value = 0.07506\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.4821023        Inf\nsample estimates:\nmean of x mean of y \n3.9151948 0.5985106 \n\n\nThe test suggests that there is just a small chance (a 7.5% chance) that the reference site could have generated mean concentrations as great as those found at the contaminated site. The test also outputs the means of each batch: 3.9 ppb for the contaminated site and 0.6 ppb for the reference site.\nMany ancillary data analysts may stop here and proceed with the decision making process. This is not good practice. To see why, let’s deconstruct the t-test.\nFirst, we need to find out how the test is characterizing the batches of numbers. The t-test characterizes the location of the batch using the mean, and the spread using the standard deviation. In essence, the test is reducing the complexity of the batches down to two numbers.\nThe t-test uses these numbers to reconstruct, then compare the distributions. For example, here’s how the t-test is characterizing the distribution of value for the reference site, Ref:\n\n\n\n\n\nThe right side of the plot is the fitted Normal distribution computed from the sample’s standard deviation–this is how the t-test is characterizing the Ref distribution. The left side of the plot is the density distribution showing the actual shape of the distribution. The points in between both plot halves are the actual values. Note the skew towards higher concentrations. For the Ref data, one might argue that the Normal fit is doing a reasonably good job in characterizing the distribution of values though the outliers may be exaggerating the width of the Normal distribution.\nNow, let’s see how well the Normal fit characterizes the distribution of the contaminated site values, Cont.\n\n\n\n\n\nThe Normal fit is on the right. The density plot is barely noticeable on the left side of the plot! You’ll note the tight cluster of points near the center of the Normal distribution. There are just a few points that extend beyond the tight cluster. These outliers are disproportionately inflating the standard deviation which, in turn, leads to the disproportionately large Normal distribution that is adopted in the t-test.\nSince the t-test adopts a Normal characterization of the spread, it would behoove us to check the assumption of normality using the normal q-q plot. Here, we’ll compare the Cont values to a Normal distribution.\n\n\n\n\n\nThis is a textbook example of a batch of values that does not conform to a Normal distribution. At least four values (which represent ~5% of the data) seem to contribute to the strong skew and to a much distorted representation of location and spread. The mean and standard deviation are not robust to extreme values. In essence, all it takes is one single outlier to heavily distort the representation of location and spread in our data. The mean and standard deviation have a breakdown point of 1/n where n is the sample size.\nThe median and interquartile range are less sensitive to extreme values. In fact, the median has a breakdown point of n/2. In other words, half of the values would have to be modified to alter the median.\nThe boxplot makes use of these robust measures of location and spread; let’s compare the batches with and without the extreme (outlier) values.\n\n\n\n\n\nNote that because of the robust nature of the median and interquartile range, the boxplot helps us spot the outliers. In fact, the boxplot has a breakdown point of n/4 (i.e. 25% of the values must be extreme before we see any masking of extreme values). The standard deviation, on the other hand, can be inflated by one extreme value thus masking the potentially problematic values.\nOne observation that can also be gleaned from this plot is the skewed nature of the Cont data within the interquartile range (IQR). This suggests that even if we were to remove the outliers, the data would violate the normal distribution requirements.\nBut, most importantly, the boxplot seems to contradict the conclusion gleaned from the t-test. The plot suggests that the contaminated site has a lower overall concentration than that of the reference site when comparing medians instead of means!"
  },
  {
    "objectID": "robustness.html#re-expression",
    "href": "robustness.html#re-expression",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.3 Re-expression",
    "text": "23.3 Re-expression\nIf we are to use the t-test, we need to make sure that the distributional requirements are met. Even Welch’s modification has one requirement about the distribution: both spreads must follow a normal distribution. Let’s compare both batches to a normal distribution via a normal q-q plot. We’ll make use of the custom eda_qq function:\n\nlibrary(tukeyedar)\neda_qq(Ref, norm=TRUE)\neda_qq(Cont, norm=TRUE)\n\n\n\n\nThese batches do not follow the straight line. This suggests skewness in the distribution (as was observed with the boxplots). A workaround to this problem is to re-express the batches of values in such a way to render them as close to normal as possible. However, in doing so, we must make sure that both batches are re-expressed in an equal way to facilitate comparison. A popular re-expression used with observational data that exhibit skewness towards higher values is the log transformation. The eda_qq function has an argument, p=, that takes as input a re-expression that will be applied to the values. The function will also allow us to choose between a Tukey (tukey=TRUE) or Box-Cox (tukey=FALSE) method. The default is set to tukey=FALSE. To apply a log transformation, we set p to 0.\n\neda_qq(Ref, norm=TRUE, p = 0)\neda_qq(Cont, norm=TRUE, p = 0)\n\n\n\n\nThe log transformed data is an improvement, but a skew in both batches is still apparent. We’ll need to explore other powers next."
  },
  {
    "objectID": "robustness.html#fine-tuning-the-re-expression",
    "href": "robustness.html#fine-tuning-the-re-expression",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.4 Fine-tuning the re-expression",
    "text": "23.4 Fine-tuning the re-expression\nThe log transformation is one of many re-expressions that can be applied to the data. Let’s explore the skewness across different “depths” of the Cont values to see if the skewness is systematic. We’ll use letter value summary plots to help guide us to a reasonable re-expression. We’ll make use of the custom eda_lsum function to generate the table and ggplot2 to generate the plot.\nFirst, we’ll look at the raw contaminated site data:\n\nlibrary(ggplot2)\n\nCont.lsum &lt;- eda_lsum(Cont, l=7)\nggplot(Cont.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + \n                    scale_x_reverse() +geom_text(vjust=-.5, size=4)\n\n\n\n\nThe data become strongly skewed for 1/32th of the data (depth letter C). Let’s now look at the reference site.\n\nRef.lsum &lt;- eda_lsum(Ref, l=7)\nggplot(Ref.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + \n                   scale_x_reverse() +geom_text(vjust=-.5, size=4)\n\n\n\n\nA skew is also prominent here but a bit more consistent across the depths with a slight drop between depths D and C (16th and 32nd extreme values).\nNext, we will find a power function that re-expresses the values to satisfy the t-test distribution requirement. We’ll first look at the log transformation implemented in the last section. Note that we are using the custom eda_re function to transform the data. We’ll also make use of some advanced coding to reduce the code chunk size.\n\nlibrary(dplyr)\n\ndf  %&gt;% \n  group_by(Site) %&gt;%\n  do(eda_lsum( eda_re(.$TCB,0), l=7) ) %&gt;%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThe log transformation seems to work well with the reference site, but it’s not aggressive enough for the contaminated site. Recall that to ensure symmetry across all levels of the batches, the letter values must follow a straight (horizontal) line. Let’s try a power of -0.5. Given that the negative power will reverse the order of the values if we adopt the Tukey transformation, we’ll use the default Box-Cox method.\n\ndf  %&gt;% \n  group_by(Site) %&gt;%\n  do(eda_lsum( eda_re(.$TCB,-0.5, tukey=FALSE), l=7) ) %&gt;%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThis seems to be too aggressive. We are facing a situation where attempting to normalize one batch distorts the other batch. Let’s try a compromise and use -.35.\n\ndf  %&gt;% \n  group_by(Site) %&gt;%\n  do(eda_lsum( eda_re(.$TCB,-0.35, tukey=FALSE), l=7) ) %&gt;%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThis seems to be a bit better. It’s obvious that we will not find a power transformation that will satisfy both batches, so we will need to make a judgement call and work with a power of -.35 for now.\nLet’s compare the re-expressed batches with a normal distribution.\n\neda_qq(Cont, norm = TRUE, p = -0.35)\neda_qq(Ref, norm = TRUE, p = -0.35)\n\n\n\n\nThe distributions look quite good when viewed in a normal q-q plot. Let’s now compare the re-expressed batches using a density/normal fit plot.\n\n\n\n\n\nRecall that the right half is the Normal fit to the data and the left half is the density plot. There is far greater agreement between the normal fits and their accompanying density distributions for both sites. You’ll also note that the mean concentration is now smaller at the contaminated site than it is a the reference site–this is in agreement with what we observed in the boxplots.\nSo, how much impact did the skewed distributions have on the t-test? Now that we have normally distributed values, let’s rerun the t-test using the re-expressed values. Note that because the power is negative, we will adopt the Box-Cox transformation to preserve order by setting tukey=FALSE to the eda_re function.\n\nt.test(eda_re(Cont,-0.35, tukey=FALSE), eda_re(Ref,-0.35,tukey=FALSE), alt=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  eda_re(Cont, -0.35, tukey = FALSE) and eda_re(Ref, -0.35, tukey = FALSE)\nt = -1.0495, df = 111.68, p-value = 0.8519\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.4845764        Inf\nsample estimates:\n mean of x  mean of y \n-0.9264188 -0.7386209 \n\n\nThe result differs significantly from that with the raw data. This last run gives us a p-value of 0.85 whereas the first run gave us a p-value of 0.075. This suggests that there is an 85% chance that the reference site could have generated a mean concentration greater than that observed at the contaminated site."
  },
  {
    "objectID": "robustness.html#addendum",
    "href": "robustness.html#addendum",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.5 Addendum",
    "text": "23.5 Addendum\nIt was obvious from the boxplot of the original (un-transformed) data that for a few sites, more remediation was needed given their higher than typical TCB values. But, it was also quite apparent from the boxplot that for many other sites, the concentrations were much less than those found at the reference site suggesting that the remediation reduced TCB concentrations that occur naturally in that environment given that none of the reference site values were as low as those found at the contaminated site. No statistical procedure was needed to come to this conclusion! So while the t-test told us something about the significance of differences in their mean values, it failed to pick up on some of the intricate details that could only be gleaned from our plots. This is the whole essence of exploratory data analysis!"
  },
  {
    "objectID": "robustness.html#references",
    "href": "robustness.html#references",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.6 References",
    "text": "23.6 References\nMillard S.P, Neerchal N.K., Environmental Statistics with S-Plus, 2001."
  },
  {
    "objectID": "bivariate.html#scatter-plot",
    "href": "bivariate.html#scatter-plot",
    "title": "24  Fits and residuals",
    "section": "24.1 Scatter plot",
    "text": "24.1 Scatter plot\nA scatter plot is a popular visualization tool used to compare values between two variables. Sometimes one variable is deemed dependent on another variable; the latter being the independent variable. The dependent variable is also sometimes referred to as the response variable and the independent variable as the factor (this is not to be confused with the factor data type used in R as a grouping variable). The dependent variable is usually plotted on the y-axis and the independent variable is usually plotted on the x-axis. Other times, one does not seek a dependent-independent relationship between variables but is simply interested in studying the relationship between the tow variables.\nA scatter plot can be generated using the base plotting environment as follows. Here, we’ll plot Cleveland’s Ganglion dataset.1\n\ndf &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/ganglion.csv\")\n\nplot(cp.ratio ~ area, dat = df)\n\n\n\n\nOr, using ggplot2, as follows:\n\nlibrary(ggplot2)\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point()\n\n\n\n\nThe data represent the ratio between the ganglion cell density of a cat’s central retina to that of its peripheral density (variable cp.ratio), and the cat’s retina surface area (area) during its early development (ranging from 35 to 62 days of gestation)."
  },
  {
    "objectID": "bivariate.html#fitting-the-data",
    "href": "bivariate.html#fitting-the-data",
    "title": "24  Fits and residuals",
    "section": "24.2 Fitting the data",
    "text": "24.2 Fitting the data\nScatter plots are a good first start in visualizing bivariate data, but this is sometimes not enough. Our eyes need “guidance” to help perceive patterns. Another visual aid involves fitting the data with a line. We will explore two fitting strategies: the parametric fit and the non-parametric fit.\n\n24.2.1 Parametric fit\nA parametric fit is one where we impose a structure to the data–an example of which is a straight line (also referred to as a 1st order polynomial fit). The parametric fit is by far the most popular fitting strategy used in the realm of data analysis and statistics.\n\n24.2.1.1 Fitting a straight line\nA straight line is the simplest fit one can make to bivariate data. A popular method for fitting a straight line is the least-squares method. We’ll use R’s lm() function which provides us with a slope and intercept for the best fit line.\nThis can be implemented in the base plotting environment as follows:\n\nM &lt;- lm(cp.ratio ~ area, dat = df)\nplot(cp.ratio ~ area, dat = df)\nabline(M, col = \"red\")\n\n\n\n\nIn the ggplot2 plotting environment, we can make use of the stat_smooth function to generate the regression line.\n\nlibrary(ggplot2)\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n             stat_smooth(method =\"lm\", se = FALSE)\n\n\n\n\nThe se = FALSE option prevents R from drawing a confidence envelope around the regression line. Confidence envelopes are used in inferential statistics (a topic not covered in this course).\nThe straight line is a first order polynomial with two parameters, \\(a\\) and \\(b\\), that define an equation that best describes the relationship between the two variables:\n\\[\ny = a + b (x)\n\\]\nwhere \\(a\\) and \\(b\\) can be extracted from the regression model object M as follows:\n\ncoef(M)\n\n(Intercept)        area \n 0.01399056  0.10733436 \n\n\nThus \\(a\\) = 0.014 and \\(b\\) = 0.11.\n\n\n24.2.1.2 Fitting a 2nd order polynomial\nA second order polynomial is a three parameter function (\\(a\\), \\(b\\) and \\(c\\)) whose equation \\(y = a + bx + cx^2\\) defines a curve that best fits the data. We define such a relationship in R using the formula cp.ratio ~ area + I(area^2). The identity function I() preserves the arithmetic interpretation of area^2 as part of the model. Our new lm expression and resulting coefficients follow:\n\nM2 &lt;- lm(cp.ratio ~  area + I(area^2) , dat = df)\ncoef(M2)\n\n  (Intercept)          area     I(area^2) \n 2.8684792029 -0.0118691702  0.0008393243 \n\n\nThe quadratic fit is thus,\n\\[\ny = 2.87 - 0.012 x + 0.000839 x^2\n\\]\nIn using the base plot environment, we cannot use abline to plot the predicted 2nd order polynomial curve since abline only draws straight lines. We will need to construct the line manually using the predict and lines functions.\n\nplot(cp.ratio ~ area, dat=df)\nx.pred &lt;- data.frame( area = seq(min(df$area), max(df$area), length.out = 50) )\ny.pred &lt;- predict(M2, x.pred)\nlines(x.pred$area, y.pred, col = \"red\")\n\n\n\n\nTo generate the same plot in ggplot2, simply pass the formula as an argument to stat_smooth:\n\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n  stat_smooth(method = \"lm\", se = FALSE, formula = y ~  x + I(x^2) )\n\n\n\n\n\n\n\n24.2.2 Non-parametric fits\nNon-parametric fit applies to the family of fitting strategies that do not impose a structure on the data. Instead, they are designed to let the dataset reveal its inherent structure. One explored in this course is the loess fit.\n\n24.2.2.1 Loess\nA flexible curve fitting option is the loess curve (short for local regression; also known as the local weighted regression). Unlike the parametric approach to fitting a curve, the loess does not impose a structure on the data. The loess curve fits small segments of a regression lines across the range of x-values, then links the mid-points of these regression lines to generate the smooth curve. The range of x-values that contribute to each localized regression lines is defined by the \\(\\alpha\\) parameter which usually ranges from 0.2 to 1. The larger the \\(\\alpha\\) value, the smoother the curve. The other parameter that defines a loess curve is \\(\\lambda\\): it defines the polynomial order of the localized regression line. This is usually set to 1 (though ggplot2’s implementation of the loess defaults to a 2nd order polynomial).\n\n\n24.2.2.2 How a loess is constructed\nBehind the scenes, each point (xi,yi) that defines the loess curve is constructed as follows:\n\nA subset of data points closest to point xi are identified (xi i shown as the vertical dashed line in the figures below). The number of points in the subset is computed by multiplying the bandwidth \\(\\alpha\\) by the total number of observations. In our current example, \\(\\alpha\\) is set to 0.5. The number of points defining the subset is thus 0.5 * 14 = 7. The points are identified in the light blue area of the plot in panel (a) of the figure below.\nThe points in the subset are assigned weights. Greater weight is assigned to points closest to xi and vice versa. The weights define the points’ influence on the fitted line. Different weighting techniques can be implemented in a loess with the gaussian weight being the most common. Another weighting strategy we will also explore later in this course is the symmetric weight.\nA regression line is fit to the subset of points. Points with smaller weights will have less leverage on the fitted line than points with larger weights. The fitted line can be either a first order polynomial fit or a second order polynomial fit.\nNext, the value yi from the regression line is computed. This is shown as the red dot in panel (d). This is one of the points that will define the shape of the loess.\n\n\n\n\n\n\nThe above steps are repeated for as many xi values practically possible. Note that when xi approaches an upper or lower x limit, the subset of points becomes skewed to one side of xi. For example, when estimating x10, the seven closest points to the right of x10 are selected. Likewise, for the upper bound x140, the seven closest points to the left of x140 are selected.\n\n\n\n\n\nIn the following example, just under 30 loess points are computed at equal intervals. This defines the shape of the loess.\n\n\n\n\n\nIt’s more conventional to plot the line segments than it is to plot the points.\n\n\n\n\n\n\n\n\n24.2.2.3 Plotting a loess in R\nThe loess fit can be computed in R using the loess() function. It takes as arguments span (\\(\\alpha\\)), and degree (\\(\\lambda\\)).\n\n# Fit loess function\nlo &lt;- loess(cp.ratio ~ area, df, span = 0.5, degree = 1)\n\n# Predict loess values for a range of x-values\nlo.x &lt;- seq(min(df$area), max(df$area), length.out = 50)\nlo.y &lt;- predict(lo, lo.x)\n\nThe modeled loess curve can be added to the scatter plot using the lines function.\n\nplot(cp.ratio ~ area, dat = df)\nlines(lo.x, lo.y, col = \"red\")\n\n\n\n\nIn ggplot2 simply pass the method=\"loess\" parameter to the stat_smooth function.\n\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n             stat_smooth(method = \"loess\", se = FALSE, span = 0.5)\n\nHowever, ggplot defaults to a second degree loess (i.e. the small regression line elements that define the loess are modeled using a 2nd order polynomial and not a 1st order polynomial). If a first order polynomial (degree=1) is desired, you need to include an argument list in the form of method.args=list(degree=1) to the stat_smooth function.\n\nggplot(df, aes(x = area, y = cp.ratio)) + geom_point() + \n             stat_smooth(method = \"loess\", se = FALSE, span = 0.5, \n                         method.args = list(degree = 1) )"
  },
  {
    "objectID": "bivariate.html#residuals",
    "href": "bivariate.html#residuals",
    "title": "24  Fits and residuals",
    "section": "24.3 Residuals",
    "text": "24.3 Residuals\nFitting the data with a line is just the first step in EDA. Your next step should be to explore the residuals. The residuals are the distances (parallel to the y-axis) between the observed points and the fitted line. The closer the points are to the line (i.e. the smaller the residuals) the better the fit.\nThe residuals can be computed using the residuals() function. It takes as argument the model object. For example, to extract the residuals from the linear model M computed earlier type,\n\nresiduals(M)\n\n         1          2          3          4          5          6          7          8          9         10 \n 1.3596154  1.3146525  0.5267426 -0.1133131 -0.1421694  0.3998247 -1.2188692 -2.0509148 -1.8622085 -0.1704418 \n        11         12         13         14 \n-0.5788861  0.4484394 -1.0727624  3.1602905 \n\n\n\n24.3.1 Residual-dependence plot\nOne way to visualize the residuals is to create a residual-dependence plot–this plots the residuals as a function of the x-values. We’ll do this using ggplot so that we can also fit a loess curve to help discern any pattern in the residuals.\n\ndf$residuals &lt;- residuals(M)\nggplot(df, aes(x = area, y = residuals)) + geom_point() +\n             stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                         method.args = list(degree = 1) )\n\n\n\n\nWe are interested in identifying any pattern in the residuals. If the model does a good job in fitting the data, the points should be uniformly distributed across the plot and the loess fit should approximate a horizontal line. With the linear model M, we observe a convex pattern in the residuals suggesting that the linear model is not a good fit. We say that the residuals show dependence on the x values.\nNext, we’ll look at the residuals from the second order polynomial model M2.\n\ndf$residuals2 &lt;- residuals(M2)\nggplot(df, aes(x = area, y = residuals2)) + geom_point() +\n               stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                           method.args = list(degree = 1) )\n\n\n\n\nThere is no indication of dependency between the residual and the area values. The second order polynomial is a large improvement over the first order polynomial. Let’s look at the loess model.\n\ndf$residuals3 &lt;- residuals(lo)\nggplot(df, aes(x = area, y = residuals3)) + geom_point() +\n               stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                           method.args = list(degree = 1) )\n\n\n\n\nNo surprise here. The loess model does a good job in capturing any overall pattern in the data. But note that this is to be expected given that the loess fit is a non-parametric model–it lets the data define its shape!\n\nYou may ask “if the loess model does such a good job in fitting the data, why bother with polynomial fits?” If you are seeking to generate a predictive model that explains the relationship between the y and x variables, then a mathematically tractable model (like a polynomial model) should be sought. If the interest is simply in identifying a pattern in the data, then a loess fit is a good choice.\n\nA model is deemed “well defined” when its residuals are constant over the full range of \\(x\\). In essence, we expect a formula of the form:\n\\[\nY = a + bx + \\epsilon\n\\] where \\(\\epsilon\\) is a constant that does not vary as a function of varying \\(x\\). This should sound quite familiar to you given that we’ve spent a good part of the univariate analysis section seeking a homogeneous spread in the data (i.e. a spread that did not change as a function of fitted group values). So what other diagnostic plots can we (and should we) generate from the fitted model? We explore such plots next.\n\n\n24.3.2 Spread-location plot\nThe M2 and lo models do a good job in eliminating any dependence between residual and x-value. Next, we will check that the residuals do not show a dependence with fitted y-values. This is analogous to univariate analysis where we checked if residuals increased or decreased with increasing medians across factors. Here we will compare residuals to the fitted cp.ratio values (think of the fitted line as representing a level across different segments along the x-axis). We’ll generate a spread-level plot of model M2’s residuals (note that in the realm of regression analysis, such plot is often referred to as a scale-location plot). We’ll also add a loess curve to help visualize any patterns in the plot.\n\nsl2 &lt;- data.frame( std.res = sqrt(abs(residuals(M2))), \n                   fit     = predict(M2))\n\nggplot(sl2, aes(x = fit, y  =std.res)) + geom_point() +\n              stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                          method.args = list(degree = 1) )\n\n\n\n\nThe function predict() extracts the y-values from the fitted model M2 and is plotted along the x-axis. It’s clear from this plot that the residuals are not homogeneous; they increase as a function of increasing fitted CP ratio. The “bend” observed in the loess curve is most likely due to a single point at the far (right) end of the fitted range. Given that we have a small batch of numbers, a loess can be easily influenced by an outlier. We may want to increase the loess span.\n\nggplot(sl2, aes(x = fit, y = std.res)) + geom_point() +\n              stat_smooth(method = \"loess\", se = FALSE, span = 1.5, \n                          method.args = list(degree = 1) )\n\n\n\n\nThe point’s influence is reduced enough to convince us that the observed monotonic increase is real (Note that we would observe this monotone spread with our loess model as well).\nWe learned with the univariate data that re-expressing values was one way to correct for any increasing or decreasing spreads across the range of fitted values. At this point, we may want to look into re-expressing the data. But, before we do, we’ll explore another diagnostic plot: the normal q-q plot.\n\n\n24.3.3 Checking residuals for normality\nIf you are interested in conducting a hypothesis test (i.e. addressing the question “is the slope significantly different from 0”) you will likely want to check the residuals for normality since this is an assumption made when computing a confidence interval and a p-value.\nIn ggplot, we learned that a normal q-q plot could be generated using the stat_qq and stat_qq_line function.\n\nggplot(df, aes(sample = residuals2)) + \n  stat_qq(distribution = qnorm) +\n  stat_qq_line(distribution = qnorm, col = \"blue\")\n\n\n\n\nHere, the residuals seem to stray a little from a normal distribution."
  },
  {
    "objectID": "bivariate.html#re-expressing-the-data",
    "href": "bivariate.html#re-expressing-the-data",
    "title": "24  Fits and residuals",
    "section": "24.4 Re-expressing the data",
    "text": "24.4 Re-expressing the data\nThe monotone spread can be problematic if we are to characterize the spread of cp.ratio as being the same (homogeneous) across the full range of area values. To remedy this, we can re-express the cp.ratio values. Variables measured as ratios (such is the case with cp.ratio) are good candidates for log transformation. We will therefore fit a new linear model to the data after transforming the y-value.\n\ndf.log &lt;- data.frame( area = df$area, cp.ratio.log = log(df$cp.ratio))\nM3     &lt;- lm(cp.ratio.log ~ area, dat = df.log)\n\nTransforming a variable, whether \\(x\\) or \\(y\\), will change the nature of the relationship between both variables. We therefore need to recreate the scatter plot. We’ll also see how well a 1st order polynomial can characterize this relationship (re-expressing values can sometimes help “straighten” a relationship between variables, hence why we are not starting off first with a 2nd order polynomial fit).\n\nggplot(df.log, aes(x = area, y = cp.ratio.log)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nAt first glance, the log transformation seems to have done a good job at straightening the batch of values. Next, let’s look at the residual-dependence plot.\n\ndf.log$residuals &lt;- residuals(M3)\nggplot(df.log, aes(x = area, y = residuals)) + geom_point() +\n               stat_smooth(method = \"loess\", se = FALSE, span = 1, \n                           method.args = list(degree = 1) )\n\n\n\n\nLogging the y values has eliminated the residual’s dependence on area. Next, let’s assess homogeneity in the residuals using the s-l plot.\n\nsl3 &lt;- data.frame( std.res = sqrt(abs(residuals(M3))), \n                   fit     = predict(M3))\nggplot(sl3, aes(x = fit, y = std.res)) + geom_point() +\n               stat_smooth(method =\"loess\", se = FALSE, span = 1, \n                           method.args=list(degree = 1) )\n\n\n\n\nEven though we observe a mid range peak in spread, we do not observe a systematic increase in spread. The log transformation seems to have removed the monotone spread.\nFinally, we’ll check for normality of the residuals.\n\nggplot(df.log, aes(sample = residuals)) + \n  geom_qq(distribution = qnorm) +\n  geom_qq_line(distribution = qnorm, col = \"blue\")\n\n\n\n\nRe-expressing the data seems to have improved the normality of the residuals somewhat. Though not perfect, more points seem to follow a straight line. But, recall from chapter 18.4 that noise can be quite prominent in a normal q-q plot when working with small datasets."
  },
  {
    "objectID": "resistant_lines.html#introduction",
    "href": "resistant_lines.html#introduction",
    "title": "25  Resistant lines",
    "section": "25.1 Introduction",
    "text": "25.1 Introduction\nOrdinary least squares regression lines (those created with the lm() function) suffer from sensitivity to outliers. Because lm’s best fit line makes use of the mean (which is not a robust measure of location), its breakdown point is \\(1/n\\), meaning that all it takes is for one data point to behave differently from the rest of the points to significantly alter the slope and intercept of the best fit line. For example, let’s start with a well behaved dataset where all points are perfectly aligned, and fit this batch with a regression line:\n\nx &lt;- seq(1:11)\ny &lt;- 5 + 2 * x\nplot(y ~ x, pch = 20)\nM &lt;- lm(y ~ x)\nabline( M , col = \"red\")\n\n\n\n\nAs expected, we have a perfect fit. And the regression model’s coefficients match those used to create the data.\n\ncoef(M)\n\n(Intercept)           x \n          5           2 \n\n\nNow, what if one of the points is re-positioned in the plot, what happens to the regression line?\n\ny[11] &lt;- 2\nplot(y ~ x, pch = 20)\nM &lt;- lm(y ~ x)\nabline( M , col = \"red\")\n\n\n\n\nNote the significant change in the line’s characteristics, its intercept and slope are now:\n\ncoef(M)\n\n(Intercept)           x \n  9.5454545   0.8636364 \n\n\nThe slope dropped from 2 to 0.86 because of a single point!\nIf our goal is to explore what the bulk of the data has to say about the phenomena being investigated, we certainly don’t want a few “maverick” values to hijack the analysis. We therefore need a set of fitting tools that minimize the influence of outliers. There are many options out there; most notable are Tukey’s resistant line and the bisquare robust estimation method."
  },
  {
    "objectID": "resistant_lines.html#robust-lines",
    "href": "resistant_lines.html#robust-lines",
    "title": "25  Resistant lines",
    "section": "25.2 Robust lines",
    "text": "25.2 Robust lines\n  \n\n25.2.1 Tukey’s resistant line\nThe idea is simple in concept. It involves dividing the dataset into three approximately equal groups of values and summarizing these groups by computing their respective medians. The batches’ median values are then used to compute the slope and intercept. The motivation behind this plot is to use the three-point summary to provide a robust assessment of the type of relationship between both variables.\nLet’s look at an example of its implementation. We’ll continue with the example used in the previous section. First, we divide the plot into three approximately equal batches.\n\n\n\n\n\nNext, we compute the median x and y values within each section.\n\n\n\n\n\nThe x median values are 2.5, 6, 9.5 and the y median values are 10, 17, 22 respectively.\nThe two end medians are used to compute the slope as:\n\\[\nb = \\frac{y_r - y_l}{x_r-x_l}\n\\] where the subscripts \\(r\\) and \\(l\\) reference the median values for the right-most and left-most batches.\nOnce the slope is computed, the intercept can be computed as follows:\n\\[\nmedian(y_{l,m,r} - b * x_{l,m,r})\n\\]\nwhere \\((x,y)_{l,m,r}\\) are the median x and y values for each batch.\n\n\n\n\n\nThis is not bad for a first iteration. But the outlier’s influence is still present given that the line is not passing through all nine points. Recall that the goal is to not allow one lone observation from influencing a fitted line.\nFrom this first iteration, we can extract the residuals. A line is then fitted to the residuals following the same procedure outlined above.\n\n\n\n\n\nThe initial model intercept and slope are 6.429 and 1.714 respectively and the residual’s slope and intercept are -1.224 and 0.245 respectively. The residual slope is then added to the first computed slope giving us an updated slope of 1.959 and the process is again repeated thus generating the following tweaked slope and updated residuals:\n\n\n\n\n\nThe second set of residuals is added to the previously computed slope giving us an updated slope of 1.994. The iteration continues until the slope residuals stabilize. Each successive iteration is shown in the following plot with the final line in dark red. This took three iterations giving us the final fitted line whose intercept and slope are 5.03 and 1.99 respectively–very close to the actual relationship between the two variables.\n\n\n\n\n\nTukey’s resistant line can be computed using the R built-in line() function. You specify the maximum number of iterations via the iter argument. Usually, five iterations will be enough. But, the iteration will stop before the maximum number of iterations is reached if the residual slope stabilizes before then.\n\nr.fit &lt;- line(x,y, iter = 5)\n\nYou can add the resistant line to a base plot as follows:\n\nplot(y ~ x, df)\nabline(r.fit, col = \"red\")\n\n\n\n\nYou can also add the line to a ggplot:\n\nlibrary(ggplot2)\nggplot(df, aes(x,y)) + geom_point() +\n  geom_abline(intercept = r.fit$coefficients[1], \n              slope = r.fit$coefficients[2],\n              col = \"red\")\n\n\n\n\n\n\n25.2.2 Bisquare\nA bisquare fitting strategy makes use of a non-parametric model that assigns weights to observations based on their proximity to the center of the distribution. The closer an observation is to that center, the greater its weight.\nThe first step is to run a linear regression model on the data. The next step is to extract its residuals. Next, a weighting scheme is fitted to the residuals such that the points associated with extreme residuals are assigned the smallest weight and the points associated with the smaller residual values are assigned the largest weight. The regression analysis is then re-run using those same weights thus minimizing the influence of the rogue points. This process is repeated several times until the residuals stabilize. The following figure shows three iterations of the bisquare function whereby the weights (shown as grey text next to each point) start off as 1 then are modified following the residuals derived from the most recent regression model.\n\n\n\n\n\nThere are different weighing strategies that can be implemented. One such implementation is presented next.\n\n# Create the bisquare function\nwt.bisquare &lt;- function(u, c = 6) {\n   ifelse( abs(u/c) &lt; 1, (1-(u/c)^2)^2, 0)\n}\n\n# Assign an equal weight to all points\nwt &lt;- rep(1, length(x))\n\n# Compute the regression, then assign weights based on residual values\nfor(i in 1:10){\n  dat.lm &lt;- lm(y ~ x ,weights=wt)\n  wt &lt;- wt.bisquare( dat.lm$res/ median(abs(dat.lm$res)), c = 6 )\n}\n\n# Plot the data and the resulting line\nplot(x, y, pch = 20)\nabline(dat.lm, col = rgb(1,0,0,0.3))\n\n\n\n\nIn the above example, the bisquare method does a great job in eliminating the outlier’s influence.\n\n25.2.2.1 Built-in implementation of the bisquare\nThe MASS package has a robust linear modeling function called rlm that will implement a variation of the aforementioned bisquare estimation technique. Its results may differ slightly from those presented above, but the difference will be insignificant for the most part.\nNote that if you make use of dplyr in a workflow, loading MASS after dplyr will mask dplyr’s select function. This can be problematic. So you either want to load MASS before dplyr, or you can call the function via MASS::rlm. An example of its use follows.\n\ndat &lt;- data.frame(x,y)\nM   &lt;- lm( y ~ x, dat=dat)\nMr  &lt;- MASS::rlm( y ~ x, dat=dat, psi=\"psi.bisquare\")\nplot(y ~ x,dat, pch=16, col=rgb(0,0,0,0.2))\n\n# Add the robust line\nabline( Mr, col=\"red\")\n\n# Add the default regression line for reference\nabline( M , col=\"grey50\", lty=2)\n\n\n\n\nThe default regression model is added as a dashed line for reference.\nThe function rlm can also be called directly from within ggplot.\n\nlibrary(ggplot2)\nggplot(dat, aes(x=x, y=y)) + geom_point() + \n       stat_smooth(method = MASS::rlm, se = FALSE, col = \"red\",\n                   method.args = list(psi = \"psi.bisquare\"))"
  },
  {
    "objectID": "resistant_lines.html#robust-loess",
    "href": "resistant_lines.html#robust-loess",
    "title": "25  Resistant lines",
    "section": "25.3 Robust loess",
    "text": "25.3 Robust loess\nThe loess fit is also not immune to outliers. The bisquare estimation method can be extended to the loess smoothing function by passing the \"symmetric\" option to the family argument.\n\n# Fit a regular loess model\nlo &lt;- loess(y ~ x, dat)\n\n# Fit a robust loess model\nlor &lt;- loess(y ~ x, dat, family = \"symmetric\")\n\n# Plot the data\nplot(y ~ x, dat, pch = 16, col = rgb(0,0,0,0.2))\n\n# Add the default loess\nlines(dat$x, predict(lo), col = \"grey50\", lty = 2)\n\n# Add the robust loess\nlines(dat$x, predict(lor), col = \"red\")\n\n\n\n\nThe robust loess is in red, the default loess fit is added as a dashed line for reference.\nThe robust loess can be implemented in ggplot2’s stat_smooth function as follows:\n\nlibrary(ggplot2)\nggplot(dat, aes(x=x, y=y)) + geom_point() + \n          stat_smooth(method = \"loess\", \n                      method.args = list(family=\"symmetric\"), \n                      se=FALSE,  col=\"red\") \n\n\n\n\nThe loess parameters are passed to the loess function via the call to method.args. The red curve is that of the robust loess and the grey dashed curve is that of the standard loess."
  },
  {
    "objectID": "example.html#an-eda-example-co2-analysis",
    "href": "example.html#an-eda-example-co2-analysis",
    "title": "26  The third R of EDA: Residuals",
    "section": "26.1 An EDA example: CO2 analysis",
    "text": "26.1 An EDA example: CO2 analysis\nLet’s start off by plotting the atmospheric CO2 concentrations (in ppm) over time. The original dataset was pulled from NOAA’s website.\n\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndat &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/co2_2020.csv\")\n\ndat2 &lt;- dat %&gt;%\n      mutate( Date = ymd(paste(Year,\"/\", Month,\"/15\")),\n              Year = decimal_date(Date),\n              CO2 = Average)    %&gt;%\n     select(Year, Date, CO2)\n\nNext, let’s look at the data:\n\nggplot(dat2, aes(x = Year , y = CO2)) + geom_point(size = 0.6)\n\n\n\n\nLet’s see if plotting the data using lines instead of points helps identify any underlying patterns.\n\nggplot(dat2, aes(x = Year , y = CO2)) + geom_line()\n\n\n\n\nWe note two patterns of interest: an overall upward trend, and a cyclical trend. We will first smooth out the overall trend by finding a model that best explains the overall variability, then we’ll remove that trend from the data to explore the cyclical component."
  },
  {
    "objectID": "example.html#exploring-the-overall-trend",
    "href": "example.html#exploring-the-overall-trend",
    "title": "26  The third R of EDA: Residuals",
    "section": "26.2 Exploring the overall trend",
    "text": "26.2 Exploring the overall trend\nWe can attempt to model the overall trend by fitting a straight line to the data using a 1st order polynomial. The fitted line is displayed in red in the following plot. We will use ggplot2’s built-in smoothing function to plot the regression line.\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n             stat_smooth(method = \"lm\", se = FALSE, col = \"red\") \n\n\n\n\nNext, we subtract the modeled line from the CO2 data and plot the result–this gives us the residuals and can be thought of as representing what the linear model does not explain about the data. But first, we will need to create a regression model object since it will provide us with the residuals needed to generate the residual plot.\n\n# Generate a model object from the regression fit\nM1 &lt;- lm(CO2 ~ Year, dat2) \ndat2$res1 &lt;- M1$residuals\n\n# Plot the residuals from the model object\nggplot(dat2, aes(x = Year, y = res1)) + geom_point()\n\n\n\n\nAn overall trend is still present, despite having attempted to control for it. This suggests that our simple linear model does not do a good job in capturing the shape of the overall trend.\nIt appears that the overall trend is slightly convex and has a small peak around the 1990’s. This suggests that we should move on to a 2nd order polynomial of the form:\n\\[\nCO2_{trend} = a + b(Year) + c(Year^2)\n\\]\nThe fitted line looks like this:\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n           stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), \n                       se = FALSE, col = \"red\")\n\n\n\n\nNow, let’s look at the residuals. In helping discern any pattern in our residuals, we add a loess smooth to the plot.\n\nM2 &lt;- lm(CO2 ~ Year + I(Year^2), dat2)\ndat2$res2 &lt;- M2$residuals\n\nggplot(dat2 , aes(x = Year, y = res2)) + geom_point() +\n           stat_smooth(method = \"loess\", se = FALSE)\n\n\n\n\nThis is an improvement over the simple line model (which was a 1st order polynomial fit). So what we have learned so far is that the overall trend is not perfectly linear but instead follows a parabolic like trajectory with a small hump halfway across the time span. However, we can still make out a “W” shaped trend in the residuals which can hamper our analysis of the oscillatory patterns in the data. We can try a 3rd order polynomial and see if it can capture the 1980 to 1995 hump.\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n           stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3), \n                       se = FALSE, col = \"red\")\n\n\n\n\nNow, let’s look at the residuals.\n\nM2 &lt;- lm(CO2 ~ Year + I(Year^2) + I(Year^3), dat2)\ndat2$res2 &lt;- M2$residuals\n\nggplot(dat2 , aes(x = Year, y = res2)) + geom_point() +\n           stat_smooth(method = \"loess\", se = FALSE)\n\n\n\n\nThis does not seem to be an improvement over the 2nd order polynomial. At this point, we could play with different order polynomials in an attempt to smooth the trend or, we may opt for a non-parametric fit. When the goal is to peel off one pattern to explore any underlying pattern we should not limit ourselves to parametric fits (which impose a mathematical model on our data) and instead explore non-parametric smoothing techniques that do not impose any structure on the data whatsoever. We’ll make use of the loess fit which is shown in the following figure.\n\nggplot(dat2, aes(x = Year, y = CO2)) + geom_line() + \n             stat_smooth(method = \"loess\", span = 1/4, se = FALSE, col = \"red\")\n\n\n\n\nAt first glance, this may not look any different from our 2nd or 3rd order polynomial model. But the resulting residuals suggest that the loess smooth did a better job in removing any decadal patterns in our batch of CO2 values.\n\nlo &lt;- loess(CO2 ~ Year, dat2, span=1/4)\ndat2$res3 &lt;- lo$residuals\n\nggplot(dat2,  aes(x = Year, y = res3)) + geom_point() +\n           stat_smooth(method = \"loess\", se = FALSE, span = 0.2)"
  },
  {
    "objectID": "example.html#exploring-the-seasonal-component",
    "href": "example.html#exploring-the-seasonal-component",
    "title": "26  The third R of EDA: Residuals",
    "section": "26.3 Exploring the seasonal component",
    "text": "26.3 Exploring the seasonal component\nLet’s now explore the cyclical pattern in the residuals. Note the residuals’ y-axis values: they are three orders of magnitude less than the overall range of CO2 values. This indicates that the oscillating component of the data is relatively small (the CO2 values have a range of [313.33, 417.31] ppm whereas the residuals have a range of [-4.34, 4.34] ppm).\nNow, we may be tempted to fit a smooth to the residuals but that may not prove to be fruitful. Instead, let’s see if the oscillation follows a 12 month cycle. We’ll group all the residual values by month of the year. In other words, we will remove the year tag associated with each value and explore those values as a function of month alone. Each month’s batch of values is distilled into a boxplot.\n\n# Aggregate residuals by year\ndat2$Month &lt;- month(dat2$Date, label=TRUE) \n\nggplot(dat2, aes(x = Month, y = res3)) + geom_boxplot()         \n\n\n\n\nIt’s clear from the plot that the oscillation follows a yearly cycle: a peak in the spring and a dip in the fall. This cycle is explained in part by the increased land mass in the northern hemisphere relative to the southern hemisphere. Because plants (and by extension photosynthesis) go dormant during the winter months in the northern hemisphere, atmospheric CO2 is no longer being photosynthesized; this despite the southern hemisphere’s photosynthesis peak during the October-March period (a result of the southern hemisphere’s smaller land mass). Other factors such as increased CO2 emissions during the winter months may also contribute to the oscillatory nature of atmospheric CO2 concentrations.\nThus far, we have uncovered three patterns of interest: an overall trend, a “hump” around the 1980-1995 time period, and a cyclical component. Note that to effectively explore the cyclical pattern we had to de-trend (or smooth) the data. Next, we should smooth the seasonal component of the data to see if another pattern emerges. We may, for example, smooth the data by subtracting the monthly median from each residual batch leaving us with the next batch of residual values to explore:\n\nd3 &lt;- dat2 %&gt;% \n  group_by(month(Date)) %&gt;%\n  mutate(Resid.lev = res3 - median(res3)) \n\nggplot(d3, aes(x = Month, y = Resid.lev)) + geom_boxplot()     \n\n\n\n\nAll the boxplots are lined up along their median values. We are now exploring the data after having accounted for both overall trend and seasonality. What can be gleaned from this dataset? We may want to explore the skewed nature of the residuals in March, or the narrower range of CO2 values for the fall months, for example.\nWe may also be interested in seeing if any trend in CO2 concentrations exists within each month. Let’s explore this by fitting a first order polynomial to the monthly data.\n\nggplot(d3, aes(x = Year, y = Resid.lev)) + geom_point(col = \"grey\", cex = .6)  + \n     stat_smooth(se = FALSE, method = \"lm\") + facet_wrap(~ Month, nrow = 1) +\n     theme(axis.text.x=element_text(angle=45, hjust = 1.5, vjust = 1.6,  size = 7))\n\n\n\n\nNote the overall increase in CO2 for the winter and spring months followed by a similar (but not as persistent) decrease in CO2 loss in the summer and fall months. This suggests either an increase in cyclical amplitude with the deviation from a central value being possibly greater for the winter/spring months. It could also suggest a gradual offset in the periodicity over the 60 year period. More specifically, a leftward shift in periodicity.\nLet’s fit a loess to see if the trends within each month are monotonic. We’ll adopt a robust loess (family = \"symmetric\") to control for possible outliers in the data.\n\nggplot(d3, aes(x = Year, y = Resid.lev)) + geom_point(col = \"grey\", cex = .6)  + \n     stat_smooth(method = \"loess\", se = FALSE, \n                 method.args = list(family = \"symmetric\")) + \n     facet_wrap(~ Month, nrow = 1) +\n     theme(axis.text.x=element_text(angle=45, hjust = 1.5, vjust = 1.6,  size = 7))\n\n\n\n\nThe trends are clearly not monotonic suggesting that the processes driving the cyclical concentrations of CO2 are not systematic. However, despite the curved nature of the fits, the overall trends observed with the straight lines still remain."
  },
  {
    "objectID": "example.html#exploring-what-remains",
    "href": "example.html#exploring-what-remains",
    "title": "26  The third R of EDA: Residuals",
    "section": "26.4 Exploring what remains",
    "text": "26.4 Exploring what remains\nNow that we’ve fitted the monthly medians to the seasonal component of the data, let’s plot the residuals across the years. We’ll make use of the robust version of the loess given that the data may be quite noisy.\n\nggplot(d3, aes(x = Year, y = Resid.lev)) +  geom_line(col = \"grey\") + \n     stat_smooth(method = \"loess\", se = FALSE, span = 0.1 ,\n                 method.args = list(family = \"symmetric\", degree = 2))    \n\n\n\n\nRecall that at this points, we have accounted for the overall trend (which has by far the greatest effect with a range of over 90 ppm) and the seasonal component (which has a smaller effect with a range of about 6 ppm). The 3rd set of residuals account for about 2 ppm of the total variability in the dataset. Note that we can still make out the seasonal fluctuations if we look closely at the data. But we can also make out broader patterns of interest not consistent with random noise and not explained by seasonal variability. Some of the patterns could be explained by the southern oscillation index (SOI), for example, which is a measure of the difference in atmospheric pressure across the pacific basin. When the index is positive the resulting winds drive upwelling currents that release additional CO2 into the atmosphere.\nWe’ll create a simple overlay the SOI data on top of our last plot, but note that a scatter plot of the two variables with a lagging component would be best suited for this analysis.\n\nlibrary(tidyr)\n\nsoi &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/Southern_oscillation.csv\")\n\nsoi &lt;- gather(soi, key = Month, value = Index, -1)\nDate &lt;- ymd(paste(soi$YEAR, soi$Month, 15) )\nsoi$Yr &lt;- decimal_date(Date)\n\nggplot(d3, aes(x = Year, y = Resid.lev)) +  \n             geom_line(col = \"blue\", alpha = 0.2) + \n             stat_smooth(method = \"loess\", se = FALSE, span = 0.1, lwd = 0.5,\n                         method.args = list(family = \"symmetric\", degree = 2)) +\n             geom_line(dat = soi, aes(Yr, Index/2, colour = \"red\"), alpha = 0.3) +\n             stat_smooth(dat = soi, aes(Yr, Index/2, colour = \"red\"), lwd = 0.5,\n                         method = \"loess\", se = FALSE, span = 0.2,\n                         method.args = list(family = \"symmetric\", degree = 2)) +\n             guides(color = FALSE)\n\n\n\n\nThe blue lines represent the residuals (with the bold blue line showing the loess fit) and the red lines represent the SOI trend (with the bold line showing its loess fit). Note that we halved the SOI values to match the range along the y-scale; the intent is to facilitate the comparison of SOI trend to CO2 trend. There appears to be matching maxima (notably around 1975 and 1988) but we also note some opposing trends such as in the period after 2010. When the index drops below 0, “El Nino” events become more prominent and upwelling flows are suppressed. Though this may prevent CO2 trapped in the deeper Pacific oceans from reaching the surface, “El Nino” events can also have a measurable impact on land climate thus influencing the rate of vegetation growth and, by extension, CO2 uptake. El nino is also believed to increase the rate of forest and brush fires around the world which may also contribute to atmospheric CO2 concentrations.\nIt’s important to realize that this is one of many ways in which we could have proceeded with the analysis. For example, we could have started off the analysis by removing the seasonal component from the data, then analyzed the long term trend."
  },
  {
    "objectID": "discontinuity.html#introduction",
    "href": "discontinuity.html#introduction",
    "title": "27  Slicing data",
    "section": "27.1 Introduction",
    "text": "27.1 Introduction\nLet’s start off by downloading a dataset.\n\ndf &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/Sample1.csv\")\n\nNext, we’ll plot the data and fit a straight line.\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = x,y = y)) + geom_point(alpha = 0.3) +\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nThe line seems to do a decent job in depicting the overall trend but the relationship does not appear perfectly linear. Let’s check the residuals via a residual-dependence plot.\n\nM1    &lt;- lm( y ~ x, df)\ndf$residuals &lt;- residuals(M1)\n\nggplot(df, aes(x = x, y = residuals)) + geom_point(alpha = 0.3) +\n  stat_smooth(method = \"loess\", se = FALSE, span = 0.2, \n              method.args = list(degree = 1))\n\n\n\n\nThere appears to be a dip in residual values between ~95 and ~107 followed by an increase beyond ~107. The kinks in the residuals seem to delineate three perfectly straight line segment suggesting that the raw data may be modeled using three distinct lines (i.e. with differing slopes and intercepts). Note the use of a small loess span in the above code to reveal the kinks in the pattern.\nSometimes, the data may represent outcomes from different processes given different ranges in independent (x) values. Since the residual plot seems to suggest that the kinks occur are x=95 and x=106, we’ll split the data into three groups: less than 95, between 95 and 106 and greater than 106. These groups will be labeled 1, 2, and 3 and will be assigned to a new column called grp. This new column will then be used to facet the scatter plots and associated loess curve. We’ll use of the case_when() function to perform this task.\n\nlibrary(dplyr)\ndf2 &lt;- df %&gt;% mutate(grp = case_when(x &lt; 95 ~ 1,\n                                     x &lt; 106 ~ 2,\n                                     x &gt;= 106 ~ 3))\n\nggplot(df2, aes(x = x,y = y)) + geom_point(size = 0.5, alpha = 0.3) +\n  stat_smooth(method = \"loess\", se = FALSE) + facet_grid(. ~ grp)\n\n\n\n\nThe segmented plots seem to confirm our earlier suspicion that the data followed three separate linear processes. We can extract the slope and intercept for each segment using the following chunk of code:\n\nlibrary(tidyr)\ndf2  %&gt;% \n  group_by(grp) %&gt;% \n  do( M1 = (lm(y ~ x, . ) ) )  %&gt;% \n  mutate(intercept = coef(M1)[1],\n         slope = coef(M1)[2]) %&gt;% \n  select(-M1)\n\n# A tibble: 3 × 3\n# Rowwise: \n    grp intercept slope\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1      1.41  2.09\n2     2    103.    1.04\n3     3    -53.4   2.50\n\n\nSo, three separate models were needed to capture, what we believed to be, three different underlying processes.\nExploring the residuals using the loess fit was pivotal in identifying the breaks in the pattern. But, as we will learn in the next section, these breaks may not always be so obvious in a dataset."
  },
  {
    "objectID": "discontinuity.html#example",
    "href": "discontinuity.html#example",
    "title": "27  Slicing data",
    "section": "27.2 Example",
    "text": "27.2 Example\n\nDisclaimer: the analysis presented here is only exploratory and does not mirror the complete analysis conducted by Vincent et al. nor the one conducted by Stone.\n\n\n27.2.1 Original analysis\nThe following data are pulled from the paper titled “Observed Trends in Indices of Daily Temperature Extremes in South America 1960-2000” (Vincent et al., 2005) and represent the percentage of nights with temperatures greater than or colder than the 90th and 10th percentiles respectively within each year. The percentiles are calculated for the 1961 to 2000 period.\n\nlibrary(tidyr)\n\nYear &lt;- 1960:2000\nPerC &lt;- c(11.69,9.33,14.35,10.73,14.15,11.16,13,12.13,14.25,10.01,11.94,14.35,\n          10.83,9.38,11.5,10.44,12.66,7.55,9.77,9.81,8.9,8.51,7.02,6.83,9.67,\n          7.84,7.11,8.56,10.59,7.93,8.85,8.8,8.75,8.18,7.16,9.91,10.15,6.58,\n          6.44,9.43,8.03)\nPerH &lt;- c(8.62,10.1,6.67,11.13,5.71,9.48,7.63,8.12,7.2,9.64,8.42,5.71,11.72,\n          11.32,7.2,7.17,7.46,13.17,9.28,8.75,12.38,10,13.83,17.59,10.14,\n          9.84,11.23,14.39,9.44,8.26,12.15,12.45,13.14,13.67,15.22,11.79,11.16,\n          20.37,17.56,11.13,11.49)\ndf2 &lt;- data.frame(Year, PerC, PerH)\n\ndf2.l &lt;- pivot_longer(df2, names_to = \"Temp\", values_to = \"Percent\", -Year)\n\nLet’s plot the data and fit a straight line to the points.\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) + facet_wrap(~ Temp, nrow = 1)\n\n\n\n\nThe plot on the left shows percent cold nights and the one on the right shows percent hot nights. At first glance, the trends seem real and monotonic.\nNext we’ll fit a loess to see if the trends are indeed monotonic. To minimize the undue influence of end values in the plot, we’ll implement loess’ bisquare estimation method via the family=symmetric option. We’ll also use a small span to help identify any “kinks” in the patterns.\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"loess\", se = FALSE, span = 0.5, \n              method.args = list(degree = 1, family = \"symmetric\"))  + \n  facet_wrap(~ Temp, nrow = 1)\n\n\n\n\nThe patterns seem to be segmented around the 1975-1980 period for both plots suggesting that the observed trends may not be monotonic. In fact, there appears to be a prominent kink in the percent cold data around the mid to late 1970`s. A similar, but not as prominent kink can also be observed in the percent hot data at around the same time period.\n\n\n27.2.2 Changepoint\nIn a comment to Vincent et al.’s paper, R.J. Stone argues that the trend observed in the percent hot and cold dataset is not monotonic but segmented instead. In other words, there is an abrupt change in patterns for both datasets that make it seem as though a monotonic trend exists when in fact the data may follow relatively flat patterns for two different segments of time. He notes that the abrupt change (which he refers to as a changepoint) occurs around the 1976 and 1977 period. He suggests that this time period coincides with a change in the Pacific Decadal Oscillation (PDO) pattern. PDO refers to an ocean/atmospheric weather pattern that spans multiple decades and that is believed to impact global climate.\nThe following chunk of code loads the PDO data, then summarizes the data by year before plotting the resulting dataset.\n\ndf3  &lt;- read.table(\"http://mgimond.github.io/ES218/Data/PDO.dat\", \n                   header = TRUE, na.strings = \"-9999\")\npdo &lt;- df3 %&gt;%\n  pivot_longer(names_to = \"Month\", values_to = \"PDO\", -YEAR) %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(PDO = median(PDO) )\n\nggplot(pdo, aes(x = YEAR, y = PDO)) + geom_line() + \n  stat_smooth(se = FALSE, span = 0.25) +\n  geom_vline(xintercept = c(1960, 1976, 2000), lty = 3)\n\n\n\n\nThe contrast in PDO indexes between the 1960-1976 period and the 1976-2000 period is obvious with the pre-1977 index values appearing to remain relatively flat over a 15 year period and with the post-1977 index appearing to show a gradual increase towards a peak around the early 1990’s.\nTo see if distinct patterns emerge from the percent hot and cold data before and after 1976, we’ll split the data into two segments using a cutoff year of 1976-1977. Values associated with a period prior to 1977 will be assigned a seg value of Before and those associated with a post-1977 period will be assigned a seg value of After.\n\ndf2.l$seg &lt;- ifelse(df2.l$Year &lt; 1977, \"Before\", \"After\")\n\nNext, we’ll plot the data across four facets:\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) + facet_grid(seg ~ Temp)\n\n\n\n\nWe can also choose to map seg to the color aesthetics which will split the points by color with the added benefit of fitting two separate models to each batch.\n\nggplot(df2.l, aes(x = Year, y = Percent, col = seg)) + geom_point() +\n  stat_smooth(method = \"lm\", se=FALSE) + facet_wrap( ~ Temp, nrow = 1)\n\n\n\n\nTo test for “straightness” in the fits, we’ll fit a loess to the points.\n\nggplot(df2.l, aes(x = Year, y = Percent, col = seg)) + geom_point() +\n  stat_smooth(method = \"loess\", se = FALSE, method.args = list(degree = 1)) + \n  facet_wrap( ~ Temp, nrow = 1)\n\n\n\n\nThere is a clear “stair-step” pattern for the percent cold nights. However, there seems to be an upward trend in the percent of hot nights for the post-1977 period which could imply that in addition to the PDO effect, another process could be at play.\nIt should be noted that in a followup to Stone’s comment, Vincent et al. defend their analysis results. However, this little exercise highlights the ease in which an analysis can follow different (and seemingly sound) paths."
  },
  {
    "objectID": "discontinuity.html#reference",
    "href": "discontinuity.html#reference",
    "title": "27  Slicing data",
    "section": "27.3 Reference",
    "text": "27.3 Reference\nOriginal paper: Vincent, L. A., et al., 2005. Observed trends in indices of daily temperature extremes in South America 1960–2000. J. Climate, 18, 5011–5023.\nComment to the paper Stone, R. J., 2011. Comments on “Observed trends in indices of daily temperature extremes in South America 1960–2000.” J. Climate, 24, 2880–2883.\nThe reply to the comment Vincent, L. A., et al., 2011. Reply, J. Climate, 24, 2884-2887."
  },
  {
    "objectID": "two_way.html#introduction",
    "href": "two_way.html#introduction",
    "title": "28  Analyzing two-way tables: The median polish",
    "section": "28.1 Introduction",
    "text": "28.1 Introduction\nBefore tackling two-way tables, let’s explore a simpler one-way table. The following dataset shows mean annual income by occupation for the year 2014 (src: Bureau of Labor Statistics)\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n\n\nArchitecture and Engineering\n\n\n112490\n\n\n\n\nArts, Design, Entertainment, Sports, and Media\n\n\n72410\n\n\n\n\nBuilding and Grounds Cleaning and Maintenance\n\n\n83970\n\n\n\n\nBusiness and Financial Operations\n\n\n81520\n\n\n\n\nCommunity and Social Service\n\n\n70070\n\n\n\n\nComputer and Mathematical\n\n\n45310\n\n\n\n\nConstruction and Extraction\n\n\n101110\n\n\n\n\nEducation, Training, and Library\n\n\n52210\n\n\n\n\nFarming, Fishing, and Forestry\n\n\n55790\n\n\n\n\nFood Preparation and Serving Related\n\n\n76010\n\n\n\n\nHealthcare Practitioners and Technical\n\n\n28820\n\n\n\n\nHealthcare Support\n\n\n43980\n\n\n\n\nInstallation, Maintenance, and Repair\n\n\n21980\n\n\n\n\nLegal\n\n\n26370\n\n\n\n\nLife, Physical, and Social Science\n\n\n24980\n\n\n\n\nManagement\n\n\n38660\n\n\n\n\nOffice and Administrative Support\n\n\n35530\n\n\n\n\nPersonal Care and Service\n\n\n25160\n\n\n\n\nProduction\n\n\n46600\n\n\n\n\nProtective Service\n\n\n45220\n\n\n\n\nSales and Related\n\n\n35490\n\n\n\n\nTransportation and Material Moving\n\n\n34460\n\n\n\n\n\nSo, what can we glean from this dataset other than the raw values associated with each occupation type? For starters, we may be interested in coming up with a summary of income values such as the median, or $45265 in our working example. This is a single value that can be used to characterize the overall income value. However, the income values in our batch are not all equal to $45265–they vary above or below this median value. A good way to measure the variability about the median is to subtract the median income value from each occupation income value. We can do this in a table, but will choose a dotplot chart instead to facilitate comparison.\n\n\n\n\n\nWe have separated the income values into two parts: an overall value and the difference between each value and the overall–i.e. the residuals. This can be written in the following algebraic form:\n\\[\nIncome_{Occupation} = Income_{median} + Income_{residual}\n\\]\nFor example, ‘Healthcare Support’ average income of $43980 can be separated into the median, $45265, and its residual, $-1285.\nSo, in addition to gleaning a median income value from this batch of values, we have also devised a way to facilitate the comparison of the income values to one another. Alternatively, we could have assigned colors to each cell of the table reflecting their values relative to the overall median.\n\n\n\n\n\nThe income value can be thought of as the response to the occupation type (also known as a factor). In other words, the factor determines the value in our table. We can re-write the last equation in a more formal way:\n\\[\ny_{i} = \\mu + \\epsilon_i\n\\]\nwhere \\(y_{i}\\) is the response variable (e.g. income) for factor \\(i\\) (e.g. occupation), \\(\\mu\\) is the overall typical value (hereafter referred to as the common value) and is usually the mean or median, and \\(\\epsilon_i\\) is the residual. Next, we will see how the idea of separating the values into components can be expanded to two-way (aka two-factor) tables."
  },
  {
    "objectID": "two_way.html#anatomy-of-a-two-way-table",
    "href": "two_way.html#anatomy-of-a-two-way-table",
    "title": "28  Analyzing two-way tables: The median polish",
    "section": "28.2 Anatomy of a two-way table",
    "text": "28.2 Anatomy of a two-way table\nLet’s decompose the following table which represents infant mortality rates (per 1000 live births) by region and by a father’s educational attainment for the years 1964 through 1966 (Hoaglin et al.).\n\nA two-way table is composed of three variables:\n\na row factor which has four levels in our working example: Northeast, North Central, South and West,\na column factor which has five levels in our working example: under 8, 9-11, 12, 13-15 and 16 and greater years of education,\nresponse variables of which we have 4 (rows) x 5 (columns) = 20 all together.\n\nA two-way table is an extension to the one-way table described in the introduction where instead of grouping a continuous variable by a single factor category we explore the variable across two factor categories.\nWe can represent the relationship between the response variable, \\(y\\), and the two factors as:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\n\\]\nwhere \\(y_{ij}\\) is the response variable for row \\(i\\) and column \\(j\\), \\(\\mu\\) is the overall typical value (hereafter referred to as the common value), \\(\\alpha_i\\) is the row effect, \\(\\beta_j\\) is the column effect and \\(\\epsilon_{ij}\\) is the residual or value left over after all effects are taken into account.\nThe goal of this analysis is to decompose the response variable into its respective effects–i.e. the two contributing factors: education of father and region–via an iterative process where row medians and column medians are removed from the response variables repeatedly until the row and column medians approach zero."
  },
  {
    "objectID": "two_way.html#analysis-workflow",
    "href": "two_way.html#analysis-workflow",
    "title": "28  Analyzing two-way tables: The median polish",
    "section": "28.3 Analysis workflow",
    "text": "28.3 Analysis workflow\nLet’s first create the dataframe.\n\ndf &lt;- data.frame(row.names = c(\"NE\",\"NC\",\"S\",\"W\"),\n                 ed8       = c(25.3,32.1,38.8,25.4), \n                 ed9to11   = c(25.3,29,31,21.1),\n                 ed12      = c(18.2,18.8,19.3,20.3),\n                 ed13to15  = c(18.3,24.3,15.7,24),\n                 ed16      = c(16.3,19,16.8,17.5)\n                 )\n\n\n28.3.1 Visualizing the data\nIt’s often easier to look at a graphical representation of the data than a tabular one. Even a table as small as this can benefit from a plot.\nWe will adopt Bill Cleveland’s dotplot for this purpose. R has a built-in dotplot function called dotchart. It requires that the table be stored as a matrix instead of a dataframe; we will therefore convert df to a matrix by wrapping it with the as.matrix() function.\n\ndotchart( as.matrix(df), cex=0.7)\n\n\n\n\nThe plot helps visualize any differences in mortality rates across different father educational attainment levels. There seems to be a gradual decrease in child mortality with increasing father educational attainment.\nBut the plot does not help spot differences across regions (except for the ed12 group). We can generate another plot where region becomes the main grouping factor. We do this by wrapping the matrix with the transpose function t().\n\ndotchart( t(as.matrix(df)), cex=0.7)\n\n\n\n\nAt first glance, there seems to be higher death rates for the NC and S regions and relatively lower rates for the W and NE regions. But our eyes may be fooled by outliers in the data.\nNext, we’ll generate side-by-side boxplots to compare the effects between both categories. Note that we’ll need to create a long version of the table using the tidyr package.\n\nlibrary(tidyr)\nlibrary(dplyr)\n\ndf.l &lt;- df %&gt;%\n  mutate(Region = as.factor(row.names(.)) )  %&gt;%\n  pivot_longer(names_to = \"Edu\", values_to = \"Deaths\", -Region) %&gt;% \n  mutate(Edu = factor(Edu, levels=names(df)))\n\n# side-by-side plot\nOP &lt;- par(mfrow=c(1,2))\nplot(Deaths ~ Region + Edu, df.l)\npar(OP)\n\n\n\n\nSo at this point the plots suggest that there may be a father’s educational attainment effect as well as a regional effect on infant mortality with the former effect being possibly more important.\nNext we will attempt to quantify these effects using Tukey’s median polish.\n\n\n28.3.2 Median polish\nA median polish is an iterative process whereby we seek to extract the effects from the data into their row and column components. The steps involved in each iteration are described next.\n\n28.3.2.1 Step 1: Compute overall median and residual table\nFirst, we will compute the median value for all values in the dataset; this will be our first estimate of the common value. The resulting median value is placed in the upper left-hand margin of the table. A residual table is created by taking the difference between the original value and the overall median.\n\nYou’ll also note that the row and column effect values are initially populated with 0 values.\n\n\n28.3.2.2 Step 2: Compute the row medians\nNext, row medians are computed (shown in red in the following figure) from the residual table.\n\nNote that in addition to computing the row median values for the response variable, we are also computing the median value for the column effects in the upper margin (which happens to be zero since we have no column effect values yet).\n\n\n28.3.2.3 Step 3: Create a new residual table from the row medians\nThe row medians are added to the left hand margin of the new residual table (shown in green). These values represent the row effects in this first iteration. A new set of residual values is created from the row medians where each cell takes on the value of the subtraction of the row median from each response variable in that row.\n\nThe subtraction is also done for the column effect values (even though all values remain zero) and the global median (common effect value).\n\n\n28.3.2.4 Step 4: Compute the column medians\nNext, column medians are computed (shown in red in the following figure) from the residual table. Note that you are NOT computing the column medians from the original response variables. Note too that we are also computing the median value for the row effect values (left margin).\n\n\n\n28.3.2.5 Step 5: Create a new residual table from the column medians\nA new residual table is created from the column medians where each new cell takes on the value of the subtraction of the column median from each initial residual value in that column. For example, the first upper-left cell’s residual is \\(7.0 - 7.4 = -0.4\\).\nThe column medians are also added to the column effect margin. Note that we are also adding the row effect median value, -0.5, to the common effect cell in the upper left-hand corner.\n\nWe have now completed our first row and column smoothing iteration. However, there may be more row and column effects that can be pulled from the residuals. We therefore move on to a second iteration.\n\n\n28.3.2.6 Step 6: Second iteration – row effects\nNext, we compute the row medians from the residuals, then add the column effect median to the top margin (and the common value) and subtract the row medians from the residuals. as in step 3.\n\n\n\n28.3.2.7 Step 7: Second iteration – column effects\nTo wrap-up the second iteration, we compute the column median values from the residuals then subtract the medians from those residuals. We also add the medians to the row effect values and the common value.\n\n\n\n28.3.2.8 When do we stop iterating?\nThe goal is to iterate through the row and column smoothing operations until the row and column effect medians are close to 0. However, Hoaglin et al. (1983) warn against “using unrestrained iteration” and suggest that a few steps should be more than adequate in most instances. In our working example, a third iteration may be warranted. The complete workflow for this third iteration follows.\n\nThe final version of the table (along with the column and row values) is shown below:\n\nA progression of each iteration is shown side-by-side in the next figure. The left-most table is the original data showing death rates. The second table shows the outcome of the first round of polishing (including the initial overall median value of 20.2). The third and forth table show the second and third iterations of the smoothing operations. Additional iterations are not deemed necessary given that little more can be extracted from the residuals.\n\n\n\n\n\nA few takeway observations from the analysis follow:\n\nThe common factor ends up contributing the largest effect.\nBetween Region and Education effects, the latter explains more of the variability in deaths with a range of 7.43 - (-3.7) = 11.13 deaths per 1000 births. The Region effect explains about 2.3 - (-1.46) = 3.76 deaths per 1000 births of the variability in the data.\nOne abnormally high residual–10.97 deaths above the overall median deaths–is associated with the southern region and an 8th grade education.\n\n\n\n\n28.3.3 Interpreting the median polish\nAs noted earlier, a two-way table represents the relationship between the response variable, \\(y\\), and the two categories as:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}\n\\]\nIn our working example, \\(\\mu\\) = 20.6; \\(\\alpha_i\\) = -1.5, 2.6 -0.4, and 0.4 for \\(i\\) = NE, Nc, S and W respectively; \\(\\beta_j\\) = 7.6, 6.0, -0.9, 0.2 and -3.5 for \\(j\\) = ed8, ed9to11, ed12, ed13to15 and ed16 respectively.\nThe residuals represent the portion of the mortality rates that can’t be explained by either factors.\nSo the mortality rate in the upper left-hand cell from the original table can be deconstructed as:\n\nThe examination of the table suggests that the infant mortality rate is greatest for fathers who did not attain more than 8 years of school (i.e. who has not completed high school) as noted by the high column effect value of 7.6. This is the rate of infant mortality relative to the overall median (i.e. on average, 20.6 infants per thousand die every year and the rate goes up to 7.6 + 20.6 for infants whose father has not passed the 8th grade). Infants whose father has completed more than 16 years of school (i.e. who has completed college) have a lower rate of mortality as indicated by the low effect value of -3.5 (i.e. 3.5 fewer depths than average). The effects from regions also show higher infant mortality rates for North Central and Western regions (with effect values of 2.6 and 0.4 respectively) and lower rates for the northeastern and southern regions; however the regional effect does not appear to be as dominant as that of the father’s educational attainment.\nIt’s also important to look at the distribution of the residual numbers across the two-way table. One should identify unusually high or low residuals in the table. Such residuals may warrant further investigation (e.g. the high southern region residual value of 10.8 may need further exploration).\n\n\n28.3.4 Test for non-additivity\nThus far, we have assumed an additive relationship between the effects (factors). But this additive model may not be the best fit for our data. A good way to test this is by generating a Tukey Additivity Plot where we plot residuals vs. the comparison value, \\(cv_{ij}\\), defined as \\(\\alpha_i \\beta_j / \\mu\\). If the plot is devoid of any obvious trend or pattern we can conclude that our dataset is consistent with an additive model. Such seems to be the case with our working example as shown in the following plot.\n\n\n\n\n\nIf the diagnostic plot revealed a trend, its shape–or more specifically, its slope–could be used in helping define an appropriate transformation for the data. A rule of thumb is to apply a \\((1 – slope)\\) power transformation to the original data. If the resulting power is not appropriate for the dataset then the \\(cv_{ij}\\) can be added to the additive model as follows:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + k(\\alpha_i \\beta_j / \\mu)  + \\epsilon_{ij}\n\\] where the constant \\(k\\) is the slope estimated from the Tukey Additivity plot."
  },
  {
    "objectID": "two_way.html#implementing-the-median-polish-in-r",
    "href": "two_way.html#implementing-the-median-polish-in-r",
    "title": "28  Analyzing two-way tables: The median polish",
    "section": "28.4 Implementing the median polish in R",
    "text": "28.4 Implementing the median polish in R\n\n28.4.1 Using base R’s medpolish\nThe steps outlined in the previous section can be easily implemented using pen and paper or a spreadsheet environment for larger datasets. R has a built-in function called medpolish() which does this for us. We can define the maximum number of iterations by setting the maxiter= parameter but note that medpolish will, by default, automatically estimate the best number of iterations for us.\n\ndf.med &lt;- medpolish( df)\n\n1: 47.1\n2: 42.9\n3: 42.45\nFinal: 42.25\n\n\nThe median polish ran through 4 iterations–one more than was run in our step-by-step example.\nThe four values printed to the console gives us the sum of absolute residuals at each iteration. The output from the model is stored in the df.med object. To see its contents , simply type it at the command line.\n\ndf.med\n\n\nMedian Polish Results (Dataset: \"df\")\n\nOverall: 20.775\n\nRow Effects:\n    NE     NC      S      W \n-1.475  2.375 -0.350  0.350 \n\nColumn Effects:\n     ed8  ed9to11     ed12 ed13to15     ed16 \n  7.4750   5.9250  -1.1125   0.0750  -3.6250 \n\nResiduals:\n      ed8 ed9to11    ed12 ed13to15   ed16\nNE -1.475   0.075  0.0125   -1.075  0.625\nNC  1.475  -0.075 -3.2375    1.075 -0.525\nS  10.900   4.650 -0.0125   -4.800  0.000\nW  -3.200  -5.950  0.2875    2.800  0.000\n\n\nAll three effects are displayed as well as the residuals (note that the precision returned is greater than that used in our earlier analysis).\nTo generate the Tukey additivity plot, simply wrap the median polish object with the plot command as in:\n\nplot( df.med )\n\n\n\n\n\n\n28.4.2 Using eda_pol\nThe tukeyedar package has a custom function called eda_pol that will output the polished table as a graphic element.\nUnlike the base medpolish function, eda_pol requires that the data be in long form. In creating the long form version of the data, we’ll explicitly set the levels for educational attainment and region.\n\ngrd &lt;- c(\"ed8\", \"ed9-11\", \"ed12\", \"ed13-15\", \"ed16\")\nreg &lt;- c(\"NE\", \"NC\", \"S\", \"W\")\ndfl &lt;- data.frame(Region =  factor(rep( c(\"NE\", \"NC\", \"S\", \"W\"), each = 5),\n                                   levels = reg),\n                 Education = factor(rep( grd , 4), levels = grd),\n                 Deaths = c(25.3, 25.3, 18.2, 18.3, 16.3, 32.1, 29, 18.8,\n                          24.3, 19, 38.8, 31, 19.3, 15.7, 16.8, 25.4, \n                          21.1, 20.3, 24, 17.5))\n\nThe eda_pol requires that you specify which variables will be mapped to the row and column effects.\n\nlibrary(tukeyedar)\n\ndf.pol &lt;- eda_pol(dfl, row = Region, col = Education , val = Deaths )\n\n\n\n\n\n\nHere, the eda_pol function required five iterations which explains the slight difference in values from those generated with the medpolish function. You can control the maximum number of iterations in eda_pol with the maxiter argument.\nBy default, the column and row values are listed in the same order as they are presented in the input data frame (either defined alphabetically or defined by their levels). You can have the function sort the values by their effect size using the sort argument.\n\ndf.pol &lt;- eda_pol(dfl, row = Region, col = Education, val = Deaths, sort = TRUE )\n\n\n\n\n\n\nThe df.pol object is a list that stores output values such as the common, row and column effects.\n\ndf.pol$global\n\n[1] 20.85\n\ndf.pol$row\n\n  Region  effect\n1     NE -1.4625\n2     NC  2.3000\n3      S -0.3500\n4      W  0.3500\n\ndf.pol$col\n\n  Education   effect\n1       ed8  7.43125\n2    ed9-11  5.88125\n3      ed12 -1.19375\n4   ed13-15  0.03125\n5      ed16 -3.70000\n\n\nThe eda_pol will also output a diagnostic plot.\n\nplot(df.pol, type = \"diagnostic\")\n\n$slope\n    cv \n1.3688 \n\n\n\n\n\nIn addition to generating the plot, the function will also return the slope of the fitted line in the console.\nFor a comprehensive review of the median polish and the eda_pol function, see the eda_pol vignette."
  },
  {
    "objectID": "two_way.html#what-if-we-use-the-mean-instead-of-the-median",
    "href": "two_way.html#what-if-we-use-the-mean-instead-of-the-median",
    "title": "28  Analyzing two-way tables: The median polish",
    "section": "28.5 What if we use the mean instead of the median?",
    "text": "28.5 What if we use the mean instead of the median?\nThe procedure is similar with some notable differences. First, we compute the global mean instead of the medians (the common effect) then subtract it from all values in the table. Next, we compute the row means (the row effect) then subtract each mean from all values in its associated row. We finally compute the column means (from the residuals) to give us the column effect. That’s it, unlike the median polish, we do not iterate the smoothing sequences. An example of a “mean” polish applied to our data follows:\n\nThe results differ slightly from those produced using the median polish. Recall that the mean is sensitive to outliers and medians are not. If a robust estimate of the effects is sought, stick with the median polish.\nSo what can we gain from the “mean” polish? Well, as it turns out, it serves as the basis of the two-way ANOVA.\n\n28.5.1 Two-way ANOVA\nA two-way ANOVA assesses whether the factors (categories) have a significant effect on the outcome. Its implementation can be conceptualized as a regression analysis where each row and column level is treated as a dummy variable. The computed regression coefficients are the same as the levels computed using the “mean” polish outlined in the previous step except for one notable difference: the ANOVA adds one of the column level values and one of the row level values to the grand mean. It then subtracts those values from their respective row/column effects.\n\nThe resulting row/column levels are the regression coefficients:\n\nM &lt;- lm(Deaths ~ Education + Region, dfl)\ncoef(M)\n\n     (Intercept)  Educationed9-11    Educationed12 Educationed13-15    Educationed16         RegionNC          RegionS \n          28.255           -3.800          -11.250           -9.825          -13.000            3.960            3.640 \n         RegionW \n           0.980 \n\n\nThe regression formula takes on the form:\n\\[\n\\begin{split}\nDeaths = 32.21 - 3.8(ed9to11) -11.25(ed12) -9.83(ed13to15) -13(ed16) \\\\\n         -3.96(RegionNE) -0.320(RegionS)  -2.980(RegionW) + \\epsilon_{ij}\n\\end{split}\n\\]\nSo, for example, to compute the first value in the raw table (death rate = 25.3) from the formula, substitute the variables as follows:\n\\[\n\\begin{split}\nDeaths &= 32.21 - 3.8(0) -11.25(0) -9.83(0) -13(0)&& \\\\\n         &-3.96(1) -0.320(0)  -2.980(0) - 3.0& \\\\\n       &= 32.21 -3.96 -3.0&& \\\\\n       &= 25.3&&\n\\end{split}\n\\]\nTo assess if any of the factors have a significant effect on the Death variables, simply wrap the regression with an anova() function.\n\nanova(M)\n\nAnalysis of Variance Table\n\nResponse: Deaths\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nEducation  4 478.52 119.630  7.7539 0.00251 **\nRegion     3  57.44  19.146  1.2410 0.33801   \nResiduals 12 185.14  15.428                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results suggest that the father’s educational attainment has a significant effect on infant mortality (p = 0.0025) whereas the region does not (p = 0.338).\n\n\n28.5.2 Visualizing a two-way ANOVA as a table\nYou can view the output of a two-way ANOVA as a “mean polish” table using the eda_pol function. Simply set the stat argument to mean.\n\ndf.ano &lt;- eda_pol(dfl, row = Region, col = Education, val = Deaths, stat = mean)"
  },
  {
    "objectID": "two_way.html#references",
    "href": "two_way.html#references",
    "title": "28  Analyzing two-way tables: The median polish",
    "section": "28.6 References",
    "text": "28.6 References\nUnderstanding Robust and Exploratory Data Analysis, D.C. Hoaglin, F. Mosteller and J.W. Tukey, 1983."
  },
  {
    "objectID": "rmarkdown.html#introduction",
    "href": "rmarkdown.html#introduction",
    "title": "29  R markdown document",
    "section": "29.1 Introduction",
    "text": "29.1 Introduction\nAn R markdown document is a text file usually ending with an .Rmd extension. It allows one to embed R code chunks and their output into a comprehensive report thus eliminating the possibility of loading the wrong figure into the document, or forgetting to update a statistical summary in the text when the original data file was revised.\nCreating an R markdown output from an Rmd file requires knitting the file as opposed to running the code as you would an R script. The RStudio interface provides you with a knit button at the top of its interface. The knit button also allows you to choose the output format (HMTL, Word or PDF). You can also knit an Rmd file in R using the render function from the rmarkdown package. For example,\n\nrmarkdown::render(\"HW16.Rmd\")"
  },
  {
    "objectID": "rmarkdown.html#the-yaml-header",
    "href": "rmarkdown.html#the-yaml-header",
    "title": "29  R markdown document",
    "section": "29.2 The YAML header",
    "text": "29.2 The YAML header\nThe YAML header controls the look and feel of your document. At the very least, your R markdown document should contain the following YAML header sandwiched between two sets of ---:\n ---\n title: \"Your document title\"\n author: \"ES 218\"\n output:\n   html_document: default\n ---\n\nMake sure that the html_document: default line is indented at least two spaces. If you intend on creating a Word document, substitute html_document with word_document.\n ---\n title: \"Your document title\"\n author: \"ES 218\"\n output:\n   word_document: default\n ---\n\nThe YAML header can take on several parameters. For example, to add the current date, add:\ndate: '`r format(Sys.time(), \"%d %B, %Y\")`'\n\nThe above chunk makes use of an inline code chunk that will be discussed later in this tutorial. Note the mix of single quotes and back ticks that wrap the inline code. The %d, %B and %y parameters specify the date format. You can read more on date formats here.\nTo have the document automatically generate a table of contents add toc: true to the html_document or word_document header. Make sure that the toc parameter is indented at least two spaces from the xxx_document header:\n...\noutput:\n  html_document: \n    toc: true\n\nThe above generates a static TOC. If you want to generate a floating TOC, add toc_float: true.\n...\noutput:\n  html_document: \n    toc: true\n    toc_float: true"
  },
  {
    "objectID": "rmarkdown.html#code-folding",
    "href": "rmarkdown.html#code-folding",
    "title": "29  R markdown document",
    "section": "29.3 Code folding",
    "text": "29.3 Code folding\nRmarkdown offers the option to interactively collapse the code chunks in a knitted document. This may not be an option to have in a final report, but it may prove useful for a technical document where both code and output are to be shared. Code folding option is set with code_folding: .... The options are hide to collapse the code chunks by default and show to reveal the code chunks by default.\n...\noutput:\n  html_document: \n    toc: true\n    toc_float: true\n    code_folding: hide"
  },
  {
    "objectID": "rmarkdown.html#section-headers",
    "href": "rmarkdown.html#section-headers",
    "title": "29  R markdown document",
    "section": "29.4 Section headers",
    "text": "29.4 Section headers\nYou can add section headers to your document by preceding the header with one or more hashtags. Each hashtag represents one heading level. For example, the top heading level is # and the third heading level is ###.\nThe top header hashtag is usually avoided because its default font size tends to be too big. It’s not uncommon to see R markdown files assign the top level to ##.\n## Use this as a top section level \n\nSome text\n\n### Use this as the second section level \n\nSome text\n\n#### Use this as the third section level \n\netc..."
  },
  {
    "objectID": "rmarkdown.html#use-this-as-a-top-section-level",
    "href": "rmarkdown.html#use-this-as-a-top-section-level",
    "title": "29  R markdown document",
    "section": "Use this as a top section level",
    "text": "Use this as a top section level\nSome text\n\nUse this as the second section level\nSome text\n\nUse this as the third section level\netc…"
  },
  {
    "objectID": "rmarkdown.html#text-formats",
    "href": "rmarkdown.html#text-formats",
    "title": "29  R markdown document",
    "section": "29.5 Text formats",
    "text": "29.5 Text formats\nThe markdown language has several built-in text formatting options. A brief summary of some their syntax follows:\n\nItalic: To italicize text, wrap it in asterisks as in *this is italicized*. Note that you do not want spaces between the asterisks and the text.\nBold: To bold text, wrap it with a pair of asterisks **this is bold**.\nWeb links: To create web links wrap the text with [ ] followed by the web link wrapped with ( ) as in [ES 218 website](https://mgimond.github.io/ES218). Make sure that there are no spaces between [] and ().\nLists: To create lists in your document, precede each list item with an asterisk followed by a space. For example:\n\n* First list element\n* Second list element\n* Third list element\n\n\n\nFirst list element\nSecond list element\nThird list element\n\n\n\nBlock equations: You can embed Latex block equations using double dollar signs,\n\n$$\nx = \\frac{1 + x}{x}\n$$\n\nwhich generates,\n\n\\[\nx = \\frac{1 + x}{x}\n\\]\n\n\nInline equations: You can also add inline Latex equations using single dollar signs,\n\nThe equation $x(1 + x)$ can be re-written as $x + x^2$. \n\nwhich generates,\n\nThe equation \\(x(1 + x)\\) can be re-written as \\(x + x^2\\)."
  },
  {
    "objectID": "rmarkdown.html#code-chunks",
    "href": "rmarkdown.html#code-chunks",
    "title": "29  R markdown document",
    "section": "29.6 Code chunks",
    "text": "29.6 Code chunks\nTo embed a code chunk, simply wrap the code between ```{r} and ```.\n```{r} \nplot(hp ~ mpg, mtcars)\n```\nCode chunks can take on many options. Examples of a few common options follow:\n\necho: If you don’t want the code chunks to appear in the ouput, set echo=FALSE.\ninclude: If you want neither the code chunk nor its ouput displayed in the output, set include=FALSE.\nfig.width and fig.height: These parameters control a figure’s height and width (in inches).\nwarning and message: Some functions will output warnings or messages, most of which you probably do not want in your output document. To hide these, set warning and message to FALSE.\n\nAn example of a code chunk with a few of the aformentioned parameter follows:\n```{r message=FALSE, warning=FALSE, echo=TRUE, fig.width=3, fig.height=2} \nplot(hp ~ mpg, mtcars)\n```\nHere’s the output (note that echo was set to TRUE in this example):\n\n\nplot(hp ~ mpg, mtcars)"
  },
  {
    "objectID": "rmarkdown.html#document-wide-code-chunk-options",
    "href": "rmarkdown.html#document-wide-code-chunk-options",
    "title": "29  R markdown document",
    "section": "29.7 Document wide code chunk options",
    "text": "29.7 Document wide code chunk options\nYou can apply document wide code chunk options. For example, to avoid adding message=FALSE and warning=FALSE to each chunk of code, you can add this single chunk of code to the beginning of your Rmd file.\n```{r include=FALSE}\nknitr::opts_chunk$set(message=FALSE, warning=FALSE)\n```"
  },
  {
    "objectID": "rmarkdown.html#inline-code-chunks",
    "href": "rmarkdown.html#inline-code-chunks",
    "title": "29  R markdown document",
    "section": "29.8 Inline code chunks",
    "text": "29.8 Inline code chunks\nIf statistical summaries or data derived vectors are to populate text in your document, it’s best to do so as inline code chunks. For example, you might want to embed the mean mpg value from the mtcars dataset in your text. The traditional approach is to compute the mean in an R console as follows,\n\nmean(mtcars$mpg)\n\n[1] 20.09062\n\n\nthen transcribe this value into your document as follows:\n\nThe mean miles per gallon is 20.1 mpg.\n\nAn automated way to do this is to embed the code chunk directly into your text by wrapping it between `r (backtic followed by the letter r) and another backtick.\nThe mean miles per gallon is `r mean(mtcars$mpg)` mpg.\n\nwhich generates:\n\nThe mean miles per gallon is 20.090625 mpg.\n\nTo control the precision, you can wrap the output with round,\nThe mean miles per gallon is `r round(mean(mtcars$mpg), 1)` mpg.\n\n\nThe mean miles per gallon is 20.1 mpg.\n\nIf the code chunk becomes too long and unwieldy to embed in your text, you can create an object from that code in a separate chunk of code, then reference that object inline. For example:\n```{r include=FALSE}\nM &lt;- lm(mpg ~ hp, mtcars)\nr.sq &lt;- round(summary(M)$r.square, 3)\n```\n\nThe modeled r-square between miles-per-gallon and engine horsepower is `r r.sq`.\n\nThe code chunk is hidden from the output, but the object r.sq is created nonetheless and converted to its value in the inline code chunk. The output thus looks like:\n\nThe modeled r-square between miles-per-gallon and engine horsepower is 0.602."
  },
  {
    "objectID": "rmarkdown.html#tables",
    "href": "rmarkdown.html#tables",
    "title": "29  R markdown document",
    "section": "29.9 Tables",
    "text": "29.9 Tables\nYou can create two types of tables: static tables where you manually populate the cell values, and dynamic tables which are populated with R data tables.\n\n29.9.1 Static tables\nHere’s an example of a static table syntax:\ncolumn 1        Column 2    column3 \n-----------  -----------  ------------\nval1                 2.3  apple\nval2                   5  orange\nval3                0.34  kiwi\n\n\n\n\n\ncolumn 1\nColumn 2\ncolumn3\n\n\n\n\nval1\n2.3\napple\n\n\nval2\n5.0\norange\n\n\nval3\n0.34\nkiwi\n\n\n\n\nNote how the left and right adjusted columns in the output reflect the left and right adjusted columns in the above syntax. It’s important that the column elements not extend beyond the dashed line extents.\n\n\n29.9.2 Dynamic tables\nThere are many R packages that specialize in table output formats such as xtable and stargazer. However, decent tables can be created with knitr’s kable function in conjunction with kableExtra. Note that this requires the magrittr package (if the pipe %&gt;% is used). However, if dplyr is used elsewhere in the Rmd document, the magrittr package can be omitted. Here’s an example:\nlibrary(magrittr)\nknitr::kable( head(mtcars), format=\"html\" ) %&gt;% \n  kableExtra::kable_styling(bootstrap_options = \"striped\", \n                            full_width = FALSE, position = \"left\")\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n\n\nDatsun 710\n22.8\n4\n108\n93\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n\n\nValiant\n18.1\n6\n225\n105\n\n\n\n\n\n\n\n\n\nIf the output file format is a Word document, substitute format = \"html\" with format = \"pandoc\".\nFor more kableExtra options, visit its website."
  },
  {
    "objectID": "rmarkdown.html#a-complete-example",
    "href": "rmarkdown.html#a-complete-example",
    "title": "29  R markdown document",
    "section": "29.10 A complete example",
    "text": "29.10 A complete example\nHere’s what a complete Rmd file might look like:\n\n ---\n title: \"A simple example\"\n author: \"ES 218\"\n output:\n   html_document: \n     toc: true\n editor_options: \n   chunk_output_type: console\n ---\n\n\n```{r include=FALSE}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE)\n```\n\n## A basic plot\n\n\n```{r fig.width = 3, fig.height = 2.5} \nlibrary(ggplot2)\n\nggplot(mtcars, aes(mpg, hp)) + geom_point() + \n  geom_smooth(method = lm, se = FALSE)\n```\n\n\n## Here's a glimpse of the data table\n\n\n```{r echo = FALSE}\nknitr::kable(head((mtcars), format = \"html\"))\n```\n\n\n## A basic analysis\n\n\n```{r include = FALSE}\nM &lt;- lm(mpg ~ hp, mtcars)\nr.sq &lt;- round(summary(M)$r.square, 2)\n```\n\n\nThe modeled r-square between miles-per-gallon and engine horsepower is `r r.sq`."
  },
  {
    "objectID": "rmarkdown.html#additional-resources",
    "href": "rmarkdown.html#additional-resources",
    "title": "29  R markdown document",
    "section": "29.11 Additional resources",
    "text": "29.11 Additional resources\n\nHere’s a sample Buoy data report Rmd file\nRstudio’s website has additional R markdown configuration options."
  },
  {
    "objectID": "spatial.html#loading-a-sample-dataset",
    "href": "spatial.html#loading-a-sample-dataset",
    "title": "30  Mapping variables",
    "section": "30.1 Loading a sample dataset",
    "text": "30.1 Loading a sample dataset\nThe following chunk loads a spatial dataset that is in an sf spatial format. The data represent the 2018 census tracts for the state of Maine (USA).\n\nlibrary(sf)\n\nshp &lt;- readRDS(gzcon(url(\"https://github.com/mgimond/ES218/blob/gh-pages/Data/maine_tracts.Rds?raw=true\")))\n\nThis next chunk is a data table of the average commute time (in minutes) for each Maine census tract.\n\ndat &lt;- read.csv(\"https://raw.githubusercontent.com/mgimond/ES218/gh-pages/Data/maine_commute.csv\")"
  },
  {
    "objectID": "spatial.html#joining-a-table-to-a-spatial-object",
    "href": "spatial.html#joining-a-table-to-a-spatial-object",
    "title": "30  Mapping variables",
    "section": "30.2 Joining a table to a spatial object",
    "text": "30.2 Joining a table to a spatial object\nMany mapping operations require that the data table be joined to an already created spatial dataset. If your data are aggregated at some administrative boundary level, you can find boundary files for most countries (and at different aggregate levels) in an sf format at https://gadm.org/.\nHere, we’ll join the dat table to the shp spatial object using Geo_FIPS as the common key.\n\nlibrary(dplyr)\n\nshp2 &lt;- left_join(shp, dat, by = \"Geo_FIPS\")"
  },
  {
    "objectID": "spatial.html#mapping-the-data",
    "href": "spatial.html#mapping-the-data",
    "title": "30  Mapping variables",
    "section": "30.3 Mapping the data",
    "text": "30.3 Mapping the data\nThere are at least two mapping solutions in R: ggplot2 and tmap. Examples using both mapping environments follow.\n\n30.3.1 Mapping with ggplot2\nThe ggplot grammar is no different here from what you’ve learned earlier in the course. We simply make use of the geom_sf() function to generate the map.\n\nlibrary(ggplot2) \n\nggplot(shp2) + geom_sf(aes(fill = Commute)) \n\n\n\n\nThe commute time variable, commute, is mapped to the map’s fill aesthetic. You can overcome many of the plot features’ default behavior. For example, you can bin the color schemes by assigning ranges of commute time values to a unique set of color swatches using one of the scale_fill_steps* family of functions. You can also remove the border colors by setting col = NA. This is demontrated next.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35))\n\n\n\n\nHere, we make use of hex defined colors. Note that you can use the built-in color names or the rgb() function to define the palette colors.\nIt might help to darken the background panel using the theme function. We’ll assign the grey70 color to the background and grey80 to the grid lines.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35)) +\n  theme(panel.grid = element_line(color = \"grey80\"),\n        panel.background = element_rect(fill = \"grey70\"))\n\n\n\n\nCoordinate systems can play an important role in both analyzing and visualizing spatial data. If you want to control the coordinate system used in your map, add the coord_sf function. Here we’ll adopt a UTM NAD83 Zone 19N coordinate system using its EPSG code of 26919. Note that the grid axes will still adopt the lat/long designation.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  coord_sf(crs = 26919) +\n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35)) +\n  theme(panel.grid = element_line(color = \"grey80\"),\n        panel.background = element_rect(fill = \"grey70\"))\n\n\n\n\nFinally, we’ll modify the bin intervals by generating a non-uniform classification scheme. As such we’ll scale the legend bar so as to reflect the non-uniform intervals using the guide_coloursteps() function and its even.steps = FALSE argument (note that this feature is only available in ggplot2 ver 3.3 or greater). We’ll also modify the legend bar dimensions and title.\n\nggplot(shp2) + geom_sf(aes(fill = Commute), col = NA) + \n  coord_sf(crs = 26919) +\n  scale_fill_stepsn(colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                               \"#D9EF8B\", \"#91CF60\", \"#1A9850\") ,\n                    breaks = c(5, 15, 20, 30, 35),\n                    values = scales::rescale(c(5, 15, 20, 30, 35), c(0,1)),\n                    guide = guide_coloursteps(even.steps = FALSE,\n                                              show.limits = TRUE,\n                                              title = \"Commute time \\n(min)\",\n                                              barheight = unit(2.3, \"in\"),\n                                              barwidth = unit(0.15, \"in\"))) +\n  theme(panel.grid = element_line(color = \"grey80\"),\n        panel.background = element_rect(fill = \"grey70\"))\n\n\n\n\n\n\n30.3.2 Mapping with tmap\nWhile ggplot2 can handle basic mapping operations, it does not offer the ease and flexibility of the tmap package. Much like ggplot2, tmap adopts the same layering strategy. For example, to construct the above plot we first specify the layer to plot via the call to tm_shape, then the geometry to symbolize the spatial feature, tm_fill().\n\nlibrary(tmap)\n\ntm_shape(shp2) +  tm_fill(col = \"Commute\")\n\n\n\n\nTo change the projection, pass the PROJ4 string to the projection parameter. To move the legend box outside of the map, set tm_legend(outside = TRUE).\n\ntm_shape(shp2, projection = 26919) + \n  tm_fill(col = \"Commute\") +\n  tm_legend(outside = TRUE)\n\n\n\n\nYou can also modify the classification breaks and colors. For example, to break the values following quantiles, and assigning the same color scheme used with the ggplot output in an earlier code chunk, you would write:\n\ntm_shape(shp2, projection = 26919) + \n  tm_fill(col = \"Commute\", n = 6, style = \"quantile\",  \n          palette = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \n                      \"#D9EF8B\", \"#91CF60\", \"#1A9850\")) +\n  tm_legend(outside = TRUE)\n\n\n\n\nTo explore additional mapping options with tmap Click here."
  },
  {
    "objectID": "spatial.html#another-example",
    "href": "spatial.html#another-example",
    "title": "30  Mapping variables",
    "section": "30.4 Another example",
    "text": "30.4 Another example\nIn this section we explore a more complicated workflow that requires additional data manipulation steps to allow a proper join between US counties and Census data.\nFirst, we’ll load the cnty spatial object to your current session environment.\n\nlibrary(sf)\n\nload(url(\"https://github.com/mgimond/ES218/blob/gh-pages/Data/counties48.RData?raw=true\"))\n\nNext, we’ll plot the data. Since the data spans the continental US, we’ll change the projection to an equal area projection. This projection will be defined using a string (ea object)–note that a string is another way to define a projection. To learn more about defining projections in R, click here.\nSince we have no polygons to fill just yet, we’ll simply draw the outline using tm_polygons()\n\n# Define the projection using PROJ4 syntax\nea &lt;- \"+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 \n       +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\"\n\n# Draw the map\ntm_shape(cnty, projection = ea) + \n  tm_polygons() +\n  tm_layout(outer.margins = c(.1,.1,.1,.1))"
  },
  {
    "objectID": "spatial.html#joining-tables-to-spatial-objects",
    "href": "spatial.html#joining-tables-to-spatial-objects",
    "title": "30  Mapping variables",
    "section": "30.5 Joining tables to spatial objects",
    "text": "30.5 Joining tables to spatial objects\nIn the examples that follow, you will learn how to join census income data to a spatial object in one of two ways: First, by state and county names; Then, by FIPS (Federal Information Processing Standards) ids.\n\n30.5.1 Joining income table to the map by county and state names\nWe will use the income-by-gender-education dataset used earlier in the course and limit the data to median income per person (B20004001 column).\n\nlibrary(dplyr)\ndf &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/Income_education.csv\")\ndf1 &lt;- df %&gt;% select(subregion = County, region = State, B20004001 )\n\nNext, we need to join the census data to the county map using two columns: state name and county name (or region and subregion columns). Let’s compare the two dataframes by viewing the first few rows of each.\n\nhead(cnty)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -88.01778 ymin: 30.24071 xmax: -85.06131 ymax: 34.2686\nGeodetic CRS:  WGS 84\n               ID                       geometry\n1 alabama,autauga MULTIPOLYGON (((-86.50517 3...\n2 alabama,baldwin MULTIPOLYGON (((-87.93757 3...\n3 alabama,barbour MULTIPOLYGON (((-85.42801 3...\n4    alabama,bibb MULTIPOLYGON (((-87.02083 3...\n5  alabama,blount MULTIPOLYGON (((-86.9578 33...\n6 alabama,bullock MULTIPOLYGON (((-85.66866 3...\n\nhead(df1)\n\n  subregion region B20004001\n1   Autauga     al     35881\n2   Baldwin     al     31439\n3   Barbour     al     25201\n4      Bibb     al     29016\n5    Blount     al     32035\n6   Bullock     al     26408\n\n\nWe note two problems. First, the cnty object encodes states using their full name and not their two letter abbreviation. Second, the county names in cnty are in all lower case. Before attempting to join the dataframes, we must first fix these discrepancies. We will choose to modify df so that its state and county names match those in the cnty (map) dataframe.\nWe will first create a state name/abbreviation look-up table using the built-in state.name and state.abb objects. Note that using these pre-existing objects will require that we add the District of Columbia to the table since it’s not present in either data objects.\n\nst &lt;- data.frame(region=tolower(state.name), State = tolower(state.abb)) %&gt;% \n      bind_rows( data.frame(region=\"district of columbia\", State=\"dc\") ) \n\nNext, we join the st look-up table to df then make the additional changes needed to join the census data to the counties map. Note that we are overwriting the earlier instance of df1.\n\ndf1 &lt;- df %&gt;% \n  inner_join(st, by=\"State\") %&gt;%\n  mutate(ID = paste(region,tolower(County), sep = \",\"))  %&gt;%\n  select(ID, B20004001 )\n\nWe can now join df1 to cnty.\n\ncnty.df1 &lt;- inner_join(cnty, df1, by=\"ID\" )\n\nNow let’s map the income distribution.\n\ntm_shape(cnty.df1) + tm_fill(col = \"B20004001\", palette = \"Greens\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nYou may notice that the counties map does not seem complete; most notable is the absence of all Louisiana counties. Let’s look at the rows in cnty.df1 associated with the state of Louisiana.\n\nlibrary(stringr)\ntable(str_detect(cnty.df1$ID,  \"louisiana\") )\n\n\nFALSE \n 2971 \n\n\nThere are no rows returned (i.e. all cases returned FALSE). This suggests that the Louisiana counties did not properly join. Let’s compare the Louisiana county names between df1 and cnty.\n\ndf1 %&gt;% filter(str_detect(ID,  \"louisiana\")) %&gt;% head()\n\n                           ID B20004001\n1     louisiana,acadia parish     28678\n2      louisiana,allen parish     29870\n3  louisiana,ascension parish     43464\n4 louisiana,assumption parish     34269\n5  louisiana,avoyelles parish     26483\n6 louisiana,beauregard parish     33983\n\ncnty %&gt;% filter(str_detect(ID,  \"louisiana\")) %&gt;% head()\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -93.74163 ymin: 29.62765 xmax: -90.63619 ymax: 31.35225\nGeodetic CRS:  WGS 84\n                    ID                       geometry\n1     louisiana,acadia MULTIPOLYGON (((-92.61863 3...\n2      louisiana,allen MULTIPOLYGON (((-92.5957 30...\n3  louisiana,ascension MULTIPOLYGON (((-91.12894 3...\n4 louisiana,assumption MULTIPOLYGON (((-91.23207 3...\n5  louisiana,avoyelles MULTIPOLYGON (((-92.32642 3...\n6 louisiana,beauregard MULTIPOLYGON (((-93.12856 3...\n\n\nIt turns out that Louisiana does not divide its administrative areas into counties but parishes instead. The income data (originally downloaded from the Census Bureau) follows through with that designation convention by adding the word “parish” to each of its administrative area names. We therefore need to remove all instances of parish in the subregion names associated with Louisiana. We will therefore need to recreate the df1 object as follows:\n\nlibrary(stringr)\ndf1 &lt;- df %&gt;% \n  inner_join(st, by=\"State\") %&gt;%\n  mutate(ID = paste(region,tolower(County), sep = \",\"),\n         ID = ifelse(region==\"louisiana\", \n                     str_replace(ID, \" parish\", \"\"), ID))  %&gt;%\n  select(ID, B20004001 )\n\nLet’s re-join the dataframes and re-plot the map.\n\ncnty.df1 &lt;- inner_join(cnty, df1, by=\"ID\" )\n\ntm_shape(cnty.df1) + tm_fill(col = \"B20004001\", palette = \"Greens\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nMost of Louisiana’s parishes are now mapped, but we are still missing a few parishes as well as a few counties. This is a result of differences in spelling for two-word county and parish names. For example, df1 encodes St. Lucie county (Florida) as st. lucie whereas cnty omits the dot and encodes it as st lucie. Fixing these discrepancies will require additional labor. At this point, it may prove more fruitful to do away with state/county names as joining keys and use FIPS county codes instead.\nFIPS (Federal Information Processing Standards) codes assign each county/state a unique five number designation thus making it easier to join data to spatial features. However, neither our census data table nor the built-in counties map have FIPS codes. We will download another version of the income data (one that has FIPS codes). Note that FIPS codes are normally part of datasets provided by the Census Bureau. The maps package has a dataset called county.fips that can be used to match state/county names in the county map to FIPS codes.\n\n\n30.5.2 Joining income table by FIPS code\nLet’s download the FIPS version of the dataset. Note that we will not need to modify/add columns to the data since we will be joining this table to the county map using the FIPS code.\n\ndf &lt;- read.csv(\"http://mgimond.github.io/ES218/Data/Income_education_with_FIPS.csv\")\ndf1 &lt;- df %&gt;% select(FIPS, B20004001 )\n\nNext, we’ll load the county.fips dataset from the maps package.\n\n# Load the county.fips dataset\ncounty.fips &lt;- maps::county.fips\nhead(county.fips)\n\n  fips        polyname\n1 1001 alabama,autauga\n2 1003 alabama,baldwin\n3 1005 alabama,barbour\n4 1007    alabama,bibb\n5 1009  alabama,blount\n6 1011 alabama,bullock\n\n\n\ncnty2 &lt;- cnty %&gt;%\n         left_join(county.fips, by=c(\"ID\" = \"polyname\"))\n\nNow that the FIPS codes are part of the county map dataframe, let’s join the income data table to the county map, then map the income values.\n\ncnty2.df1 &lt;- inner_join(cnty2, df1, by=c(\"fips\" = \"FIPS\") )\n\ntm_shape(cnty2.df1) + tm_fill(col = \"B20004001\", palette = \"Greens\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nThis is an improvement over the last map. But there are still a few counties missing. After a closer scrutiny of the data, it seems that the county.fips table splits some counties into two or more sub-regions. For example, Accomack county (Virginia) is split into two regions (and thus into two different names):\n\ncounty.fips %&gt;% filter(str_detect(polyname,  \"virginia,accomack\"))\n\n   fips                       polyname\n1 51001         virginia,accomack:main\n2 51001 virginia,accomack:chincoteague\n\n\nFixing these discrepancies would require manual intervention and time. Another work around is to use the Census Bureau’s map files instead of maps’s built-in map. We will cover this in the next section."
  },
  {
    "objectID": "spatial.html#working-with-external-gis-files",
    "href": "spatial.html#working-with-external-gis-files",
    "title": "30  Mapping variables",
    "section": "30.6 Working with external GIS files",
    "text": "30.6 Working with external GIS files\n  \n\n30.6.1 Reading a shapefile into R\nThe Census Bureau maintains its own library of administrative boundary shapefiles (shapefiles are popular map data formats). This may prove to be the best map source to use since its areal units are designed to match the records in the income data table. Loading a shapefile can easily be accomplished using the st_read() function from the sf package. One option is to download the shapefile from the Census Bureau’s website (usually in a zipped format) then unpack it in a local project folder before reading it into the current session with st_read(). But we can maximize the replicability of the workflow by writing a chunk of code that will download the zipped shapefile into a temporary directory before unzipping it and loading the shapefile into an active R session as shown below.\n\nlibrary(sf)\n\ntmp &lt;- tempdir()\nlink &lt;- \"http://www2.census.gov/geo/tiger/GENZ2010/gz_2010_us_050_00_20m.zip\"\nfilename &lt;- basename(link)\ndownload.file(link, filename)\nunzip(filename, exdir = tmp )\nshapefile &lt;- st_read(dsn = tmp, layer = tools::file_path_sans_ext(filename))\n\nReading layer `gz_2010_us_050_00_20m' from data source `C:\\Users\\mgimo\\AppData\\Local\\Temp\\Rtmp0iNDJq' using driver `ESRI Shapefile'\nSimple feature collection with 3221 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 17.88481 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\n\nThe shapefile is unzipped into a temporary folder, tmp, then read from that same directory and stored in an sf object called shapefile. The code tools::file_path_sans_ext(filename) extracts the name of the zip file (minus the extension) which turns out to also be the name of the shapefile. Note that if the zip file was manually unpacked in a project folder then the only command that would have needed to be executed is the st_read function as in st_read(\"gz_2010_us_050_00_20m.shp\" ).\nThe Census shapefile has a field called GEO_ID that includes the county FIPS codes along with other superfluous county ID values. We only need the last five digit county FIPS code so we’ll extract these codes into a new column called FIPS.\n\nshapefile$FIPS &lt;- as.numeric(str_sub(shapefile$GEO_ID, 10, 14))\n\nNext we can append the income table to the shapefile object.\n\ncnty2.cs &lt;- shapefile %&gt;%\n            inner_join(df1, by=\"FIPS\")\n\nNow, we can plot the income distribution map. Note that the Census Bureau shapefile includes the 48 states, Alaska and Hawaii. If you want to limit the extent to the 48 states, define the boundary limits using the bbox parameter in the tm_shape function.\n\ntm_shape(cnty2.cs, \n         bbox = st_bbox(c(xmin = -125, xmax = -67 , ymin = 25, ymax = 50))) +\n  tm_fill(col = \"B20004001\", palette = \"Greens\", title = \"Income\") +\n  tm_legend(outside = TRUE) \n\n\n\n\n\n\n30.6.2 Modifying the color scheme\nThe classification scheme can be customized. For example, to split the income values across six intervals (e.g. $0 to $20000, $20000 to $30000, $30000 to $50000, and $50000 to $100000) and to label the values as dollars, type the following:\n\ntm_shape(cnty2.cs, \n         bbox = st_bbox(c(xmin = -125, xmax = -67 , ymin = 25, ymax = 50))) +\n  tm_fill(col = \"B20004001\", palette = \"Greens\",\n          style=\"fixed\", breaks=c(0, 20000 , 30000, 50000, 100000 ),\n          labels = c(\"under $20k\", \"$20k - $30k\", \"$30k - $50k\", \"above $50k\"),\n          title = \"Income\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nWe might choose to map the income distribution using a divergent color scheme where we compare the counties to some overall value such as the counties’ median income value. We can use the quantile() function to break the income data into its percentiles. This will be helpful in generating symmetrical intervals about the median.\n\ninc.qt &lt;- quantile(df1$B20004001, probs = c(0, 0.125, .25, 0.5, .75, .875 , 1 ))\n\nA divergent color scheme consists of two different hues whose lightness values converge towards some central value (the median income in our case). We’ll use a red-to-green color palette, RdYlGn, for this next map where red hues highlight counties below the county median income and green hues highlight counties above the county median value.\n\ntm_shape(cnty2.cs,\n         bbox = st_bbox(c(xmin = -125, xmax = -67 , ymin = 25, ymax = 50))) +\n  tm_fill(col = \"B20004001\", palette = \"RdYlGn\",\n          style=\"fixed\", breaks = inc.qt,\n          title = \"Income\") +\n  tm_legend(outside = TRUE) \n\n\n\n\nThe divergent map gives us a different handle on the data. Here, we can identify the poorer than average counties such as the southeast and parts of the Mississippi river region as well as the wealthier counties such as the mid-Atlantic region and the west coast."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "31  Data used in ES218",
    "section": "",
    "text": "CO2 (1959-2018): file, README\nCO2 (1959-2020): file, README\nSouthern oscillation index (1960-2014): file, README\nPacific decadal oscillation index (1900-2001): file, README\nACS Income and Education (2008-2012): file (CSV), file (RDS), Code book\nIncome by educational attainment (2008 - 2012): file (CSV), file with FIPS (CSV), Code book\nPopulation (1850-2013): file, README\nColby courses (Spring 2020): file, Department/Division key\nSeeded clouds: file, README\nGHCND temperature data for five US locations: file, README, Documentation\nHistorical temperature data for New York: file, README\nHistorical temperature data for Bombay: file, README\nHistorical temperature data for Shanghai: file, README\nGulf of Maine buoy data (2012): file, README\nGulf of Maine buoy data (1905 - 2012): file\nKennebec River daily discharge (2004 - 2014): file (csv),file (xlsx), README\nNFL Combine: file, README\nUninsured by county: file, README\nGreenhouse gas emissions for the northeast (2008 & 2011): file, README\nNorth american grain production (1961-2012): file, README\nLogan airport (BOS) flight data (2014): file (CSV),file (rds), README\nConsumer expenditure (1995-2012): file (csv), file (RDS), README\nWilliam Cleveland’s fusion time data: file\nWilliam Cleveland’s food web data: file\nWilliam Cleveland’s ganglion data: file\nWilliam Cleveland’s figure 3.6 data: file\nWilliam Cleveland’s carbon dating data: file\nIn-class sample data 1: file"
  },
  {
    "objectID": "case_study.html#introduction",
    "href": "case_study.html#introduction",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.1 Introduction",
    "text": "23.1 Introduction\nThe following data represent 1,2,3,4-tetrachlorobenzene (TCB) concentrations (in units of ppb) for two site locations: a reference site free of external contaminants and a contaminated site that went through remediation (dataset from Millard et al., p. 416-417).\n\n# Create the two data objects (TCB concentrations for reference and contaminated sites)\nRef &lt;-  c(0.22,0.23,0.26,0.27,0.28,0.28,0.29,0.33,0.34,0.35,0.38,0.39,\n          0.39,0.42,0.42,0.43,0.45,0.46,0.48,0.5,0.5,0.51,0.52,0.54,\n          0.56,0.56,0.57,0.57,0.6,0.62,0.63,0.67,0.69,0.72,0.74,0.76,\n          0.79,0.81,0.82,0.84,0.89,1.11,1.13,1.14,1.14,1.2,1.33)\nCont &lt;- c(0.09,0.09,0.09,0.12,0.12,0.14,0.16,0.17,0.17,0.17,0.18,0.19,\n          0.2,0.2,0.21,0.21,0.22,0.22,0.22,0.23,0.24,0.25,0.25,0.25,\n          0.25,0.26,0.28,0.28,0.29,0.31,0.33,0.33,0.33,0.34,0.37,0.38,\n          0.39,0.4,0.43,0.43,0.47,0.48,0.48,0.49,0.51,0.51,0.54,0.6,\n          0.61,0.62,0.75,0.82,0.85,0.92,0.94,1.05,1.1,1.1,1.19,1.22,\n          1.33,1.39,1.39,1.52,1.53,1.73,2.35,2.46,2.59,2.61,3.06,3.29,\n          5.56,6.61,18.4,51.97,168.64)\n\n# We'll create a long-form version of the data for use with some of the functions\n# in this exercise\ndf &lt;- data.frame( Site = c(rep(\"Cont\",length(Cont) ), rep(\"Ref\",length(Ref) ) ),\n                  TCB  = c(Cont, Ref ) )\n\nOur goal is to assess if, overall, the concentrations of TCB at the contaminated site are different from those of the reference site with the alternative being that the contaminated site has concentrations greater than those at the reference site."
  },
  {
    "objectID": "case_study.html#a-typical-statistical-approach-the-two-sample-t-test",
    "href": "case_study.html#a-typical-statistical-approach-the-two-sample-t-test",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.2 A typical statistical approach: the two sample t-Test",
    "text": "23.2 A typical statistical approach: the two sample t-Test\nWe are interested in answering the question: “Did the cleanup at the contaminated site reduce the concentration of TCB down to background (reference) levels?”. If the question being addressed is part of a decision making process such as “Should we continue with the remediation?” we might want to assess if the difference in TCBs between both sites is “significant” enough to conclude that the TCBs are higher than would be expected if chance alone was the process at play.\nA popular statistical procedure used to help address this question is the two sample t-Test. The test is used to assess whether or not the mean concentration between both batches of values are significantly different from one another. The test can be framed in one of three ways: We can see if the batches are similar, if one batch is greater than the other batch, or if one batch is smaller than the other batch. In our case, we will assess if the Cont batch is greater than the Ref batch (this is the alternative hypothesis). We’ll make use of the t.test function and set the parameter alt to \"greater\".\n\nt.test(Cont, Ref, alt=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  Cont and Ref\nt = 1.4538, df = 76.05, p-value = 0.07506\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.4821023        Inf\nsample estimates:\nmean of x mean of y \n3.9151948 0.5985106 \n\n\nThe test suggests that there is just a small chance (a 7.5% chance) that the reference site could have generated mean concentrations as great as those found at the contaminated site. The test also outputs the means of each batch: 3.9 ppb for the contaminated site and 0.6 ppb for the reference site.\nMany ancillary data analysts may stop here and proceed with the decision making process. This is not good practice. To see why, let’s deconstruct the t-test.\nFirst, we need to find out how the test is characterizing the batches of numbers. The t-test characterizes the location of the batch using the mean, and the spread using the standard deviation. In essence, the test is reducing the complexity of the batches down to two numbers.\nThe t-test uses these numbers to reconstruct, then compare the distributions. For example, here’s how the t-test is characterizing the distribution of value for the reference site, Ref:\n\n\n\n\n\nThe right side of the plot is the fitted Normal distribution computed from the sample’s standard deviation–this is how the t-test is characterizing the Ref distribution. The left side of the plot is the density distribution showing the actual shape of the distribution. The points in between both plot halves are the actual values. Note the skew towards higher concentrations. For the Ref data, one might argue that the Normal fit is doing a reasonably good job in characterizing the distribution of values though the outliers may be exaggerating the width of the Normal distribution.\nNow, let’s see how well the Normal fit characterizes the distribution of the contaminated site values, Cont.\n\n\n\n\n\nThe Normal fit is on the right. The density plot is barely noticeable on the left side of the plot! You’ll note the tight cluster of points near the center of the Normal distribution. There are just a few points that extend beyond the tight cluster. These outliers are disproportionately inflating the standard deviation which, in turn, leads to the disproportionately large Normal distribution that is adopted in the t-test.\nSince the t-test adopts a Normal characterization of the spread, it would behoove us to check the assumption of normality using the normal q-q plot. Here, we’ll compare the Cont values to a Normal distribution.\n\n\n\n\n\nThis is a textbook example of a batch of values that does not conform to a Normal distribution. At least four values (which represent ~5% of the data) seem to contribute to the strong skew and to a much distorted representation of location and spread. The mean and standard deviation are not robust to extreme values. In essence, all it takes is one single outlier to heavily distort the representation of location and spread in our data. The mean and standard deviation have a breakdown point of 1/n where n is the sample size.\nThe median and interquartile range are less sensitive to extreme values. In fact, the median has a breakdown point of n/2. In other words, half of the values would have to be modified to alter the median.\nThe boxplot makes use of these robust measures of location and spread; let’s compare the batches with and without the extreme (outlier) values.\n\n\n\n\n\nNote that because of the robust nature of the median and interquartile range, the boxplot helps us spot the outliers. In fact, the boxplot has a breakdown point of n/4 (i.e. 25% of the values must be extreme before we see any masking of extreme values). The standard deviation, on the other hand, can be inflated by one extreme value thus masking the potentially problematic values.\nOne observation that can also be gleaned from this plot is the skewed nature of the Cont data within the interquartile range (IQR). This suggests that even if we were to remove the outliers, the data would violate the normal distribution requirements.\nBut, most importantly, the boxplot seems to contradict the conclusion gleaned from the t-test. The plot suggests that the contaminated site has a lower overall concentration than that of the reference site when comparing medians instead of means!"
  },
  {
    "objectID": "case_study.html#re-expression",
    "href": "case_study.html#re-expression",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.3 Re-expression",
    "text": "23.3 Re-expression\nIf we are to use the t-test, we need to make sure that the distributional requirements are met. Even Welch’s modification has one requirement about the distribution: both spreads must follow a normal distribution. Let’s compare both batches to a normal distribution via a normal q-q plot. We’ll make use of the custom eda_qq function:\n\nlibrary(tukeyedar)\neda_qq(Ref, norm=TRUE)\neda_qq(Cont, norm=TRUE)\n\n\n\n\nThese batches do not follow the straight line. This suggests skewness in the distribution (as was observed with the boxplots). A workaround to this problem is to re-express the batches of values in such a way to render them as close to normal as possible. However, in doing so, we must make sure that both batches are re-expressed in an equal way to facilitate comparison. A popular re-expression used with observational data that exhibit skewness towards higher values is the log transformation. The eda_qq function has an argument, p=, that takes as input a re-expression that will be applied to the values. The function will also allow us to choose between a Tukey (tukey=TRUE) or Box-Cox (tukey=FALSE) method. The default is set to tukey=FALSE. To apply a log transformation, we set p to 0.\n\neda_qq(Ref, norm=TRUE, p = 0)\neda_qq(Cont, norm=TRUE, p = 0)\n\n\n\n\nThe log transformed data is an improvement, but a skew in both batches is still apparent. We’ll need to explore other powers next."
  },
  {
    "objectID": "case_study.html#fine-tuning-the-re-expression",
    "href": "case_study.html#fine-tuning-the-re-expression",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.4 Fine-tuning the re-expression",
    "text": "23.4 Fine-tuning the re-expression\nThe log transformation is one of many re-expressions that can be applied to the data. Let’s explore the skewness across different “depths” of the Cont values to see if the skewness is systematic. We’ll use letter value summary plots to help guide us to a reasonable re-expression. We’ll make use of the custom eda_lsum function to generate the table and ggplot2 to generate the plot.\nFirst, we’ll look at the raw contaminated site data:\n\nlibrary(ggplot2)\n\nCont.lsum &lt;- eda_lsum(Cont, l=7)\nggplot(Cont.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + \n                    scale_x_reverse() +geom_text(vjust=-.5, size=4)\n\n\n\n\nThe data become strongly skewed for 1/32th of the data (depth letter C). Let’s now look at the reference site.\n\nRef.lsum &lt;- eda_lsum(Ref, l=7)\nggplot(Ref.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + \n                   scale_x_reverse() +geom_text(vjust=-.5, size=4)\n\n\n\n\nA skew is also prominent here but a bit more consistent across the depths with a slight drop between depths D and C (16th and 32nd extreme values).\nNext, we will find a power function that re-expresses the values to satisfy the t-test distribution requirement. We’ll first look at the log transformation implemented in the last section. Note that we are using the custom eda_re function to transform the data. We’ll also make use of some advanced coding to reduce the code chunk size.\n\nlibrary(dplyr)\n\ndf  %&gt;% \n  group_by(Site) %&gt;%\n  do(eda_lsum( eda_re(.$TCB,0), l=7) ) %&gt;%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThe log transformation seems to work well with the reference site, but it’s not aggressive enough for the contaminated site. Recall that to ensure symmetry across all levels of the batches, the letter values must follow a straight (horizontal) line. Let’s try a power of -0.5. Given that the negative power will reverse the order of the values if we adopt the Tukey transformation, we’ll use the default Box-Cox method.\n\ndf  %&gt;% \n  group_by(Site) %&gt;%\n  do(eda_lsum( eda_re(.$TCB,-0.5, tukey=FALSE), l=7) ) %&gt;%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThis seems to be too aggressive. We are facing a situation where attempting to normalize one batch distorts the other batch. Let’s try a compromise and use -.35.\n\ndf  %&gt;% \n  group_by(Site) %&gt;%\n  do(eda_lsum( eda_re(.$TCB,-0.35, tukey=FALSE), l=7) ) %&gt;%\n  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + \n  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)\n\n\n\n\nThis seems to be a bit better. It’s obvious that we will not find a power transformation that will satisfy both batches, so we will need to make a judgement call and work with a power of -.35 for now.\nLet’s compare the re-expressed batches with a normal distribution.\n\neda_qq(Cont, norm = TRUE, p = -0.35)\neda_qq(Ref, norm = TRUE, p = -0.35)\n\n\n\n\nThe distributions look quite good when viewed in a normal q-q plot. Let’s now compare the re-expressed batches using a density/normal fit plot.\n\n\n\n\n\nRecall that the right half is the Normal fit to the data and the left half is the density plot. There is far greater agreement between the normal fits and their accompanying density distributions for both sites. You’ll also note that the mean concentration is now smaller at the contaminated site than it is a the reference site–this is in agreement with what we observed in the boxplots.\nSo, how much impact did the skewed distributions have on the t-test? Now that we have normally distributed values, let’s rerun the t-test using the re-expressed values. Note that because the power is negative, we will adopt the Box-Cox transformation to preserve order by setting tukey=FALSE to the eda_re function.\n\nt.test(eda_re(Cont,-0.35, tukey=FALSE), eda_re(Ref,-0.35,tukey=FALSE), alt=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  eda_re(Cont, -0.35, tukey = FALSE) and eda_re(Ref, -0.35, tukey = FALSE)\nt = -1.0495, df = 111.68, p-value = 0.8519\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.4845764        Inf\nsample estimates:\n mean of x  mean of y \n-0.9264188 -0.7386209 \n\n\nThe result differs significantly from that with the raw data. This last run gives us a p-value of 0.85 whereas the first run gave us a p-value of 0.075. This suggests that there is an 85% chance that the reference site could have generated a mean concentration greater than that observed at the contaminated site."
  },
  {
    "objectID": "case_study.html#addendum",
    "href": "case_study.html#addendum",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.5 Addendum",
    "text": "23.5 Addendum\nIt was obvious from the boxplot of the original (un-transformed) data that for a few sites, more remediation was needed given their higher than typical TCB values. But, it was also quite apparent from the boxplot that for many other sites, the concentrations were much less than those found at the reference site suggesting that the remediation reduced TCB concentrations that occur naturally in that environment given that none of the reference site values were as low as those found at the contaminated site. No statistical procedure was needed to come to this conclusion! So while the t-test told us something about the significance of differences in their mean values, it failed to pick up on some of the intricate details that could only be gleaned from our plots. This is the whole essence of exploratory data analysis!"
  },
  {
    "objectID": "case_study.html#references",
    "href": "case_study.html#references",
    "title": "23  A working example: t-tests and re-expression",
    "section": "23.6 References",
    "text": "23.6 References\nMillard S.P, Neerchal N.K., Environmental Statistics with S-Plus, 2001."
  },
  {
    "objectID": "bivariate.html#footnotes",
    "href": "bivariate.html#footnotes",
    "title": "24  Fits and residuals",
    "section": "",
    "text": "Cleveland, William. Visualizing data. Hobart Press. 1993. [pp 87-88]↩︎"
  },
  {
    "objectID": "discontinuity.html#case-study-trends-in-daily-temperature-extremes",
    "href": "discontinuity.html#case-study-trends-in-daily-temperature-extremes",
    "title": "27  Slicing data",
    "section": "27.2 Case study: Trends in daily temperature extremes",
    "text": "27.2 Case study: Trends in daily temperature extremes\n\nDisclaimer: the analysis presented here is only exploratory and does not mirror the complete analysis conducted by Vincent et al. nor the one conducted by Stone.\n\n\n27.2.1 Original analysis\nThe following data are pulled from the paper titled “Observed Trends in Indices of Daily Temperature Extremes in South America 1960-2000” (Vincent et al., 2005) and represent the percentage of nights with temperatures greater than or colder than the 90th and 10th percentiles respectively within each year. The percentiles were calculated for the 1961 to 2000 period.\n\nlibrary(tidyr)\n\nYear &lt;- 1960:2000\nPerC &lt;- c(11.69,9.33,14.35,10.73,14.15,11.16,13,12.13,14.25,10.01,11.94,14.35,\n          10.83,9.38,11.5,10.44,12.66,7.55,9.77,9.81,8.9,8.51,7.02,6.83,9.67,\n          7.84,7.11,8.56,10.59,7.93,8.85,8.8,8.75,8.18,7.16,9.91,10.15,6.58,\n          6.44,9.43,8.03)\nPerH &lt;- c(8.62,10.1,6.67,11.13,5.71,9.48,7.63,8.12,7.2,9.64,8.42,5.71,11.72,\n          11.32,7.2,7.17,7.46,13.17,9.28,8.75,12.38,10,13.83,17.59,10.14,\n          9.84,11.23,14.39,9.44,8.26,12.15,12.45,13.14,13.67,15.22,11.79,11.16,\n          20.37,17.56,11.13,11.49)\ndf2 &lt;- data.frame(Year, PerC, PerH)\n\ndf2.l &lt;- pivot_longer(df2, names_to = \"Temp\", values_to = \"Percent\", -Year)\n\nLet’s plot the data and fit a straight line to the points.\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) + facet_wrap(~ Temp, nrow = 1)\n\n\n\n\nThe plot on the left shows percent cold nights and the one on the right shows percent hot nights. At first glance, the trends seem real and monotonic.\nNext we’ll fit a loess to see if the trends are indeed monotonic. To minimize the undue influence of end values in the plot, we’ll implement loess’ bisquare estimation method via the family=symmetric option. We’ll also use a small span to help identify any “kinks” in the patterns.\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"loess\", se = FALSE, span = 0.5, \n              method.args = list(degree = 1, family = \"symmetric\"))  + \n  facet_wrap(~ Temp, nrow = 1)\n\n\n\n\nThe patterns seem to be segmented around the 1975-1980 period for both plots suggesting that the observed trends may not be monotonic. In fact, there appears to be a prominent kink in the percent cold data around the mid to late 1970`s. A similar, but not as prominent kink can also be observed in the percent hot data at around the same time period.\n\n\n27.2.2 Changepoint\nIn a comment to Vincent et al.’s paper, R.J. Stone argues that the trend observed in the percent hot and cold dataset is not monotonic but segmented instead. In other words, there is an abrupt change in patterns for both datasets that make it seem as though a monotonic trend exists when in fact the data may follow relatively flat patterns for two different segments of time. He notes that the abrupt change (which he refers to as a changepoint) occurs around the 1976 and 1977 period. He suggests that this time period coincides with a change in the Pacific Decadal Oscillation (PDO) pattern. PDO refers to an ocean/atmospheric weather pattern that spans multiple decades and that is believed to impact global climate.\nThe following chunk of code loads the PDO data, then summarizes the data by year before plotting the resulting dataset.\n\ndf3  &lt;- read.table(\"http://mgimond.github.io/ES218/Data/PDO.dat\", \n                   header = TRUE, na.strings = \"-9999\")\npdo &lt;- df3 %&gt;%\n  pivot_longer(names_to = \"Month\", values_to = \"PDO\", -YEAR) %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(PDO = median(PDO) )\n\nggplot(pdo, aes(x = YEAR, y = PDO)) + geom_line() + \n  stat_smooth(se = FALSE, span = 0.25) +\n  geom_vline(xintercept = c(1960, 1976, 2000), lty = 3)\n\n\n\n\nThe contrast in PDO indexes between the 1960-1976 period and the 1976-2000 period is obvious with the pre-1977 index values appearing to remain relatively flat over a 15 year period and with the post-1977 index appearing to show a gradual increase towards a peak around the early 1990’s.\nTo see if distinct patterns emerge from the percent hot and cold data before and after 1976, we’ll split the data into two segments using a cutoff year of 1976-1977. Values associated with a period prior to 1977 will be assigned a seg value of Before and those associated with a post-1977 period will be assigned a seg value of After.\n\ndf2.l$seg &lt;- ifelse(df2.l$Year &lt; 1977, \"Before\", \"After\")\n\nNext, we’ll plot the data across four facets:\n\nggplot(df2.l, aes(x = Year, y = Percent)) + geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) + facet_grid(seg ~ Temp)\n\n\n\n\nWe can also choose to map seg to the color aesthetics which will split the points by color with the added benefit of fitting two separate models to each batch.\n\nggplot(df2.l, aes(x = Year, y = Percent, col = seg)) + geom_point() +\n  stat_smooth(method = \"lm\", se=FALSE) + facet_wrap( ~ Temp, nrow = 1)\n\n\n\n\nTo check for “straightness” in the fits, we’ll fit a loess to the points.\n\nggplot(df2.l, aes(x = Year, y = Percent, col = seg)) + geom_point() +\n  stat_smooth(method = \"loess\", se = FALSE, method.args = list(degree = 1)) + \n  facet_wrap( ~ Temp, nrow = 1)\n\n\n\n\nThere is a clear “stair-step” pattern for the percent cold nights. However, there seems to be an upward trend in the percent of hot nights for the post-1977 period which could imply that in addition to the PDO effect, another process could be at play.\nThis little exercise highlights the ease in which an analysis can follow different (and seemingly sound) paths. It also serves as a reminder of the importance of domain knowledge when exploring data."
  },
  {
    "objectID": "sl_plot.html#example-the-iris-dataset",
    "href": "sl_plot.html#example-the-iris-dataset",
    "title": "20  Spread-level plots",
    "section": "20.3 Example: the iris dataset",
    "text": "20.3 Example: the iris dataset\nR has a built-in dataset called iris that provide measurements of sepal and petal dimensions for three different species of the iris family. In this next example, we will plot the spreads of the Petal.Length residuals (after removing their group median values) to their group medians.\n\n# Create two new columns: group median and group residuals\n df1 &lt;- iris %&gt;%\n   group_by(Species)  %&gt;%\n   mutate( Median = median(Petal.Length),\n           Residuals = sqrt(abs( Petal.Length - Median)))  \n\n# Generate the s-l plot \n ggplot(df1, aes(x = Median, y = Residuals)) + \n   geom_jitter(alpha = 0.4, width = 0.05, height = 0) +\n   stat_summary(fun = median, geom = \"line\", col = \"red\") +\n   ylab(expression(sqrt( abs(\" Residuals \")))) +\n   geom_text(aes(x = Median, y = 1.3, label = Species))\n\n\n\n\nA monotonic spread is apparent in this dataset too, i.e. as the median length of the Petal increases, so does the spread."
  }
]