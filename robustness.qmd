---
output: html_document
editor_options: 
  chunk_output_type: console
---
# A working example and a case for robust statistics

```{r echo=FALSE}
source("libs/Common.R")
```


```{r echo = FALSE}
pkg_ver(c("dplyr", "ggplot2", "tukeyedar"))
```


## Introduction
  
The following data represent 1,2,3,4-tetrachlorobenzene (**TCB**)  concentrations (in units of ppb) for two site locations: a reference site free of external contaminants and a contaminated site that went through remediation (dataset from Millard *et al.*, p. 416-417).
  
```{r}
# Create the two data objects (TCB concentrations for reference and contaminated sites)
Ref <-  c(0.22,0.23,0.26,0.27,0.28,0.28,0.29,0.33,0.34,0.35,0.38,0.39,
          0.39,0.42,0.42,0.43,0.45,0.46,0.48,0.5,0.5,0.51,0.52,0.54,
          0.56,0.56,0.57,0.57,0.6,0.62,0.63,0.67,0.69,0.72,0.74,0.76,
          0.79,0.81,0.82,0.84,0.89,1.11,1.13,1.14,1.14,1.2,1.33)
Cont <- c(0.09,0.09,0.09,0.12,0.12,0.14,0.16,0.17,0.17,0.17,0.18,0.19,
          0.2,0.2,0.21,0.21,0.22,0.22,0.22,0.23,0.24,0.25,0.25,0.25,
          0.25,0.26,0.28,0.28,0.29,0.31,0.33,0.33,0.33,0.34,0.37,0.38,
          0.39,0.4,0.43,0.43,0.47,0.48,0.48,0.49,0.51,0.51,0.54,0.6,
          0.61,0.62,0.75,0.82,0.85,0.92,0.94,1.05,1.1,1.1,1.19,1.22,
          1.33,1.39,1.39,1.52,1.53,1.73,2.35,2.46,2.59,2.61,3.06,3.29,
          5.56,6.61,18.4,51.97,168.64)

# We'll create a long-form version of the data for use with some of the functions
# in this exercise
df <- data.frame( Site = c(rep("Cont",length(Cont) ), rep("Ref",length(Ref) ) ),
                  TCB  = c(Cont, Ref ) )
```

Our goal is to assess if, overall, the concentrations of TCB at the contaminated site are greater than those at the reference.

## A typical statistical approach: the two sample t-Test

We are interested in answering the question: *"Did the cleanup at the contaminated site reduce the concentration of TCB down to background (reference) levels?"*. If the question being addressed is part of a decision making process such as "Should we continue with the remediation?" we might want to assess if the difference in TCBs between both sites is "significant" enough to conclude that the TCBs are higher than would be expected if chance alone was the process at play.

A popular statistical procedure used to help address this question is the two sample [t-Test](https://mgimond.github.io/Stats-in-R/z_t_tests.html#t-test-with-two-samples-small-sample-sizes). The test is used to assess whether or not the mean concentration between both batches of values are significantly different from one another.  The test can be framed in one of three ways: We can see if the batches are similar, if one batch is greater than the other batch, or if one batch is smaller than the other batch. In our case, we will assess if the `Cont` batch is greater than the `Ref` batch (this is the *alternative* hypothesis). We'll make use of the `t.test` function and set the parameter `alt` to `"greater"` (indicating that we are assessing if the `Cont` mean is significantly greater than that of `Ref`). 

```{r}
t.test(Cont, Ref, alt="greater")
```

The test suggests that there is just a small chance (**7.5%**) that the two batches of concentrations are the same. The test outputs the means of each batch: **3.9 ppb** for the contaminated site and **0.6 ppb** for the reference site.

Many ancillary data analysts may stop here and proceed with the decision making process. This is not good practice. To see why, let's deconstruct the t-test.

First, we need to find out how the test is representing the batches of numbers. The t-test characterizes the *location* of the batch using the **mean**, and the *spread* using the **standard deviation**. Nothing more. In essence, the test is reducing the complexity of the batches down to two pairs of numbers.

```{r echo=FALSE}
Ref.mean  <- mean(Ref)  # Reference site mean
Ref.sd    <- sd(Ref)    # Reference site standard deviation
Cont.mean <- mean(Cont) # Contaminated site mean
Cont.sd   <- sd(Cont)   # Contaminated site standard deviation
```

The t-test uses these pairs of numbers to reconstruct, then compare the distributions. Here's how the t-test is characterizing the distribution of value for the reference site, `Ref`:

```{r echo = FALSE, fig.width=3, fig.height=3.5}
library(tukeyedar)
eda_normfit(Ref, size = 0.5, tsize = 1.1, show.par = FALSE)
```
The right side of the plot is the fitted *Normal* distribution computed from the sample's standard deviation--this is how the t-test is characterizing the `Ref` distribution. The left side of the plot is the density distribution of the actual values. The points in between both plot halves are the actual values. Note the skew towards higher concentrations. With the `Ref` data, one might argue that the *Normal* fit is doing a reasonable job in characterizing the distribution of values.  

Now let's see how well the *Normal* fit characterizes the dsitrbuion of the contaminated site values, `Cont`.

```{r echo = FALSE, fig.width=3, fig.height=3.5}
eda_normfit(Cont, size = 0.5, tsize = 1.1, show.par = FALSE)

```
The *Normal* fit is on the right. The density plot is barely noticeable of the left! You'll also note the tight clusters the points near the center of the Normal distribution. There are just a few points that extend beyond the tight cluster. This small handful of points is what disproportionately inflates the standard deviation which, in turns, leads to the disproportionately large Normal distribution as adopted in the t-test.

> As an aside, it's worth mentioning that the variances are clearly unequal thus violating a basic requirement for the t-test however, R invokes the Welch's t-test by default to mitigate for unequal variance and sample size. But, Welch's test does require that the distributions follow a normal theoretical distribution (i.e. they can be different in size but must be normal in shape).

Since the t-test adopts a Normal characterization of the spread, it would behoove us to check the assumption of normality using the *normal q-q plot*.

```{r echo = FALSE, fig.width = 3, fig.height = 3.5}
eda_qq(Cont, norm=TRUE, show.par = FALSE)
```

This is a textbook example of a batch of values that does **not** conform to a *Normal* distribution. At least four values (which represent ~5% of the data) seem to contribute to the strong skew and to a much distorted representation of location and spread. The mean and standard deviation are **not robust** to extreme values. In essence, all it takes is one single outlier to heavily distort the representation of location and spread in our data. We say that the mean and standard deviation have a breakdown point of `1/n` where `n` is the sample size.

The median and interquartile range are less sensitive to extreme values. In fact, the median has a breakdown point of `n/2`. In other words, half of the values would have to be modified to alter the median.

The boxplot makes use of these robust measures of location and spread; let's compare the batches with and without the extreme (outlier) values.

```{r fig.width=6, fig.height=3.5, fig.show='hold', echo = FALSE}
OP <- par(mfrow=c(1,2), mar=c(3,2,1,1))
# Boxplot with outliers
eda_boxls(df, TCB, Site, ylab = "With outliers")
# Boxplot without outliers
eda_boxls(df, TCB, Site, outlier = FALSE, ylab = "Without outliers")
par(OP)
```

Note that because of the robust nature of the median and interquartile range, the boxplot helps us to spot the outliers. In fact, the boxplot has a breakdown point of `n/4` (i.e. 25% of the values must be extreme before we see any masking of extreme values). The standard deviation, on the other hand, can be inflated by *one* extreme value thus *masking* the potentially problematic values.

One observation that can also be gleaned from this plot is the skewed nature of the `Cont` data within the interquartile range (IQR). This suggests that even if we were to remove the outliers, the data would violate the normal distribution requirements.

## Re-expression

If we are to use the t-test, we need to make sure that the distributional requirements are met. Even Welch's modification has one requirement about the distribution: **both spreads must follow a normal distribution**. Let's compare both batches to a normal distribution via a normal q-q  plot. We'll make use of the custom `eda_qq` function:

```{r class.source="eda", fig.width=6, fig.height=3.5, fig.show='hold', echo=2:3}
OP <- par(mfrow=c(1,2), mar=c(2,2,1,1))
eda_qq(Ref, norm=TRUE)
eda_qq(Cont, norm=TRUE)
par(OP)
```

These batches do not follow the straight line. This suggests skewness in the distribution (as was observed with the boxplots). A workaround to this problem is to re-express the batches of values in such a way to render them as close to normal as possible. However, in doing so, we must make sure that both batches are re-expressed in an equal way to facilitate comparison. A popular re-expression used with observational data that exhibit skewness towards higher values is the log transformation. The `eda_qq` function has an argument, `p=`, that takes as input a re-expression that will be applied to the values. The function will also allow us to choose between a Tukey (`tukey=TRUE`) or Box-Cox (`tukey=FALSE`) method. The default is set to `tukey=FALSE`. To apply a log transformation, we set `p` to `0`.

```{r class.source="eda", fig.width=6, fig.height=3.5, fig.show='hold', echo=2:3}
OP <- par(mfrow=c(1,2), mar=c(2,2,1,1))
eda_qq(Ref, norm=TRUE, p = 0)
eda_qq(Cont, norm=TRUE, p = 0)
par(OP)
```

## Fine-tuning the re-expression

The log transformation is one of many re-expressions that can be applied to the data. Let's explore the skewness across different "depths" of the `Cont` values to see if the skewness is systematic. We'll use letter value summary plots to help guide us to a reasonable re-expression. We'll make use of the `eda_lsum` function to generate the table and `ggplot2` to generate the plot.

First, we'll look at the raw contaminated site data:

```{r fig.width=3, fig.height=2.8}
library(ggplot2)

Cont.lsum <- eda_lsum(Cont, l=7)
ggplot(Cont.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + 
                    scale_x_reverse() +geom_text(vjust=-.5, size=4)

```

The data become strongly skewed for 1/32^th^ of the data (depth letter `C`). Let's now look at the reference site.

```{r fig.width=3, fig.height=2.8}
Ref.lsum <- eda_lsum(Ref, l=7)
ggplot(Ref.lsum) + aes(x=depth, y=mid, label=letter) +geom_point() + 
                   scale_x_reverse() +geom_text(vjust=-.5, size=4)
```

A skew is also prominent here but a bit more consistent across the depths with a slight drop between depths `D` and `C` (16^th^ and 32^nd^ extreme values). 

Next, we will find a power function that re-expresses the values to satisfy the t-test distribution requirement. We'll first look at the log transformation implemented in the last section. Note that we are using the custom `eda_re` function to transform the data. We'll also make use of some *advanced* coding to reduce the code chunk size.


```{r fig.width=5, fig.height=3, message=FALSE}
library(dplyr)

df  %>% 
  group_by(Site) %>%
  do(eda_lsum( eda_re(.$TCB,0), l=7) ) %>%
  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + 
  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)
```


The log transformation seems to work well with the reference site, but it's not aggressive enough for the contaminated site. Recall that to ensure symmetry across *all* levels of the batches, the letter values must follow a straight (horizontal) line. Let's try a  power of `-0.5`. Given that the negative power will reverse the order of the values if we adopt the Tukey transformation, we'll use the default Box-Cox method.

```{r fig.width=5, fig.height=3, message=FALSE}
df  %>% 
  group_by(Site) %>%
  do(eda_lsum( eda_re(.$TCB,-0.5), l=7) ) %>%
  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + 
  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)
```

This seems to be too aggressive. We are facing a situation where attempting to normalize one batch distorts the other batch. Let's try a compromise and use `-.35`.

```{r fig.width=5, fig.height=3, message=FALSE}
df  %>% 
  group_by(Site) %>%
  do(eda_lsum( eda_re(.$TCB,-0.35), l=7) ) %>%
  ggplot() + aes(x=depth, y=mid, label=letter) + geom_point() + 
  scale_x_reverse() +geom_text(vjust=-.5, size=4) + facet_grid(.~Site)
```

This seems to be a bit better. It's obvious that we will not find a power transformation that will satisfy both batches, so we will need to make a judgement call and work with a power of -.35 for now.

Let's compare the re-expressed batches with a normal distribution.

```{r class.source="eda", fig.width=5, fig.height=3, message=FALSE, fig.show='hold', echo= 2:3}
OP <- par(mfrow=c(1,2), mar=c(2,2,1,1))
eda_qq(Cont, norm = TRUE, p = -0.35)
eda_qq(Ref, norm = TRUE, p = -0.35)
par(OP)
```

The distributions do not look too bad when viewed in a normal q-q plot. Let's now compare the bacthes via boxplots.

```{r fig.width=3, fig.height=3, fig.show='hold', echo = FALSE}
ggplot(df, aes(x=Site, y=eda_re(TCB, -0.35))) + geom_boxplot() + 
  geom_jitter(width = 0.02, height=0) +
  ylab(expression( "TCB"^{-0.35} ))
```

The data are far more symmetrical. So, how much impact did the skewed distributions have on the t-test? Now that we have normally distributed values, let's rerun the t-test using the re-expressed values.

```{r class.source="eda"}
t.test(eda_re(Cont,-0.35), eda_re(Ref,-0.35), alt="greater")
```

Note that the result differs significantly from that with the raw data. This last run gives us a p-value of **0.15**  whereas the first run gave us a p-value of **0.075**. 

## Don't forget the outliers!

It's obvious once we look at the re-expressed data via the prism of a boxplot that for a few sites, more remediation is needed. But, you will also note that for many other sites, the concentrations are much less than those found at the reference site. No statistical procedure is needed to come to this conclusion! So while the t-test tells us something about *overall* differences, it fails to pick up on some of these intricate details that can only be gleaned from plots. This is the whole essence of exploratory data analysis.

## Robust tests

It should be clear by now that many of the popular statistical procedures that reduce the data to a mean and standard deviation are not robust to datasets having skewed distributions or extreme outliers. In fact, most observational data seldom follow a nice normal distribution. The above exercise demonstrates how a very simple implementation of a t-Test can result in a lengthy detour through exploration and re-expression. This can be time consuming when exploring many different datasests. Fortunately there are several alternative statistics that are far less restrictive than the t-Test but serve the same purpose: comparing batches of numbers. These are covered here very superficially for reference.

### Permutation test

The idea here is that if concentrations of TCB come from sites with identical TCB profiles, then one should not be able to notice a difference in values measured at both sites. By mixing up (permuting) the values across batches, we can come up with a distribution of mean (or median) concentration differences between batches that we would expect to get if there was no difference. We then compare our observed mean (or median) differences to that of the distribution of simulated mean (or median) differences. In the following example, we will choose the median over the mean because of its robust measure of location.

```{r, fig.height=3.5, fig.width=4.0}
set.seed(321)
# Pool the concentrations
Pool <- c(Ref, Cont)

# Create an empty vector that will store the simulated median differences
med.dif <- vector()

# Run simulations
for (i in 1:9999){
  # Permute the pooled data then assign the resampled data to each batch
  Pool.rnd <- sample(Pool, replace=FALSE)
  # Grab the first batch of values
  Cont.rnd <- Pool.rnd[1:length(Cont)]
  # Grab the second batch of values
  Ref.rnd <- Pool.rnd[ (length(Cont)+1):length(Pool)]
  # Compute median differences
  med.dif[i] <- median(Cont.rnd) - median(Ref.rnd)
}

# Plot the distribution of median differences  
hist(med.dif)
# Now let's see where our observed difference in median concentration lies
abline(v = median(Cont) - median(Ref), col="red", lw=2)
```

We can compute a *pseudo* p-value from the above. Note that we are interested in the number of simulated values that are more extreme than our observed value.

```{r}
N.greater <- sum( (median(Cont) - median(Ref)) >= med.dif) # Number of simulated differences
                                                           # greater than the observed value
n <- length(med.dif) #number of simulated values
p <- min(N.greater + 1, n + 1 - N.greater) / (n +1)
p
```

Here, the p-value gives us the probability that our observed difference in median concentration value is consistent with the expected difference if the two sites were identical. In our example, that probability is around `r round(p,2)` suggesting that *overall*, the concentrations at both sites are relatively the same if we adopt the traditional cutoff of 0.05. But note that this p-value is 

### Wilcoxon rank sum test

This is another popular alternative to the t-Test. The technical implementation and interpretation is identical to that of the t-Test. It differs from the t-Test in that it is based on the observation ranks as opposed to the observation means. Here, we implement a two-sided test addressing the question *"are the differences in concentrations between the sites significant"*.

```{r}
wilcox.test(Cont, Ref, alternative = "greater")
```

The p-value (which is similar to that found via the permutation technique) suggests that the difference in *overall* concentrations is not that great between both sites... despite the presence of a few outliers!



## References

Millard S.P, Neerchal N.K., *Environmental Statistics with S-Plus*, 2001.


