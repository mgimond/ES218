---
title: "Week2: Exploring and cleaning dataframes"
output:
  html_document:
    toc: yes
    toc_depth: 3
  word_document: default
---

```{r echo=FALSE}
source("libs/Common.R")
```

Dataset used in this tutorial was downloaded from [NOAA' NDBC buoy data center](http://www.ndbc.noaa.gov/view_text_file.php?filename=44005h2010.txt.gz&dir=data/historical/stdmet/) in a space delimited format. It consists of hourly air and ocean physical measurements such as temperature, wind speed and wave height for 2012 measured at NOAA's buoy #44005 located in the Gulf of Maine (43.204 N, 69.128 W). 

Type the following to load the data file into your current R session (note that we are using the `read.table` function instead of the `read.csv` function):

```{r}
# Load the data (columns are separated by spaces)
dat <- read.table("http://mgimond.github.io/ES218/Data/buoy_44005_2012.dat", sep="", header=TRUE)
```

# Exploring a dataframe

First, let's extract the number of rows and columns in our table:
```{r}
dim(dat)
```

`dat` consists of `r dim(dat)[1]` rows and `r dim(dat)[2]` columns. 

We can extract the names of each column using the `names` function:

```{r}
names(dat)
```

A brief description of each column follows:


Field      |  Description
-----------|------------------------------------------
YY         |  Year
MM         |  Month
DD         |	Day
hh         |	Hour
mm         |	Minute
WDIR       |	Wind direction  (the direction the wind is coming from in degrees clockwise from true N)
WSPD       |	Wind speed, averaged over an eight-minute period (m/s)
GST        |	Peak 5 or 8 second gust speed measured during the eight-minute or two-minute period(m/s)
WVHT       |	Significant wave height (meters) 
DPD        |	Dominant wave period (seconds) 
APD        |	Average wave period (seconds)
MWD        |	The direction from which the waves at the dominant period (DPD) are coming. The units are degrees from true North, increasing clockwise, with North as 0 (zero) degrees and East as 90 degrees.
PRES       |	Sea level pressure (hPa)
ATMP       |	Air temperature (Celsius)
WTMP       |	Sea surface temperature (Celsius)
DEWP       |	Dewpoint temperature (Celsius)
VIS        |	visibility (nautical miles)
TIDE       |	Water level in feet above or below Mean Lower Low Water, MLLW (feet)


For large datasets, it's sometimes helpful to display just the first few lines of the table to get a sense of the kind of data we are dealing with. We'll use the `head` function to do this and have R return the first five records of the table (`n=5`).

```{r}
head( dat, n=5)
```

We can also display the last few lines of the table using the `tail` function.

```{r}
tail( dat, n =5)
```

We can usually tell what data types are assoicated with each column by viewing the first few lines. But it's best to check with the `str` function.

```{r}
str(dat)
```

All values are stored as numbers with about a third stored as integers, `int`. The `num` description indicates that the data are stored using double precision.

Now let's generate a summary of the table.

```{r}
summary(dat)
```

You'll note that many columns in our data contain values of 99 or 999. Such values are often placeholders for missing data. If not dealt with properly, these values will be interpreted as valid data by the software. One tell-tale sign that these values should be treated specially is their extreme values compared to the rest of the data batch. For example, the wave height field, `WVHT`, has values that average less than 2 meters so a value of 99 meters should be flagged as suspicious. In fact, NOAA's [documentation](http://www.ndbc.noaa.gov/measdes.shtml) states that *"Missing data ... are denoted by ... a variable number of 9's ... depending on the data type (for example: 999.0 99.0)."*

We also note that some columns have nothing but missing values (e.g. `VIS` and `TIDE`) suggesting that either these variables are not measured at this particular buoy or the instruments did not function properly during the data collection period.

Before proceeding with any analysis, we need to address the missing values by flagging them as such or removing them all together.

# Removing columns with no data

Let's first remove all columns devoid of valid data. These columns are `MWD`, `VIS` and `TIDE`. There are many ways to do this. We will opt for subsetting the table and assigning this subset to a new data frame we will call `dat1`.

```{r}
dat1 <- dat[ , !(names(dat) %in% c("MWD", "VIS", "TIDE"))]
```

Let's break down this expression into its many components. `names(dat)` returns a list of column names in the order in which they appear in the object `dat`: `r names(dat)`.

The Boolean operator `%in%` compares two sets of vectors and assesses if an element on the left-hand side of `%in%` is included in the elements on the right. For each element in the left hand set, R returns `TRUE` if the value is included in the right-hand set of values or `FALSE` if it is not. In our example, R is evaluating if the column names are included in the set of elements defined in `c("MWD", "VIS", "TIDE")`. The expression could be read as  "...are any of the column names (i.e. `r names(dat)`) included in the set `("MWD", "VIS", "TIDE")`?"  The output of this comparison is a series of `TRUE`'s and `FALSE`'s depending on whether or not a match is found.

```{r}
names(dat) %in% c("MWD", "VIS", "TIDE")
```

The order in which the `TRUE`'s and `FALSE`'s are listed match the order of the column names. This order is important because this index is what R uses to identify which columns to return. But wait, R will return columns associated with a `TRUE` value! This last expression gives us `TRUE` values for columns containing missing data. We want the reverse, so we add the "not" operator, `!`, in front of the expression thus flipping the `TRUE`/`FALSE` values.

```{r}
!(names(dat) %in% c("MWD", "VIS", "TIDE"))
```

So `dat1` contains all the columns except `MWD`, `VIS` and `TIDE`.

```{r}
head(dat1)
```

# Assigning `NA` to missing data

It's usually best to assign `NA` values the moment a data file is loaded into R. For example, had all `NA` values been flagged as `-9999`, we could have added the parameter `na.strings="-9999"` to the `read.table()` function. But our dataset contains missing values flagged as 99, 999 *and* 9999 requiring that we combine the missing string values such as `na.strings = c("99", "999", "9999")` however, this would have resulted in undesirable outcomes. For example, a wave height of 99 meters would clearly not make sense, so flagging such value as `NA` would be sensible, but a wind direction of 99 degrees would make sense and flagging such value as `NA` would not be appropriate. Had we set *all* values of 99 to `NA` the moment we loaded the file into R, we would have inadvertently removed valid values in the dataset. So flagging missing values as `NA` in our dataset will require manual intervention.

Let's review the data summary to identify each column's nodata numeric designation (i.e. 99 or 999 or 9999) if present. Remember that we are now working off of the `dat1` dataframe and not the orginal `dat`.

```{r}
summary(dat1)
```

It appears that the fields with missing values flagged as `99` are `WSPD`, `GST`, `WVHT`, `DPD` and  `APD`; fields with missing values flagged as `999` are `WDIR` and `DEWP`; and the field with missing values flagged as `9999` is `PRES`.  We will convert these values to missing values in the following chunk of code.


```{r}
# Create list of columns with missing values
na99   <- c("WSPD", "GST", "WVHT", "DPD",  "APD")
na999  <- c("WDIR",  "DEWP", "ATMP", "WTMP")
na9999 <- ("PRES")

# Replace missing values with NA
dat1[,na99][dat1[, na99] == 99] <- NA
dat1[,na999][dat1[, na999] == 999] <- NA
dat1[,na9999][dat1[, na9999] == 9999] <- NA
```

There are two components to the last three lines of code: The first is a subset of columns for which a particular missing value is designated (e.g. `dat1[,na99]` ), the second is an evaluation statement identifying which values in the column subset is flagged as missing (e.g. `[dat1[, na99] == 99]`). Together, these elements define the indices (table cells) for which a value is replaced with `NA` (e.g. ` <- NA`).

Another way this could have been accomplished (and one you might feel more comfortable with) would have been to work off of one column at a time, for example:

```{r eval=FALSE}
dat1$WSPD[dat1$WSPD == 99] <- NA
dat1$GST[dat1$WSPD == 99] <- NA
...
dat1$WDIR[dat1$WDIR == 999] <- NA
...
dat1$PRES[dat1$PRES == 9999] <- NA
```

Let's look at a summary of our table:

```{r}
summary(dat1)
```

Note the `NA`'s in the summary output. We now have a clean dataframe to work with.

# Removing rows with `NA` values

Your analysis might require that all rows in a table have complete cases (i.e. no missing values). You can use the `na.omit` function to return just the rows that have complete cases. We will create a synthetic example to demonstrate the `na.omit()` function. 

```{r tidy=FALSE}
d <- data.frame(x = c(1,4,2,5,2,3,NA), 
                y = c(3,2,5,3,8,1,1), 
                z = c(NA,NA,4,9,7,8,3))
d
```

Note the `NA` values in the dataframe. Now let's create a new dataframe that we'll name `d.complete` that only stores the rows for which we have complete cases (i.e. no `NA` values).

```{r tidy=FALSE}
d.complete <- na.omit(d)
d.complete
```

All rows with at least one `NA` value have been removed. Note that the table's dimension has changed from seven rows to four.

# Is there an easier way?

Cleaning data tables is an important component of any analytical workflow, but the syntax can get complicated  and difficult to follow. Fortunately, there is an easier way to clean data tables using the `dplyr`  package which will be covered in Week 3. 

