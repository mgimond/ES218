---
title: "Week2: Exploring and cleaning dataframes"
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
source("libs/Common.R")
```

The dataset used in this tutorial was downloaded from [NOAA' NDBC buoy data center](http://www.ndbc.noaa.gov/view_text_file.php?filename=44005h2010.txt.gz&dir=data/historical/stdmet/) in a space delimited file format. It consists of hourly air and ocean physical measurements such as temperature, wind speed and wave height for 2012 measured at NOAA's buoy #44005 located in the Gulf of Maine (43.204 N, 69.128 W). 

Type the following to load the data file into your current R session (note that we are using the `read.table` function instead of the `read.csv` function since the data are not comma delimited):

```{r}
dat <- read.table("http://mgimond.github.io/ES218/Data/buoy_44005_2012.dat", header=TRUE)
```

Since `read.table` defaults to a space separated format (the same format our data file is stored in), there is no need to specify the `sep=` parameter.

# Exploring a dataframe

First, let's extract the number of rows and columns in our table:
```{r}
dim(dat)
```

`dat` consists of `r dim(dat)[1]` rows and `r dim(dat)[2]` columns. 

We can extract the names of each column using the `names` function:

```{r}
names(dat)
```

A brief description of each column follows:


Field      |  Description
-----------|------------------------------------------
YY         |  Year
MM         |  Month
DD         |	Day
hh         |	Hour
mm         |	Minute
WDIR       |	Wind direction  (the direction the wind is coming from in degrees clockwise from true N)
WSPD       |	Wind speed, averaged over an eight-minute period (m/s)
GST        |	Peak 5 or 8 second gust speed measured during the eight-minute or two-minute period(m/s)
WVHT       |	Significant wave height (meters) 
DPD        |	Dominant wave period (seconds) 
APD        |	Average wave period (seconds)
MWD        |	The direction from which the waves at the dominant period (DPD) are coming. The units are degrees from true North, increasing clockwise, with North as 0 (zero) degrees and East as 90 degrees.
PRES       |	Sea level pressure (hPa)
ATMP       |	Air temperature (Celsius)
WTMP       |	Sea surface temperature (Celsius)
DEWP       |	Dewpoint temperature (Celsius)
VIS        |	visibility (nautical miles)
TIDE       |	Water level in feet above or below Mean Lower Low Water, MLLW (feet)


For large datasets, it's sometimes helpful to display just the first few lines of the table to get a sense of the kind of data we are dealing with. We'll use the `head` function to do this and have R return the first five records of the table (`n=5`).

```{r}
head( dat, n=5)
```

We can also display the last few lines of the table using the `tail` function.

```{r}
tail( dat, n =5)
```

To view the data type/class associated with each variable in the table, we'll use the `str` function.

```{r}
str(dat)
```

All values are stored as numbers with about a third of those stored as integers, `int`, and the others as double precision, `num`.

Now let's generate a descriptive summary of the table.

```{r}
summary(dat)
```

You'll note that many columns in our data contain values of 99 or 999. Such values are often placeholders for missing data. If not dealt with properly, these values may be interpreted as valid data in subsequent analyses. One tell-tale sign that these values should be treated specially is their extreme values compared to the rest of the data batch. For example, the wave height field, `WVHT`, has values that average less than 2 meters so a value of 99 meters should be flagged as suspicious. In fact, NOAA's [documentation](http://www.ndbc.noaa.gov/measdes.shtml) states that *"Missing data ... are denoted by ... a variable number of 9's ... depending on the data type (for example: 999.0 99.0)."*

We also note that some columns have nothing but missing values (e.g. `VIS` and `TIDE`) suggesting that either these variables are not measured at this particular buoy or the instruments did not function properly during the data collection period.

Before proceeding with any analysis, we need to address the missing values by flagging them as such or removing them all together.

# Removing columns with no data

Let's first remove all columns devoid of valid data. These columns are `MWD`, `VIS` and `TIDE`. There are many ways to do this. We will opt for subsetting the table and assigning this subset to a new data frame we will call `dat1`.

```{r}
dat1 <- dat[ , !(names(dat) %in% c("MWD", "VIS", "TIDE"))]
```

Let's break down this expression into its many components. `names(dat)` returns a list of column names in the order in which they appear in the object `dat`: `r names(dat)`.

The matching operator `%in%` compares two sets of vectors and assesses if an element on the left-hand side of `%in%` is included in the elements on the right. For each element in the left hand set, R returns `TRUE` if the value is present in the right-hand set of values or `FALSE` if it is not. In our example, R is evaluating if the column names are included in the set of elements defined in `c("MWD", "VIS", "TIDE")`. The expression could be read as  "... are any of the column names (i.e. `r names(dat)`) included in the set `("MWD", "VIS", "TIDE")`?"  The output of this comparison is a series of `TRUE`'s and `FALSE`'s depending on whether or not a match is found.

> A short video summary on using `%in%` can be found [here](./Videos/Matching_operator.mp4).

```{r}
names(dat) %in% c("MWD", "VIS", "TIDE")
```

The order in which the `TRUE`'s and `FALSE`'s are listed match the order of the column names. This order is important because this index is what R uses to identify which columns to return. But wait, R will return columns associated with a `TRUE` value! We want the reverse, so we add the NOT operator, `!`, in front of the expression thus flipping the `TRUE`/`FALSE` values.

```{r}
!(names(dat) %in% c("MWD", "VIS", "TIDE"))
```

So `dat1` contains all the columns except `MWD`, `VIS` and `TIDE`.

```{r}
head(dat1)
```

# Boolean Operations

The NOT operator, `!`, used in the last section is one of three Boolean operators you'll be making good use of in this course; the other two being the AND operator, `&`, and the OR operator, `|`.

```{r echo=FALSE}
a <- 3
b <- 6
```

The following table demonstrates a few examples using the vectors `a <- 3` and `b <- 6`.


+------------------+----------+------------------+----------+
| Boolean operator |  Syntax  | Example          | Outcome  |   
+==================+==========+==================+==========+ 
|NOT               |   `!`    |`!`(a == 3)       | FALSE    | 
+------------------+----------+------------------+----------+
|AND               |   `&`    |a == 3 `&` b == 1 | FALSE    |
+------------------+----------+------------------+----------+
|OR                |   `|`    |a == 3 `|` b == 1 | TRUE     |
+------------------+----------+------------------+----------+


The following table breaks down all possible Boolean outcomes where `T` = `TRUE` and `F` = `FALSE`:

+------------------+------------+
|Boolean operation |  Outcome   |
+==================+============+
|T `&` T           |   TRUE     |
+------------------+------------+
|T `&` F           |   FALSE    |
+------------------+------------+
|F `&` F           |   FALSE    |
+------------------+------------+
|T `|` T           |   TRUE     |
+------------------+------------+
|T `|` F           |   TRUE     |
+------------------+------------+
|F `|` F           |   FALSE    |
+------------------+------------+
|`!`T              |   FALSE    |
+------------------+------------+
|`!`F              |   TRUE     |
+------------------+------------+

## A word of caution

Note that the operation `a == (3 | 4)` is **not** the same as `(a == 3) | (a == 4)`. The former will return `FALSE` whereas the latter will return `TRUE`. This is because the Boolean operator evaluates both sides of its expression as separate **logical** outcomes (i.e. `T` and `F` values). In the latter case, the Boolean expression is asking *"is `a` equal to `3` OR is `a` equal to `4`"*. Since one of the conditions is true, the expression ends up evaluating `TRUE | FALSE` which returns `TRUE` (see above table).  

```{r}
(a == 3) | (a == 4)
```

In the former expression, the boolean operator `|` is evaluating `3` OR `4`. Recall that logical values usually take on values of `0` for FALSE and `1` for TRUE, but boolean operators will treat any none 0 values as TRUE. So when it's evaluating `3 | 4`, it's really seeing `TRUE | TRUE` which, according to the aforementioned table will output `TRUE`.

```{r}
3 | 4
```

So in the end, the expression `a == (3 | 4)` is really evaluating the condition `a == TRUE` which returns false (since 3 is not equal to the logical value `TRUE`).

```{r}
a == (3 | 4)
```


# Assigning `NA` to missing data

> A short video summary on replacing values with `NA` can be found [here](./Videos/Replace_99_with_NA.mp4).

It's usually best to assign `NA` values the moment a data file is loaded into R. For example, had all `NA` values been flagged as `-9999`, we could have added the parameter `na.strings="-9999"` to the `read.table()` function. But our dataset contains missing values flagged as 99, 999 *and* 9999 requiring that we combine the missing string values such as `na.strings = c("99", "999", "9999")` however, this would have resulted in undesirable outcomes. For example, a wave height of 99 meters would clearly not make sense, so flagging such value as `NA` would be sensible, but a wind direction of 99 degrees would make sense and flagging such value as `NA` would not be appropriate. Had we set *all* values of 99 to `NA` the moment we loaded the file into R, we would have inadvertently removed valid values in the dataset. So flagging missing values as `NA` in our dataset will require column level intervention.

Let's review the data summary to identify each column's no data numeric designation (i.e. 99 or 999 or 9999) if present. Remember that we are now working off of the `dat1` dataframe and not the original `dat` dataframe.

```{r}
summary(dat1)
```

It appears that the fields with missing values flagged as `99` are `WSPD`, `GST`, `WVHT`, `DPD` and  `APD`; fields with missing values flagged as `999` are `WDIR` and `DEWP`; and the field with missing values flagged as `9999` is `PRES`.  We will convert these values to missing values in the following chunk of code.


```{r}
# Create list of columns with missing values
na99   <- c("WSPD", "GST", "WVHT", "DPD",  "APD")
na999  <- c("WDIR",  "DEWP", "ATMP", "WTMP")
na9999 <- ("PRES")

# Replace missing values with NA
dat1[ ,na99][dat1[ ,na99] == 99] <- NA
dat1[ ,na999][dat1[ ,na999] == 999] <- NA
dat1[ ,na9999][dat1[ ,na9999] == 9999] <- NA
```

The above expression is quite succinct, but succinctness does not always translate to easy readability. There are two components to the last three lines of code: The first is a subset of columns for which a particular missing value is designated (e.g. `dat1[,na99]` ), the second is an evaluation statement identifying which values in the column subset is flagged as missing (e.g. `[dat1[, na99] == 99]`). Together, these elements define the indices (table cells) for which a value is replaced with `NA` (e.g. ` <- NA`).

Another way this could have been accomplished (and one you might feel more comfortable with) would have been to work off of one column at a time, for example:

```{r eval=FALSE}
dat1$WSPD[dat1$WSPD == 99] <- NA
dat1$GST[dat1$GST   == 99] <- NA
dat1$WVHT[dat1$WVHT == 99] <- NA
dat1$DPD[dat1$DPD   == 99] <- NA
dat1$APD[dat1$APD   == 99] <- NA

dat1$WDIR[dat1$WDIR == 999]  <- NA
dat1$DEWP[dat1$DEWP == 999]  <- NA
dat1$ATMP[dat1$ATMP == 999]  <- NA
dat1$WTMP[dat1$WTMP == 999]  <- NA

dat1$PRES[dat1$PRES == 9999] <- NA
```

This chunk of code is easier to read, but lacks the succinctness of its predecessor.

Let's look at a summary of our table:

```{r}
summary(dat1)
```

Note the `NA`'s in the summary output. We now have a clean dataframe to work with.

# Removing rows with `NA` values

Your analysis might require that all rows in a table have complete cases (i.e. no missing values). You can use the `na.omit` function to return just the rows that have complete cases. We will create a synthetic example to demonstrate the `na.omit()` function. 

```{r tidy=FALSE}
d <- data.frame(x = c(1,4,2,5,2,3,NA), 
                y = c(3,2,5,3,8,1,1), 
                z = c(NA,NA,4,9,7,8,3))
d
```

Note the `NA` values in the dataframe. Now let's create a new dataframe that we'll name `d.complete` that only stores the rows for which we have complete cases (i.e. no `NA` values).

```{r tidy=FALSE}
d.complete <- na.omit(d)
d.complete
```

All rows with at least one `NA` value have been removed. Note that the table's dimension has changed from seven rows to four.

# Checking if a value is `NA`

When assessing if a value is equal to `NA` the following evaluation may behave unexpectedly.

```{r}
a <- c (3, 67, 4, NA, 10)
a == NA
```

The output is not a logical datatype we would expect from an evaluation. Instead, you must make use of the `is.na()` function:

```{r}
is.na(a)
```

As another example, if we want to keep all rows in dataframe `d` where `z` = `NA`, we would type:

```{r}
d[ is.na(d$z), ]
```

You can, of course, use the `!` operator to reverse the evaluation and *omit* all rows where `z` = `NA`,

```{r}
d[ !is.na(d$z), ]
```


# Is there an easier way?

Cleaning data tables is an important component of any analytical workflow, but the syntax used thus far can get complicated and be difficult to follow. Fortunately, there is an easier way to clean data tables using the `dplyr`  package which will be covered in [Week 3](Week03a.html). 

